# Generated by SmolLM3-3B on 2025-01-12
"""
LlamaCpp MemOS Wrapper

Custom LLM wrapper class that implements the MemOS BaseLLM interface
around llama-cpp-python's Llama object. This bridges the gap between
the high-performance GGUF model loading and MemOS's memory system.
"""

import logging
import sys
import os
from typing import Any, Optional

# Add MemOS to Python path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'MemOS', 'src'))

try:
    from memos.configs.llm import BaseLLMConfig
    from memos.llms.base import BaseLLM
    from memos.types import MessageList
    from memos.log import get_logger
    logger = get_logger(__name__)
except ImportError:
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

try:
    from llama_cpp import Llama
    LLAMA_CPP_AVAILABLE = True
except ImportError:
    LLAMA_CPP_AVAILABLE = False


class LlamaCppWrapper(BaseLLM):
    """
    Custom LLM wrapper that implements MemOS BaseLLM interface around llama-cpp-python.
    
    This class acts as a bridge between llama-cpp-python's Llama object and MemOS's
    expected LLM interface, enabling MemOS to use our high-performance GGUF model
    for memory-enhanced conversations.
    """
    
    def __init__(self, llama_instance: "Llama", config: Optional[BaseLLMConfig] = None):
        """
        Initialize the wrapper with an existing llama-cpp-python Llama instance.
        
        Args:
            llama_instance (Llama): Pre-loaded llama-cpp-python Llama instance
            config (Optional[BaseLLMConfig]): MemOS LLM configuration (for compatibility)
        """
        if not LLAMA_CPP_AVAILABLE:
            raise RuntimeError("llama-cpp-python is required but not available")
        
        if not isinstance(llama_instance, Llama):
            raise ValueError("llama_instance must be a llama-cpp-python Llama instance")
        
        self.llama = llama_instance
        self.config = config or self._create_default_config()
        
        logger.info("LlamaCpp MemOS wrapper initialized successfully")
    
    def _create_default_config(self) -> BaseLLMConfig:
        """Create a default BaseLLMConfig for compatibility."""
        try:
            # Create a minimal config object with default values
            config_dict = {
                'model_name_or_path': 'llama-cpp-wrapped-model',
                'temperature': 0.7,
                'max_tokens': 512,
                'top_p': 0.9,
                'top_k': 50,
                'remove_think_prefix': False
            }
            return BaseLLMConfig(**config_dict)
        except Exception:
            # Fallback if BaseLLMConfig is not available
            return None
    
    def generate(self, messages: MessageList, **kwargs) -> str:
        """
        Generate a response from the wrapped llama-cpp-python model.
        
        Args:
            messages (MessageList): List of message dicts with 'role' and 'content' keys
            **kwargs: Additional generation parameters
            
        Returns:
            str: Generated response text
        """
        try:
            # Convert MemOS message format to prompt string
            prompt = self._messages_to_prompt(messages)
            
            # Extract generation parameters, using config defaults and kwargs overrides
            max_tokens = kwargs.get('max_tokens', getattr(self.config, 'max_tokens', 512))
            temperature = kwargs.get('temperature', getattr(self.config, 'temperature', 0.7))
            top_p = kwargs.get('top_p', getattr(self.config, 'top_p', 0.9))
            top_k = kwargs.get('top_k', getattr(self.config, 'top_k', 50))
            
            # Generate response using llama-cpp-python
            response = self.llama(
                prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
                top_k=top_k,
                stop=["User:", "\n\n", "Assistant:", "<|im_end|>", "Human:", "AI:"],
                echo=False
            )
            
            # Extract and clean the generated text
            generated_text = response['choices'][0]['text'].strip()
            
            # Remove thinking tags if configured
            if getattr(self.config, 'remove_think_prefix', False):
                generated_text = self._remove_thinking_tags(generated_text)
            
            logger.debug(f"Generated response: {generated_text[:100]}...")
            return generated_text
            
        except Exception as e:
            logger.error(f"Error generating response: {e}")
            raise RuntimeError(f"Failed to generate response: {e}")
    
    def _messages_to_prompt(self, messages: MessageList) -> str:
        """
        Convert MemOS MessageList format to a prompt string suitable for llama-cpp-python.
        
        Args:
            messages (MessageList): List of message dicts
            
        Returns:
            str: Formatted prompt string
        """
        prompt_parts = []
        
        for message in messages:
            role = message.get('role', 'user')
            content = message.get('content', '')
            
            if role == 'system':
                # System messages become part of the context
                prompt_parts.append(f"System: {content}")
            elif role == 'user':
                prompt_parts.append(f"User: {content}")
            elif role == 'assistant':
                prompt_parts.append(f"Assistant: {content}")
            else:
                # Handle unknown roles gracefully
                prompt_parts.append(f"{role.title()}: {content}")
        
        # Add the assistant prompt at the end for completion
        full_prompt = '\n'.join(prompt_parts)
        if not full_prompt.endswith('Assistant:'):
            full_prompt += '\nAssistant:'
        
        return full_prompt
    
    def _remove_thinking_tags(self, text: str) -> str:
        """
        Remove thinking tags from generated text.
        
        Args:
            text (str): Generated text that may contain thinking tags
            
        Returns:
            str: Text with thinking tags removed
        """
        import re
        
        # Remove content between <think> and </think> tags
        text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)
        # Remove content between [think] and [/think] tags
        text = re.sub(r'\[think\].*?\[/think\]', '', text, flags=re.DOTALL)
        # Remove content between (think) and (/think) tags
        text = re.sub(r'\(think\).*?\(/think\)', '', text, flags=re.DOTALL)
        
        return text.strip()
    
    def get_model_info(self) -> dict:
        """
        Get information about the wrapped model.
        
        Returns:
            dict: Model information
        """
        try:
            return {
                'type': 'llama-cpp-wrapped',
                'model_path': getattr(self.llama, 'model_path', 'unknown'),
                'backend': 'llama-cpp-python',
                'config': {
                    'max_tokens': getattr(self.config, 'max_tokens', 512),
                    'temperature': getattr(self.config, 'temperature', 0.7),
                    'top_p': getattr(self.config, 'top_p', 0.9),
                    'top_k': getattr(self.config, 'top_k', 50),
                } if self.config else {}
            }
        except Exception as e:
            logger.warning(f"Could not get model info: {e}")
            return {
                'type': 'llama-cpp-wrapped',
                'model_path': 'unknown',
                'backend': 'llama-cpp-python',
                'error': str(e)
            }


def create_memos_config_for_wrapper(wrapper: LlamaCppWrapper) -> dict:
    """
    Create a MemOS configuration that uses the LlamaCpp wrapper.
    
    Args:
        wrapper (LlamaCppWrapper): The initialized wrapper instance
        
    Returns:
        dict: MemOS configuration dictionary
    """
    return {
        'user_id': 'default_user',
        'session_id': 'default_session',
        'enable_textual_memory': True,
        'enable_activation_memory': False,
        'top_k': 5,
        'chat_model': {
            'backend': 'custom',  # We'll handle this specially
            'instance': wrapper   # Pass the wrapper instance directly
        },
        'textual_memory': {
            'backend': 'simple',
            'config': {
                'embedder': {
                    'backend': 'sentence_transformer',
                    'config': {
                        'model_name_or_path': 'all-MiniLM-L6-v2'
                    }
                }
            }
        }
    } 