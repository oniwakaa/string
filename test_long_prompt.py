# Generated by SmolLM3-3B on 2025-01-01
"""
Long Prompt Latency Test

Test inference latency with longer prompts that utilize the increased 16384 context window.
"""

import time
from pathlib import Path

try:
    from llama_cpp import Llama
    LLAMA_CPP_AVAILABLE = True
except ImportError:
    print("‚ùå llama-cpp-python not available")
    LLAMA_CPP_AVAILABLE = False

def test_long_prompt_latency():
    """Test latency with progressively longer prompts."""
    if not LLAMA_CPP_AVAILABLE:
        print("‚ùå llama-cpp-python not available")
        return
    
    # Find model
    model_paths = [
        "./smollm-quantized/smollm-q4_K_M.gguf",
        "./smollm-quantized/ggml-model-f16.gguf", 
        "./smollm/smollm-3b-instruct-q4_k_m.gguf",
    ]
    
    model_path = None
    for path in model_paths:
        if Path(path).exists():
            model_path = path
            break
    
    if not model_path:
        print(f"‚ùå No model found. Searched: {model_paths}")
        return
    
    print(f"üöÄ Testing long prompt latency with 16384 context window")
    print(f"Model: {model_path}")
    
    # Load model once
    try:
        model = Llama(
            model_path=model_path,
            n_ctx=16384,  # Our new larger context window
            n_gpu_layers=-1,
            n_batch=512,
            verbose=False,
            use_mmap=True,
            use_mlock=False,
            n_threads=None,
        )
        print(f"‚úÖ Model loaded with 16384 context window")
        
    except Exception as e:
        print(f"‚ùå Failed to load model: {e}")
        return
    
    # Create progressively longer prompts
    base_text = """
    Artificial intelligence (AI) is a branch of computer science that aims to create intelligent machines 
    that can perform tasks that typically require human intelligence. This includes learning, reasoning, 
    problem-solving, perception, and language understanding. The field combines computer science, mathematics, 
    linguistics, psychology, and neuroscience to develop systems that can mimic human cognitive functions.
    
    Machine learning is a subset of AI that focuses on algorithms that can learn and improve from experience 
    without being explicitly programmed. Deep learning, a subset of machine learning, uses neural networks 
    with multiple layers to model and understand complex patterns in data.
    """
    
    # Test different prompt lengths (rough token estimates)
    test_cases = [
        (500, "Short context"),
        (2000, "Medium context"), 
        (5000, "Long context"),
        (10000, "Very long context"),
    ]
    
    print(f"\nüß™ Testing inference latency with different prompt lengths:")
    print(f"{'Prompt Length':<15} {'Latency (ms)':<15} {'Status':<15}")
    print(f"{'-'*45}")
    
    for target_tokens, description in test_cases:
        # Create prompt of approximate target length (rough: 1 token ‚âà 0.75 words)
        words_needed = int(target_tokens * 0.75)
        base_words = len(base_text.split())
        repetitions = max(1, words_needed // base_words)
        
        prompt = (base_text + " ") * repetitions
        prompt += f"\n\nBased on the above context about AI and machine learning, please provide a brief summary."
        
        # Measure inference latency
        start_time = time.time()
        
        try:
            response = model(
                prompt,
                max_tokens=100,  # Keep response short to focus on prompt processing
                temperature=0.7,
                echo=False
            )
            
            latency_ms = (time.time() - start_time) * 1000
            
            # Check if latency is acceptable (<200ms target, but longer prompts may take more time)
            if latency_ms < 200:
                status = "‚úÖ Excellent"
            elif latency_ms < 1000:
                status = "üü° Good"
            elif latency_ms < 5000:
                status = "üü† Acceptable"
            else:
                status = "‚ùå Too slow"
            
            print(f"{target_tokens:<15} {latency_ms:<15.1f} {status:<15}")
            
            # Show partial response for verification
            if response.get('choices'):
                response_text = response['choices'][0]['text'][:50]
                print(f"   Response preview: {response_text}...")
            
        except Exception as e:
            print(f"{target_tokens:<15} {'ERROR':<15} ‚ùå {str(e)[:20]}...")
    
    print(f"\nüìä Summary:")
    print(f"‚úÖ Context window: 16384 (4x improvement from 4096)")
    print(f"üéØ Target latency: <200ms for short prompts, <5s for long prompts")
    print(f"üí° Longer prompts naturally take more time due to increased processing")

if __name__ == "__main__":
    test_long_prompt_latency() 