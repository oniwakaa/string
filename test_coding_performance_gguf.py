#!/usr/bin/env python3
"""
# Generated by SmolLM3-3B on 2025-01-12

Coding Performance Validation for SmolLM3-3B GGUF Model
Tests coding capabilities using llama-cpp-python with GGUF format models.
Optimized for local inference with hardware acceleration.
"""

import os
import sys
import time
import json
import psutil
import gc
import traceback
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple
from dataclasses import dataclass, asdict
import warnings
warnings.filterwarnings("ignore")

try:
    from llama_cpp import Llama
except ImportError:
    print("ERROR: llama-cpp-python not installed. Please run: pip install llama-cpp-python")
    sys.exit(1)

@dataclass
class CodingTestResult:
    """Result container for coding validation tests."""
    prompt_id: str
    category: str
    complexity: str
    prompt_text: str
    generated_code: str
    latency_ms: float
    peak_memory_mb: float
    memory_before_mb: float
    memory_after_mb: float
    token_count_input: int
    token_count_output: int
    success: bool
    error_message: Optional[str] = None
    syntax_valid: Optional[bool] = None

class GGUFCodingPerformanceTester:
    """Main class for coding performance validation using GGUF models."""
    
    def __init__(self, model_path: str, n_gpu_layers: int = -1, n_ctx: int = 8192):
        self.model_path = Path(model_path)
        self.model = None
        self.n_gpu_layers = n_gpu_layers  # -1 for all layers on GPU (Metal)
        self.n_ctx = n_ctx
        self.device = self._get_optimal_device()
        
        if not self.model_path.exists():
            raise FileNotFoundError(f"Model file not found: {self.model_path}")
        
        print(f"Initializing GGUF model from: {self.model_path}")
        print(f"Device: {self.device}")
        
    def _get_optimal_device(self) -> str:
        """Determine the optimal device for inference."""
        # On Apple Silicon Macs, Metal is preferred
        try:
            import platform
            if platform.system() == "Darwin" and platform.machine() == "arm64":
                return "Metal (Apple Silicon)"
            elif hasattr(os, "cpu_count") and os.cpu_count() > 4:
                return "CPU (Multi-core)"
            else:
                return "CPU"
        except:
            return "CPU"
    
    def _get_memory_usage(self) -> float:
        """Get current memory usage in MB."""
        process = psutil.Process(os.getpid())
        return process.memory_info().rss / (1024 * 1024)
    
    def load_model(self) -> bool:
        """Load the GGUF model."""
        try:
            print("Loading GGUF model...")
            start_time = time.time()
            
            # Configure model parameters for Apple Silicon optimization
            model_kwargs = {
                "model_path": str(self.model_path),
                "n_ctx": self.n_ctx,
                "verbose": False,
                "n_batch": 512,  # Batch size for prompt processing
                "n_threads": None,  # Auto-detect optimal thread count
            }
            
            # Enable Metal acceleration on Apple Silicon
            if "Metal" in self.device:
                model_kwargs["n_gpu_layers"] = self.n_gpu_layers
            
            self.model = Llama(**model_kwargs)
            
            load_time = time.time() - start_time
            print(f"Model loaded successfully in {load_time:.2f} seconds")
            return True
            
        except Exception as e:
            print(f"Error loading model: {e}")
            traceback.print_exc()
            return False
    
    def generate_code(self, prompt: str, max_tokens: int = 512, temperature: float = 0.1) -> Dict[str, Any]:
        """Generate code using the GGUF model."""
        if not self.model:
            raise RuntimeError("Model not loaded. Call load_model() first.")
        
        # Memory tracking
        memory_before = self._get_memory_usage()
        peak_memory = memory_before
        
        try:
            start_time = time.time()
            
            # Format prompt for code generation
            formatted_prompt = f"<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
            
            # Count input tokens (approximate)
            input_tokens = len(formatted_prompt.split())  # Rough estimate
            
            # Generate response
            response = self.model(
                formatted_prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=0.95,
                stop=["<|im_end|>", "<|endoftext|>"],
                echo=False
            )
            
            end_time = time.time()
            
            # Extract generated text
            generated_text = response["choices"][0]["text"].strip()
            output_tokens = len(generated_text.split())  # Rough estimate
            
            # Track peak memory
            current_memory = self._get_memory_usage()
            peak_memory = max(peak_memory, current_memory)
            
            latency_ms = (end_time - start_time) * 1000
            
            return {
                "generated_code": generated_text,
                "latency_ms": latency_ms,
                "memory_before_mb": memory_before,
                "memory_after_mb": current_memory,
                "peak_memory_mb": peak_memory,
                "token_count_input": input_tokens,
                "token_count_output": output_tokens,
                "success": True,
                "error_message": None
            }
            
        except Exception as e:
            current_memory = self._get_memory_usage()
            return {
                "generated_code": "",
                "latency_ms": 0.0,
                "memory_before_mb": memory_before,
                "memory_after_mb": current_memory,
                "peak_memory_mb": max(peak_memory, current_memory),
                "token_count_input": 0,
                "token_count_output": 0,
                "success": False,
                "error_message": str(e)
            }
    
    def validate_syntax(self, code: str) -> bool:
        """Validate Python syntax of generated code."""
        try:
            import ast
            # Remove markdown code blocks if present
            cleaned_code = code.strip()
            if cleaned_code.startswith("```python"):
                cleaned_code = cleaned_code[9:]
            if cleaned_code.startswith("```"):
                cleaned_code = cleaned_code[3:]
            if cleaned_code.endswith("```"):
                cleaned_code = cleaned_code[:-3]
            
            ast.parse(cleaned_code.strip())
            return True
        except:
            return False
    
    def run_test_suite(self, corpus_path: str = "validation/coding_corpus.json") -> List[CodingTestResult]:
        """Run the complete coding performance test suite."""
        corpus_file = Path(corpus_path)
        if not corpus_file.exists():
            raise FileNotFoundError(f"Corpus file not found: {corpus_file}")
        
        # Load test corpus
        with open(corpus_file, 'r', encoding='utf-8') as f:
            test_cases = json.load(f)
        
        print(f"Running {len(test_cases)} coding tests...")
        results = []
        
        for i, test_case in enumerate(test_cases, 1):
            print(f"\nTest {i}/{len(test_cases)}: {test_case['id']}")
            
            # Generate code
            generation_result = self.generate_code(test_case["prompt"])
            
            # Validate syntax if generation was successful
            syntax_valid = None
            if generation_result["success"] and generation_result["generated_code"]:
                syntax_valid = self.validate_syntax(generation_result["generated_code"])
            
            # Create result
            result = CodingTestResult(
                prompt_id=test_case["id"],
                category=test_case["category"],
                complexity=test_case["complexity"],
                prompt_text=test_case["prompt"],
                generated_code=generation_result["generated_code"],
                latency_ms=generation_result["latency_ms"],
                peak_memory_mb=generation_result["peak_memory_mb"],
                memory_before_mb=generation_result["memory_before_mb"],
                memory_after_mb=generation_result["memory_after_mb"],
                token_count_input=generation_result["token_count_input"],
                token_count_output=generation_result["token_count_output"],
                success=generation_result["success"],
                error_message=generation_result["error_message"],
                syntax_valid=syntax_valid
            )
            
            results.append(result)
            
            # Print progress
            status = "✓" if result.success else "✗"
            syntax_status = "✓" if syntax_valid else "✗" if syntax_valid is not None else "?"
            print(f"  {status} Generated | {syntax_status} Syntax | {result.latency_ms:.1f}ms | {result.peak_memory_mb:.1f}MB")
            
            # Memory cleanup
            gc.collect()
        
        return results
    
    def save_results(self, results: List[CodingTestResult], output_path: str = "validation/coding_results_gguf.json"):
        """Save test results to JSON file."""
        output_file = Path(output_path)
        output_file.parent.mkdir(exist_ok=True)
        
        # Convert results to serializable format
        serializable_results = [asdict(result) for result in results]
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, indent=2, ensure_ascii=False)
        
        print(f"Results saved to: {output_file}")
    
    def print_summary(self, results: List[CodingTestResult]):
        """Print test summary statistics."""
        total_tests = len(results)
        successful_tests = sum(1 for r in results if r.success)
        syntax_valid_tests = sum(1 for r in results if r.syntax_valid)
        
        latencies = [r.latency_ms for r in results if r.success]
        peak_memories = [r.peak_memory_mb for r in results if r.success]
        
        avg_latency = sum(latencies) / len(latencies) if latencies else 0
        max_latency = max(latencies) if latencies else 0
        max_memory = max(peak_memories) if peak_memories else 0
        
        print("\n" + "="*60)
        print("GGUF MODEL PERFORMANCE SUMMARY")
        print("="*60)
        print(f"Total Tests:           {total_tests}")
        print(f"Successful:            {successful_tests} ({successful_tests/total_tests*100:.1f}%)")
        print(f"Syntax Valid:          {syntax_valid_tests} ({syntax_valid_tests/total_tests*100:.1f}%)")
        print(f"Average Latency:       {avg_latency:.1f}ms")
        print(f"Max Latency:           {max_latency:.1f}ms")
        print(f"Peak Memory Usage:     {max_memory:.1f}MB")
        print("="*60)
        
        # Performance criteria check
        criteria_met = []
        criteria_met.append(("Latency < 200ms", avg_latency < 200))
        criteria_met.append(("Syntax Valid > 80%", syntax_valid_tests/total_tests > 0.8))
        criteria_met.append(("Peak RAM < 10GB", max_memory < 10240))
        
        print("\nPERFORMANCE CRITERIA:")
        for criterion, met in criteria_met:
            status = "✓ PASS" if met else "✗ FAIL"
            print(f"  {status}: {criterion}")
        
        all_criteria_met = all(met for _, met in criteria_met)
        final_status = "✓ ALL CRITERIA MET" if all_criteria_met else "✗ SOME CRITERIA FAILED"
        print(f"\nFINAL RESULT: {final_status}")


def main():
    """Main execution function."""
    # Model path - try multiple possible locations (prioritize quantized models)
    possible_model_paths = [
        "smollm-quantized/smollm-q4_K_M.gguf",
        "smollm/smollm-3b-instruct-q4_k_m.gguf",  # If downloaded separately
        "smollm-quantized/smollm-3b-instruct.gguf",
        "smollm-quantized/ggml-model-f16.gguf"
    ]
    
    model_path = None
    for path in possible_model_paths:
        if Path(path).exists():
            model_path = path
            break
    
    if not model_path:
        print("ERROR: No GGUF model found. Please ensure you have:")
        print("1. Converted your model to GGUF format, or")
        print("2. Downloaded a pre-quantized GGUF model")
        print("\nSearched paths:")
        for path in possible_model_paths:
            print(f"  - {path}")
        sys.exit(1)
    
    print(f"Using model: {model_path}")
    
    # Initialize tester
    try:
        # For Apple Silicon Macs, enable GPU acceleration
        n_gpu_layers = -1  # All layers on GPU
        tester = GGUFCodingPerformanceTester(model_path, n_gpu_layers=n_gpu_layers)
        
        if not tester.load_model():
            print("Failed to load model")
            sys.exit(1)
        
        # Run tests
        results = tester.run_test_suite()
        
        # Save and display results
        tester.save_results(results)
        tester.print_summary(results)
        
    except Exception as e:
        print(f"Error during testing: {e}")
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main() 