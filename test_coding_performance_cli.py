#!/usr/bin/env python3
"""
# Generated by SmolLM3-3B on 2025-01-12

Coding Performance Validation for SmolLM3-3B GGUF Model
Tests coding capabilities using native llama.cpp CLI with GGUF format models.
Optimized for local inference with hardware acceleration.
"""

import os
import sys
import time
import json
import subprocess
import psutil
import gc
import traceback
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple
from dataclasses import dataclass, asdict
import tempfile

@dataclass
class CodingTestResult:
    """Result container for coding validation tests."""
    prompt_id: str
    category: str
    complexity: str
    prompt_text: str
    generated_code: str
    latency_ms: float
    peak_memory_mb: float
    memory_before_mb: float
    memory_after_mb: float
    token_count_input: int
    token_count_output: int
    success: bool
    error_message: Optional[str] = None
    syntax_valid: Optional[bool] = None

class CLICodingPerformanceTester:
    """Main class for coding performance validation using CLI tools."""
    
    def __init__(self, model_path: str, cli_path: str = "llama.cpp/build/bin/llama-cli"):
        self.model_path = Path(model_path)
        self.cli_path = Path(cli_path)
        
        if not self.model_path.exists():
            raise FileNotFoundError(f"Model file not found: {self.model_path}")
        
        if not self.cli_path.exists():
            raise FileNotFoundError(f"CLI tool not found: {self.cli_path}")
        
        print(f"Using GGUF model: {self.model_path}")
        print(f"Using CLI tool: {self.cli_path}")
        
    def _get_memory_usage(self) -> float:
        """Get current memory usage in MB."""
        process = psutil.Process(os.getpid())
        return process.memory_info().rss / (1024 * 1024)
    
    def generate_code(self, prompt: str, max_tokens: int = 512, temperature: float = 0.1) -> Dict[str, Any]:
        """Generate code using the CLI tool."""
        memory_before = self._get_memory_usage()
        peak_memory = memory_before
        
        try:
            start_time = time.time()
            
            # Prepare CLI command
            cmd = [
                str(self.cli_path),
                "--model", str(self.model_path),
                "--prompt", prompt,
                "--n-predict", str(max_tokens),
                "--temp", str(temperature),
                "--top-p", "0.95",
                "--repeat-penalty", "1.1",
                "--ctx-size", "8192",
                "--batch-size", "512",
                "--threads", "4",
                "--no-display-prompt"
            ]
            
            # Count input tokens (rough estimate)
            input_tokens = len(prompt.split())
            
            # Run the CLI command
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=60  # 60 second timeout
            )
            
            end_time = time.time()
            
            if result.returncode != 0:
                raise Exception(f"CLI failed with return code {result.returncode}: {result.stderr}")
            
            # Extract generated text from stdout
            output_lines = result.stdout.strip().split('\n')
            
            # Find the generated content (after the prompt)
            generated_text = ""
            capture_started = False
            
            for line in output_lines:
                if "system_info:" in line:
                    capture_started = True
                    continue
                if capture_started and line.strip() and not line.startswith("llama_"):
                    generated_text += line + "\n"
            
            # If no system_info found, take all non-llama lines
            if not generated_text:
                for line in output_lines:
                    if line.strip() and not line.startswith("llama_") and not line.startswith("ggml_") and not "main:" in line:
                        generated_text += line + "\n"
            
            generated_text = generated_text.strip()
            output_tokens = len(generated_text.split())
            
            # Track peak memory
            current_memory = self._get_memory_usage()
            peak_memory = max(peak_memory, current_memory)
            
            latency_ms = (end_time - start_time) * 1000
            
            return {
                "generated_code": generated_text,
                "latency_ms": latency_ms,
                "memory_before_mb": memory_before,
                "memory_after_mb": current_memory,
                "peak_memory_mb": peak_memory,
                "token_count_input": input_tokens,
                "token_count_output": output_tokens,
                "success": True,
                "error_message": None
            }
            
        except subprocess.TimeoutExpired:
            return {
                "generated_code": "",
                "latency_ms": 0.0,
                "memory_before_mb": memory_before,
                "memory_after_mb": self._get_memory_usage(),
                "peak_memory_mb": peak_memory,
                "token_count_input": 0,
                "token_count_output": 0,
                "success": False,
                "error_message": "Timeout after 60 seconds"
            }
        except Exception as e:
            current_memory = self._get_memory_usage()
            return {
                "generated_code": "",
                "latency_ms": 0.0,
                "memory_before_mb": memory_before,
                "memory_after_mb": current_memory,
                "peak_memory_mb": max(peak_memory, current_memory),
                "token_count_input": 0,
                "token_count_output": 0,
                "success": False,
                "error_message": str(e)
            }
    
    def validate_syntax(self, code: str) -> bool:
        """Validate Python syntax of generated code."""
        try:
            import ast
            # Remove markdown code blocks if present
            cleaned_code = code.strip()
            if cleaned_code.startswith("```python"):
                cleaned_code = cleaned_code[9:]
            if cleaned_code.startswith("```"):
                cleaned_code = cleaned_code[3:]
            if cleaned_code.endswith("```"):
                cleaned_code = cleaned_code[:-3]
            
            # Extract just the Python code part
            lines = cleaned_code.split('\n')
            python_lines = []
            for line in lines:
                if line.strip() and (line.startswith('def ') or line.startswith('import ') or 
                                   line.startswith('from ') or line.startswith('class ') or
                                   line.startswith('    ') or line.startswith('#') or
                                   any(keyword in line for keyword in ['print(', 'return ', 'if ', 'for ', 'while '])):
                    python_lines.append(line)
            
            if python_lines:
                cleaned_code = '\n'.join(python_lines)
            
            ast.parse(cleaned_code.strip())
            return True
        except:
            return False
    
    def run_test_suite(self, corpus_path: str = "validation/coding_corpus.json") -> List[CodingTestResult]:
        """Run the complete coding performance test suite."""
        corpus_file = Path(corpus_path)
        if not corpus_file.exists():
            raise FileNotFoundError(f"Corpus file not found: {corpus_file}")
        
        # Load test corpus
        with open(corpus_file, 'r', encoding='utf-8') as f:
            test_cases = json.load(f)
        
        print(f"Running {len(test_cases)} coding tests...")
        results = []
        
        for i, test_case in enumerate(test_cases, 1):
            print(f"\nTest {i}/{len(test_cases)}: {test_case['id']}")
            
            # Generate code
            generation_result = self.generate_code(test_case["prompt"])
            
            # Validate syntax if generation was successful
            syntax_valid = None
            if generation_result["success"] and generation_result["generated_code"]:
                syntax_valid = self.validate_syntax(generation_result["generated_code"])
            
            # Create result
            result = CodingTestResult(
                prompt_id=test_case["id"],
                category=test_case["category"],
                complexity=test_case["complexity"],
                prompt_text=test_case["prompt"],
                generated_code=generation_result["generated_code"],
                latency_ms=generation_result["latency_ms"],
                peak_memory_mb=generation_result["peak_memory_mb"],
                memory_before_mb=generation_result["memory_before_mb"],
                memory_after_mb=generation_result["memory_after_mb"],
                token_count_input=generation_result["token_count_input"],
                token_count_output=generation_result["token_count_output"],
                success=generation_result["success"],
                error_message=generation_result["error_message"],
                syntax_valid=syntax_valid
            )
            
            results.append(result)
            
            # Print progress
            status = "✓" if result.success else "✗"
            syntax_status = "✓" if syntax_valid else "✗" if syntax_valid is not None else "?"
            print(f"  {status} Generated | {syntax_status} Syntax | {result.latency_ms:.1f}ms | {result.peak_memory_mb:.1f}MB")
            
            # Memory cleanup
            gc.collect()
        
        return results
    
    def save_results(self, results: List[CodingTestResult], output_path: str = "validation/coding_results_cli.json"):
        """Save test results to JSON file."""
        output_file = Path(output_path)
        output_file.parent.mkdir(exist_ok=True)
        
        # Convert results to serializable format
        serializable_results = [asdict(result) for result in results]
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, indent=2, ensure_ascii=False)
        
        print(f"Results saved to: {output_file}")
    
    def print_summary(self, results: List[CodingTestResult]):
        """Print test summary statistics."""
        total_tests = len(results)
        successful_tests = sum(1 for r in results if r.success)
        syntax_valid_tests = sum(1 for r in results if r.syntax_valid)
        
        latencies = [r.latency_ms for r in results if r.success]
        peak_memories = [r.peak_memory_mb for r in results if r.success]
        
        avg_latency = sum(latencies) / len(latencies) if latencies else 0
        max_latency = max(latencies) if latencies else 0
        max_memory = max(peak_memories) if peak_memories else 0
        
        print("\n" + "="*60)
        print("CLI GGUF MODEL PERFORMANCE SUMMARY")
        print("="*60)
        print(f"Total Tests:           {total_tests}")
        print(f"Successful:            {successful_tests} ({successful_tests/total_tests*100:.1f}%)")
        print(f"Syntax Valid:          {syntax_valid_tests} ({syntax_valid_tests/total_tests*100:.1f}%)")
        print(f"Average Latency:       {avg_latency:.1f}ms")
        print(f"Max Latency:           {max_latency:.1f}ms")
        print(f"Peak Memory Usage:     {max_memory:.1f}MB")
        print("="*60)
        
        # Performance criteria check
        criteria_met = []
        criteria_met.append(("Latency < 200ms", avg_latency < 200))
        criteria_met.append(("Syntax Valid > 80%", syntax_valid_tests/total_tests > 0.8))
        criteria_met.append(("Peak RAM < 10GB", max_memory < 10240))
        
        print("\nPERFORMANCE CRITERIA:")
        for criterion, met in criteria_met:
            status = "✓ PASS" if met else "✗ FAIL"
            print(f"  {status}: {criterion}")
        
        all_criteria_met = all(met for _, met in criteria_met)
        final_status = "✓ ALL CRITERIA MET" if all_criteria_met else "✗ SOME CRITERIA FAILED"
        print(f"\nFINAL RESULT: {final_status}")


def main():
    """Main execution function."""
    model_path = "smollm-quantized/smollm-q4_K_M.gguf"
    cli_path = "llama.cpp/build/bin/llama-cli"
    
    if not Path(model_path).exists():
        print(f"ERROR: Model not found: {model_path}")
        sys.exit(1)
    
    if not Path(cli_path).exists():
        print(f"ERROR: CLI tool not found: {cli_path}")
        sys.exit(1)
    
    print(f"Using quantized GGUF model: {model_path}")
    
    # Initialize tester
    try:
        tester = CLICodingPerformanceTester(model_path, cli_path)
        
        # Run tests
        results = tester.run_test_suite()
        
        # Save and display results
        tester.save_results(results)
        tester.print_summary(results)
        
    except Exception as e:
        print(f"Error during testing: {e}")
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main() 