# Generated by SmolLM3-3B on 2025-01-01
"""
GGUF Memory Service Runner

Main entry point for the persistent service that integrates SmolLM GGUF model
with MemOS memory layer. This service loads the model once upon startup and
provides enhanced inference with long-term memory.
"""

import asyncio
import logging
import os
import signal
import sys
from contextlib import asynccontextmanager
from typing import Dict, Any, Optional

# Add MemOS to Python path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'MemOS', 'src'))

import uvicorn
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field

from config_loader import load_config
from gguf_memory_service import get_service_instance, shutdown_service

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Global service instance
service = None


@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    Manage the lifecycle of the GGUF Memory Service.
    """
    global service
    
    try:
        # Startup
        logger.info("Starting GGUF Memory Service...")
        service = await get_service_instance()
        logger.info("GGUF Memory Service started successfully")
        yield
    except Exception as e:
        logger.error(f"Failed to start service: {e}")
        raise
    finally:
        # Shutdown
        logger.info("Shutting down GGUF Memory Service...")
        await shutdown_service()
        logger.info("GGUF Memory Service shutdown complete")


# Load configuration
config = load_config()
service_config = config.get('service', {})
api_config = service_config.get('api', {})

# Create FastAPI app
app = FastAPI(
    title=api_config.get('title', 'MemOS GGUF Service'),
    description=api_config.get('description', 'Persistent service integrating MemOS with GGUF models'),
    version=api_config.get('version', '1.0.0'),
    lifespan=lifespan
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# Request/Response models
class ChatRequest(BaseModel):
    """Request model for chat endpoint."""
    query: str = Field(..., description="User query/prompt")
    user_id: Optional[str] = Field(None, description="User ID for memory context")
    include_memory: bool = Field(True, description="Whether to include memory in response")
    memory_top_k: Optional[int] = Field(None, description="Number of top memories to retrieve")


class ChatResponse(BaseModel):
    """Response model for chat endpoint."""
    response: str = Field(..., description="Generated response")
    query: str = Field(..., description="Original query")
    memory_enhanced: bool = Field(..., description="Whether memory was used")
    memories_used: list[Dict[str, Any]] = Field(..., description="Memories used in generation")
    inference_time_seconds: float = Field(..., description="Time taken for inference")
    timestamp: str = Field(..., description="Response timestamp")


class HealthResponse(BaseModel):
    """Response model for health check."""
    status: str = Field(..., description="Overall service status")
    service: Dict[str, Any] = Field(..., description="Service information")
    memos: Dict[str, Any] = Field(..., description="MemOS status")
    model: Dict[str, Any] = Field(..., description="Model status")
    config: Optional[Dict[str, Any]] = Field(None, description="Configuration info")


class ServiceStatusResponse(BaseModel):
    """Response model for service status."""
    service: Dict[str, Any] = Field(..., description="Service information")
    memos: Dict[str, Any] = Field(..., description="MemOS status")
    model: Dict[str, Any] = Field(..., description="Model status")
    config: Dict[str, Any] = Field(..., description="Configuration")


# API Endpoints

@app.post("/chat", response_model=ChatResponse, summary="Memory-aware chat with MemOS")
async def chat_with_memory(request: ChatRequest):
    """
    Perform memory-aware chat using MemOS for conversational memory management.
    
    This endpoint:
    1. Uses MemOS for automatic memory retrieval and management
    2. Maintains conversation history across multiple requests
    3. Generates responses using the GGUF model via MemOS
    4. Falls back to enhanced_inference if MemOS is unavailable
    """
    global service
    
    if not service or not service.is_healthy():
        raise HTTPException(status_code=503, detail="Service not available or unhealthy")
    
    try:
        # Use MemOS chat as primary method (with automatic fallback)
        result = await service.memos_chat(
            query=request.query,
            user_id=request.user_id,
            include_memory=request.include_memory,
            memory_top_k=request.memory_top_k
        )
        
        return ChatResponse(
            response=result['response'],
            query=result['query'],
            memory_enhanced=result['memory_enhanced'],
            memories_used=result['memories_used'],
            inference_time_seconds=result['inference_time_seconds'],
            timestamp=result['timestamp']
        )
        
    except Exception as e:
        logger.error(f"Chat endpoint error: {e}")
        raise HTTPException(status_code=500, detail=f"Chat generation failed: {str(e)}")


@app.get("/health", response_model=HealthResponse, summary="Service health check")
async def health_check():
    """
    Health check endpoint that verifies the operational status of the integrated service.
    
    Checks:
    - MemOS service status
    - GGUF model loading and responsiveness
    - Overall service health
    """
    global service
    
    try:
        if not service:
            return HealthResponse(
                status="unhealthy",
                service={"name": "GGUF Memory Service", "initialized": False},
                memos={"status": "not_initialized"},
                model={"type": "gguf", "loaded": False, "healthy": False}
            )
        
        status_info = service.get_service_status()
        is_healthy = service.is_healthy()
        
        return HealthResponse(
            status="healthy" if is_healthy else "unhealthy",
            service=status_info.get('service', {}),
            memos=status_info.get('memos', {}),
            model=status_info.get('model', {}),
            config=status_info.get('config', {})
        )
        
    except Exception as e:
        logger.error(f"Health check error: {e}")
        return HealthResponse(
            status="unhealthy",
            service={"name": "GGUF Memory Service", "error": str(e)},
            memos={"status": "error"},
            model={"type": "gguf", "loaded": False, "healthy": False}
        )


@app.get("/status", response_model=ServiceStatusResponse, summary="Detailed service status")
async def get_service_status():
    """
    Get detailed service status including configuration and model information.
    """
    global service
    
    if not service:
        raise HTTPException(status_code=503, detail="Service not initialized")
    
    try:
        status_info = service.get_service_status()
        return ServiceStatusResponse(
            service=status_info.get('service', {}),
            memos=status_info.get('memos', {}),
            model=status_info.get('model', {}),
            config=status_info.get('config', {})
        )
        
    except Exception as e:
        logger.error(f"Status endpoint error: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get service status: {str(e)}")


@app.get("/", summary="Service information")
async def root():
    """
    Root endpoint providing basic service information.
    """
    return {
        "service": "GGUF Memory Service",
        "description": "Persistent service integrating MemOS memory layer with GGUF models",
        "version": api_config.get('version', '1.0.0'),
        "endpoints": {
            "chat": "/chat",
            "health": "/health",
            "status": "/status",
            "docs": "/docs"
        }
    }


# Error handlers
@app.exception_handler(Exception)
async def global_exception_handler(request, exc):
    """
    Global exception handler for unhandled errors.
    """
    logger.error(f"Unhandled error in {request.url}: {exc}")
    return JSONResponse(
        status_code=500,
        content={
            "error": "Internal server error",
            "detail": str(exc),
            "path": str(request.url)
        }
    )


def setup_signal_handlers():
    """
    Setup signal handlers for graceful shutdown.
    """
    def signal_handler(signum, frame):
        logger.info(f"Received signal {signum}, initiating shutdown...")
        # FastAPI will handle shutdown through lifespan
        sys.exit(0)
    
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)


def main():
    """
    Main entry point for the GGUF Memory Service.
    """
    # Setup signal handlers
    setup_signal_handlers()
    
    # Get service configuration
    host = api_config.get('host', '0.0.0.0')
    port = api_config.get('port', 8000)
    
    # Set logging level from config
    log_level = config.get('logging', {}).get('level', 'INFO').lower()
    
    logger.info(f"Starting MemOS Service with SmolLM3 on {host}:{port}")
    logger.info(f"Model path: {config.get('model', {}).get('model_path', 'Not specified')}")
    logger.info(f"Memory retrieval enabled: {config.get('memory', {}).get('retrieval', {}).get('enabled', True)}")
    
    # Run the service
    uvicorn.run(
        app,
        host=host,
        port=port,
        log_level=log_level,
        access_log=True,
        reload=False  # Disable reload for production
    )


if __name__ == "__main__":
    main() 