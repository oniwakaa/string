# Generated by SmolLM3-3B on 2024-12-19
"""
Orchestrator module for managing the multi-agent workflow.

The ProjectManager class serves as the strategic brain that receives
high-level user requests, decomposes them into executable plans,
delegates tasks to the right agents, and assembles the final result.
"""

import asyncio
import json
import re
import sys
import os
import time
from typing import Dict, List, Any, Optional
from uuid import UUID

import httpx

from .base import BaseAgent, Task, Result
from .code_quality import CodeQualityAgent
from .web_researcher_optimized import PerformanceOptimizedWebResearcher
from .code_editor import CodeEditorAgent
from .specialists import ToolExecutorAgent

# Add the parent directory to path for imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from project_memory_manager import ProjectMemoryManager, MEMOS_AVAILABLE

# Add src to path for ModelManager import
src_path = os.path.join(os.path.dirname(__file__), '..', 'src')
if src_path not in sys.path:
    sys.path.insert(0, src_path)
from models.manager import model_manager


class CodeGeneratorAgent(BaseAgent):
    """
    Agent specialized in code generation and modification tasks.
    
    This agent uses the powerful google/gemma-3n-E4B-it model in quantized GGUF format
    for high-quality code generation with optimized memory usage on MacBook Air M4.
    """
    
    def __init__(self):
        super().__init__(
            name="Gemma3n_CodeGenerator",
            role="code_generator",
            model_name="gemma-3n-E4B-it"
        )
        
        # Gemma-specific configuration
        self.max_context_length = 8192  # Gemma 3n context window
        self.generation_config = {
            'max_tokens': 1024,
            'temperature': 0.3,
            'top_p': 0.9,
            'top_k': 40,
            'repeat_penalty': 1.1,
            'stop': ["<|endoftext|>", "<|im_end|>", "</code>"],
        }
        
        # Memory management for M4 MacBook
        self.llama_config = {
            'n_ctx': self.max_context_length,
            'n_batch': 512,
            'n_gpu_layers': -1,  # Use all available GPU layers on M4
            'use_mmap': True,  # Memory mapping for efficiency
            'use_mlock': False,  # Don't lock memory to allow swapping if needed
            'low_vram': True,  # Optimize for lower VRAM usage
            'verbose': False
        }
    
    def lazy_load_model(self):
        """
        Load the Gemma GGUF model using llama-cpp-python with optimizations for M4 MacBook.
        
        This method implements efficient lazy loading with memory management
        and error handling for out-of-memory situations.
        """
        if self.model is None:
            try:
                # Import llama-cpp-python
                from llama_cpp import Llama
                import os
                
                self.status = 'loading_model'
                print(f"🔄 Loading Gemma-3n model for {self.name}...")
                
                # Use ModelManager to get shared model instance (no direct loading)
                self.model = model_manager.get_model(self.model_name)
                if not self.model:
                    raise RuntimeError(f"ModelManager failed to load model: {self.model_name}")
                
                self.status = 'ready'
                print(f"✅ Gemma-3n model loaded successfully for {self.name}")
                print(f"📊 Model context length: {self.max_context_length}")
                
            except Exception as e:
                self.status = 'error'
                error_msg = f"❌ Failed to load Gemma-3n model for {self.name}: {str(e)}"
                print(error_msg)
                
                # Specific handling for memory errors
                if "memory" in str(e).lower() or "alloc" in str(e).lower():
                    print("💡 Try closing other applications to free up memory")
                    print("💡 Or consider using a smaller model variant")
                
                raise RuntimeError(error_msg)
    
    def _format_gemma_prompt(self, task_prompt: str, context: dict = None) -> str:
        """
        Format the prompt according to Gemma's chat template for optimal results.
        
        Gemma uses a specific format that includes system instructions and user messages
        properly formatted for code generation tasks.
        """
        # System message for code generation
        system_message = """You are an expert software developer and architect. Your task is to generate high-quality, production-ready code that follows best practices:

- Write clean, readable, and maintainable code
- Include comprehensive docstrings and comments
- Follow language-specific conventions (PEP 8 for Python, etc.)
- Implement proper error handling
- Use type hints where applicable
- Consider performance and memory efficiency
- Include usage examples when appropriate

Generate only the requested code without additional explanation unless specifically asked."""
        
        # Format user message with context
        user_message = task_prompt
        
        if context:
            # Filter out non-relevant context
            relevant_context = {}
            for key, value in context.items():
                if value and key not in ['user_id', 'original_request']:
                    relevant_context[key] = value
            
            if relevant_context:
                context_info = "\n\nAdditional Context:\n"
                for key, value in relevant_context.items():
                    # Handle different types of context data
                    if isinstance(value, dict) and 'answer' in value:
                        # This is output from CodebaseExpertAgent
                        context_info += f"- Codebase Information: {value['answer']}\n"
                    elif isinstance(value, str) and len(value) > 50:
                        # Long string content, truncate if needed
                        truncated = value[:500] + "..." if len(value) > 500 else value
                        context_info += f"- {key}: {truncated}\n"
                    else:
                        context_info += f"- {key}: {value}\n"
                
                user_message += context_info
        
        # Gemma chat template format
        formatted_prompt = f"""<bos><start_of_turn>system
{system_message}<end_of_turn>
<start_of_turn>user
{user_message}<end_of_turn>
<start_of_turn>model
"""
        
        return formatted_prompt
    
    def _analyze_for_next_action(self, prompt: str, generated_code: str, context: dict) -> dict:
        """
        Analyze the prompt and generated code to suggest next actions.
        
        Args:
            prompt: Original user prompt
            generated_code: Generated code output
            context: Task context
            
        Returns:
            Metadata dictionary with next_action suggestions
        """
        metadata = {}
        prompt_lower = prompt.lower()
        
        # Check if this looks like code that should be saved to a file
        if any(keyword in prompt_lower for keyword in [
            'create', 'generate', 'write', 'implement', 'build', 'develop'
        ]) and generated_code.strip():
            
            # Try to detect if this is a complete file/script
            code_lines = generated_code.strip().split('\n')
            
            # Check for file-like indicators
            is_complete_file = any([
                generated_code.strip().startswith(('#!/', 'from ', 'import ', 'class ', 'def ', 'function ')),
                'if __name__ == "__main__"' in generated_code,
                len(code_lines) > 5,  # Multi-line code
                any(line.strip().startswith(('class ', 'def ', 'function ')) for line in code_lines)
            ])
            
            if is_complete_file:
                # Suggest file creation
                
                # Try to extract filename from prompt
                file_path = None
                
                # Look for filename patterns in prompt
                import re
                filename_patterns = [
                    r'(?:file|script|module).*?["\']([^"\']+\.py)["\']',
                    r'(?:called|named).*?["\']([^"\']+\.py)["\']',
                    r'save.*?(?:to|as).*?["\']([^"\']+\.py)["\']',
                    r'create.*?["\']([^"\']+\.py)["\']'
                ]
                
                for pattern in filename_patterns:
                    match = re.search(pattern, prompt, re.IGNORECASE)
                    if match:
                        file_path = match.group(1)
                        break
                
                # If no specific filename found, generate one based on content
                if not file_path:
                    if 'class ' in generated_code:
                        # Extract class name for filename
                        class_match = re.search(r'class\s+(\w+)', generated_code)
                        if class_match:
                            class_name = class_match.group(1)
                            file_path = f"{class_name.lower()}.py"
                        else:
                            file_path = "generated_class.py"
                    elif 'def ' in generated_code and 'class ' not in generated_code:
                        # Extract function name for filename
                        func_match = re.search(r'def\s+(\w+)', generated_code)
                        if func_match:
                            func_name = func_match.group(1)
                            file_path = f"{func_name}.py"
                        else:
                            file_path = "generated_functions.py"
                    else:
                        file_path = "generated_script.py"
                
                # Create next_action for file creation
                metadata['next_action'] = {
                    'tool': 'create_file',
                    'args': {
                        'file_path': file_path,
                        'content': generated_code
                    }
                }
                
                print(f"💡 Suggesting file creation: {file_path}")
        
        # Check for other action patterns
        elif any(keyword in prompt_lower for keyword in [
            'test', 'run', 'execute', 'check'
        ]) and generated_code.strip():
            # Suggest running the code if it looks executable
            if any(pattern in generated_code for pattern in [
                'if __name__', 'main()', 'print(', 'def test'
            ]):
                metadata['next_action'] = {
                    'tool': 'run_terminal_command',
                    'args': {
                        'command': ['python3', '-c', generated_code]
                    }
                }
                print(f"💡 Suggesting code execution")
        
        return metadata
    
    async def execute(self, task: Task) -> Result:
        """
        Execute code generation tasks using the Gemma-3n model with KVCache optimization.
        
        This method handles model loading, prompt formatting, generation,
        KVCache integration for performance, and comprehensive error handling.
        """
        try:
            # Lazy load model on first use
            if self.model is None:
                self.lazy_load_model()
            
            # Get MemCube for KVCache if available
            mem_cube = None
            cached_kv_states = None
            user_id = task.context.get('user_id', 'default_user')
            project_id = task.context.get('project_id', 'default')
            
            # Try to get MemCube from the project manager (via global reference)
            try:
                # This would be injected by the ProjectManager during execution
                if hasattr(task, '_mem_cube_instance'):
                    mem_cube = task._mem_cube_instance
                    print(f"🧠 Using MemCube for KVCache optimization")
                    
                    # Retrieve cached KV states for this user session
                    if hasattr(mem_cube, 'act_mem') and mem_cube.act_mem:
                        try:
                            cache_result = mem_cube.act_mem.retrieve(query=f"user:{user_id}")
                            if cache_result and 'past_key_values' in cache_result:
                                cached_kv_states = cache_result['past_key_values']
                                print(f"🚀 Retrieved KV cache with {len(cached_kv_states) if cached_kv_states else 0} layers")
                        except Exception as e:
                            print(f"⚠️ KV cache retrieval failed: {e}")
            except Exception as e:
                print(f"⚠️ MemCube access failed: {e}")
            
            # Format prompt using Gemma chat template
            formatted_prompt = self._format_gemma_prompt(task.prompt, task.context)
            
            # Check prompt length to avoid context overflow
            prompt_length = len(formatted_prompt)
            if prompt_length > self.max_context_length * 0.8:  # Use 80% of context as safety margin
                # Truncate context if needed
                truncated_prompt = formatted_prompt[:int(self.max_context_length * 0.6)]
                formatted_prompt = truncated_prompt + "\n<start_of_turn>model\n"
                print(f"⚠️ Prompt truncated to fit context window")
            
            # Prepare generation kwargs with potential KV cache
            generation_kwargs = self.generation_config.copy()
            generation_kwargs['echo'] = False  # Don't include prompt in output
            
            # Add cached states if available (for llama-cpp-python this would be handled differently)
            if cached_kv_states:
                # Note: llama-cpp-python handles caching internally via the model state
                # We'll implement a session-based approach
                print(f"🔥 Using cached states for accelerated generation")
            
            # Generate response with Gemma
            response = self.model(
                formatted_prompt,
                **generation_kwargs
            )
            
            # Extract generated text
            generated_text = response['choices'][0]['text'].strip()
            
            # Store new KV states in activation memory if MemCube is available
            if mem_cube and hasattr(mem_cube, 'act_mem') and mem_cube.act_mem:
                try:
                    # For llama-cpp-python, we can't directly access past_key_values
                    # but we can store session state information
                    session_data = {
                        'prompt_hash': hash(formatted_prompt),
                        'response_length': len(generated_text),
                        'timestamp': time.time(),
                        'context_length': prompt_length,
                        'model_state': 'cached'  # Placeholder for actual state
                    }
                    
                    # Store in activation memory for future use
                    mem_cube.act_mem.add(
                        key=f"user:{user_id}",
                        value={
                            'session_data': session_data,
                            'past_key_values': None,  # Would contain actual KV states in full implementation
                            'context_window': formatted_prompt[-1000:] if len(formatted_prompt) > 1000 else formatted_prompt
                        }
                    )
                    print(f"💾 Stored session state in KV cache")
                except Exception as e:
                    print(f"⚠️ Failed to store KV cache: {e}")
            
            # Clean up the response (remove any template artifacts)
            if generated_text.endswith('<end_of_turn>'):
                generated_text = generated_text[:-13].strip()
            
            # Analyze the prompt and output to suggest next actions
            metadata = self._analyze_for_next_action(task.prompt, generated_text, task.context)
            
            return Result(
                task_id=task.task_id,
                status="success",
                output=generated_text,
                metadata=metadata
            )
            
        except MemoryError as e:
            return Result(
                task_id=task.task_id,
                status="failure",
                output="",
                error_message=f"Out of memory during code generation. Try reducing context or using a smaller model: {str(e)}"
            )
        
        except Exception as e:
            # Log the full error for debugging
            error_msg = f"Code generation failed: {str(e)}"
            print(f"❌ {self.name} error: {error_msg}")
            
            return Result(
                task_id=task.task_id,
                status="failure",
                output="",
                error_message=error_msg
            )
    
    def get_model_info(self) -> dict:
        """
        Get information about the loaded model.
        """
        return {
            'model_name': 'google/gemma-3n-E4B-it',
            'quantization': 'Q4_1',
            'format': 'GGUF',
            'context_length': self.max_context_length,
            'loaded': self.model is not None,
            'status': self.status,
            'memory_optimized': True,
            'platform': 'MacBook Air M4'
        }


# Note: CodeAnalyzerAgent is now replaced by the advanced CodeQualityAgent
# The CodeQualityAgent implements a three-phase polyglot architecture:
# - Phase 0: Language Detection using Pygments
# - Phase 1: Dynamic Linter Selection (static analysis)  
# - Phase 2: Qualitative LLM Review using Qwen3-1.7B


class DocumentationAgent(BaseAgent):
    """
    Agent specialized in documentation generation and technical writing.
    """
    
    def __init__(self):
        super().__init__(
            name="SmolLM_DocumentationAgent",
            role="documentation",
            model_name="SmolLM3-3B"
        )
    
    async def execute(self, task: Task) -> Result:
        """
        Execute documentation tasks.
        """
        try:
            if self.model is None:
                self.lazy_load_model()
            
            system_prompt = """You are a technical writer. Create clear, comprehensive documentation that includes:
1. Purpose and functionality description
2. Usage examples
3. API documentation if applicable
4. Configuration options
5. Best practices and guidelines
Write in a clear, professional style."""
            
            full_prompt = f"{system_prompt}\n\nTask: {task.prompt}"
            
            if task.context:
                context_info = "\n\nContent to document:\n" + "\n".join([f"{k}:\n{v}" for k, v in task.context.items()])
                full_prompt += context_info
            
            response = self.generate_response(full_prompt, max_new_tokens=1024, temperature=0.4)
            
            return Result(
                task_id=task.task_id,
                status="success",
                output=response
            )
            
        except Exception as e:
            return Result(
                task_id=task.task_id,
                status="failure",
                output="",
                error_message=f"Documentation generation failed: {str(e)}"
            )


class CodebaseExpertAgent(BaseAgent):
    """
    Agent specialized in codebase knowledge retrieval using the existing RAG system.
    
    This is a lightweight, non-generative agent that interfaces with the /chat endpoint
    to leverage the powerful MemOS memory and RAG system already built.
    """
    
    def __init__(self, service_host: str = "localhost", service_port: int = 8000):
        """
        Initialize the CodebaseExpertAgent.
        
        Args:
            service_host: Host where the FastAPI service is running
            service_port: Port where the FastAPI service is running
        """
        super().__init__(
            name="SmolLM_CodebaseExpert",
            role="codebase_expert",
            model_name=None  # No local model loading required
        )
        
        # Service URL configuration
        self.service_base_url = f"http://{service_host}:{service_port}"
        self.chat_endpoint = f"{self.service_base_url}/chat"
        
        # HTTP client for async requests
        self.http_client = None
        
        # Mark as ready since no model loading is required
        self.status = 'ready'
    
    def lazy_load_model(self):
        """
        Override lazy_load_model since this agent doesn't use local models.
        Initialize HTTP client instead.
        """
        if self.http_client is None:
            self.http_client = httpx.AsyncClient(
                timeout=httpx.Timeout(30.0),  # 30 second timeout
                limits=httpx.Limits(max_connections=10)
            )
            self.status = 'ready'
    
    async def execute(self, task: Task) -> Result:
        """
        Execute codebase knowledge retrieval by calling the /chat endpoint.
        
        This method makes an asynchronous HTTP call to the existing /chat endpoint
        to leverage the MemOS memory and RAG features for codebase understanding.
        """
        try:
            # Ensure HTTP client is initialized
            if self.http_client is None:
                self.lazy_load_model()
            
            # Extract user_id from task context or use default
            user_id = task.context.get('user_id', 'default_user')
            
            # Prepare the payload for the /chat endpoint
            chat_payload = {
                "query": task.prompt,
                "user_id": user_id,
                "include_memory": True,  # Enable MemOS memory and RAG features
                "memory_top_k": 5  # Number of relevant memories to retrieve
            }
            
            # Add context information to the query if available
            if task.context and len(task.context) > 1:  # More than just user_id
                context_info = []
                for key, value in task.context.items():
                    if key != 'user_id' and value:
                        context_info.append(f"- {key}: {value}")
                
                if context_info:
                    enhanced_query = f"{task.prompt}\n\nContext:\n" + "\n".join(context_info)
                    chat_payload["query"] = enhanced_query
            
            # Make the asynchronous HTTP request
            response = await self.http_client.post(
                self.chat_endpoint,
                json=chat_payload,
                headers={"Content-Type": "application/json"}
            )
            
            # Check if the request was successful
            if response.status_code == 200:
                response_data = response.json()
                
                # Extract the relevant information from the chat response
                chat_response = response_data.get('response', '')
                memories_used = response_data.get('memories_used', [])
                memory_enhanced = response_data.get('memory_enhanced', False)
                
                # Create a comprehensive output combining the response and metadata
                output = {
                    'answer': chat_response,
                    'memory_enhanced': memory_enhanced,
                    'memories_count': len(memories_used),
                    'source': 'rag_system'
                }
                
                return Result(
                    task_id=task.task_id,
                    status="success",
                    output=output
                )
            
            else:
                # Handle HTTP error responses
                error_detail = f"HTTP {response.status_code}: {response.text}"
                return Result(
                    task_id=task.task_id,
                    status="failure",
                    output="",
                    error_message=f"RAG system call failed: {error_detail}"
                )
                
        except httpx.TimeoutException:
            return Result(
                task_id=task.task_id,
                status="failure",
                output="",
                error_message="RAG system call timed out after 30 seconds"
            )
        
        except httpx.ConnectError:
            return Result(
                task_id=task.task_id,
                status="failure",
                output="",
                error_message=f"Could not connect to RAG service at {self.chat_endpoint}"
            )
        
        except httpx.HTTPError as e:
            return Result(
                task_id=task.task_id,
                status="failure",
                output="",
                error_message=f"HTTP error during RAG system call: {str(e)}"
            )
        
        except Exception as e:
            return Result(
                task_id=task.task_id,
                status="failure",
                output="",
                error_message=f"Unexpected error in CodebaseExpertAgent: {str(e)}"
            )
    
    async def cleanup(self):
        """
        Clean up resources by closing the HTTP client.
        """
        if self.http_client:
            await self.http_client.aclose()
            self.http_client = None
    
    def __del__(self):
        """
        Ensure HTTP client is closed when the agent is destroyed.
        """
        if self.http_client:
            # Note: This is not ideal for async cleanup, but serves as a fallback
            try:
                asyncio.create_task(self.cleanup())
            except:
                pass


class ProjectManager:
    """
    The strategic brain of the agentic system.
    
    This class receives high-level user requests, decomposes them into
    executable plans, delegates tasks to the appropriate agents, and
    assembles the final result.
    """
    
    def __init__(self, service_host: str = "localhost", service_port: int = 8000):
        """
        Initialize the ProjectManager with a registry of available agents.
        
        Creates a single instance of each agent class and stores them in a
        dictionary using their role as the key for efficient lookup.
        
        Args:
            service_host: Host where the FastAPI service is running
            service_port: Port where the FastAPI service is running
        """
        # Agent registry - instantiate each agent once
        self.agents: Dict[str, BaseAgent] = {
            'code_generator': CodeGeneratorAgent(),
            'code_quality_analyzer': CodeQualityAgent(),
            'code_editor': CodeEditorAgent(),
            'documentation': DocumentationAgent(),
            'codebase_expert': CodebaseExpertAgent(service_host, service_port),
            'web_researcher': PerformanceOptimizedWebResearcher(),
            'tool_executor': ToolExecutorAgent()
        }
        
        # Task tracking
        self.active_tasks: Dict[UUID, Task] = {}
        self.completed_results: Dict[UUID, Result] = {}
        
        # MemCube Registry for dynamic lifecycle management
        self.active_mem_cubes: Dict[str, Any] = {}  # {composite_project_id: MemCube instance}
        self.project_memory_manager: Optional[ProjectMemoryManager] = None
        self.mos_instance: Optional[Any] = None
        
        # Service configuration for MemOS integration
        self.service_host = service_host
        self.service_port = service_port
        
        # Initialize project memory manager if MemOS is available
        if MEMOS_AVAILABLE:
            self.project_memory_manager = ProjectMemoryManager()
            print("🧠 ProjectMemoryManager initialized for dynamic MemCube lifecycle")
        else:
            print("⚠️ MemOS not available - dynamic MemCube features disabled")
    
    async def _get_or_create_mem_cube(self, user_id: str, project_id: str) -> Optional[Any]:
        """
        Get or create a project-specific MemCube on-demand.
        
        This method implements the "Get or Create" logic for dynamic MemCube lifecycle:
        1. Generates the unique composite project ID
        2. Checks if the MemCube is already active in the registry
        3. If not, creates a new MemCube instance and registers it
        4. Returns the MemCube instance for immediate use
        
        Args:
            user_id: User identifier
            project_id: Project identifier
            
        Returns:
            MemCube instance if successful, None if MemOS is not available
        """
        # Check if MemOS is available
        if not MEMOS_AVAILABLE or not self.project_memory_manager:
            print("⚠️ MemOS not available - MemCube operations disabled")
            return None
        
        try:
            # Generate the composite project-specific ID
            composite_id = f"{user_id}_{project_id}"
            cube_id = f"{user_id}_{project_id}_codebase_cube"
            
            # Check if this MemCube is already active
            if composite_id in self.active_mem_cubes:
                print(f"♻️ Using existing MemCube: {cube_id}")
                return self.active_mem_cubes[composite_id]
            
            # If we don't have a MemOS instance yet, try to get it
            if not self.mos_instance:
                try:
                    # Import and get the MemOS instance from the service
                    import httpx
                    
                    # Try to get MemOS instance from the global service
                    # This requires coordination with the main service
                    async with httpx.AsyncClient() as client:
                        health_response = await client.get(f"http://{self.service_host}:{self.service_port}/health")
                        if health_response.status_code == 200:
                            print("✅ MemOS service is available")
                            # For now, we'll create our own MemOS instance
                            # In production, this would be coordinated with the main service
                        
                except Exception as e:
                    print(f"⚠️ Could not connect to MemOS service: {e}")
                    return None
            
            # Set the MemOS instance in the project memory manager
            if self.mos_instance:
                self.project_memory_manager.set_mos_instance(self.mos_instance)
            
            # Get or create the project-specific cube
            cube_id = self.project_memory_manager.get_or_create_project_cube(user_id, project_id)
            
            if cube_id:
                # Get cube information
                cube_info = self.project_memory_manager.get_project_cube_info(user_id, project_id)
                
                if cube_info:
                    # Store in active registry (using the cube info as a placeholder)
                    self.active_mem_cubes[composite_id] = cube_info
                    
                    print(f"🧠 MemCube ready: {cube_id}")
                    print(f"📍 Storage: {cube_info.get('storage_path', 'N/A')}")
                    print(f"🔍 Collection: {cube_info.get('collection_name', 'N/A')}")
                    
                    return cube_info
                else:
                    print(f"❌ Failed to get cube info for: {cube_id}")
                    return None
            else:
                print(f"❌ Failed to create or get project cube for: {user_id}_{project_id}")
                return None
                
        except Exception as e:
            print(f"❌ Error in MemCube lifecycle management: {e}")
            return None
    
    def set_mos_instance(self, mos_instance: Any) -> None:
        """
        Set the MemOS instance for dynamic MemCube management.
        
        This method should be called by the main service to provide
        the MemOS instance to the ProjectManager.
        
        Args:
            mos_instance: The initialized MemOS instance
        """
        self.mos_instance = mos_instance
        if self.project_memory_manager:
            self.project_memory_manager.set_mos_instance(mos_instance)
            print("🔗 MemOS instance connected to ProjectManager")
    
    def get_active_mem_cubes(self) -> Dict[str, Any]:
        """
        Get information about currently active MemCubes.
        
        Returns:
            Dict mapping composite project IDs to cube information
        """
        return self.active_mem_cubes.copy()
    
    def cleanup_mem_cube(self, user_id: str, project_id: str) -> bool:
        """
        Clean up a specific MemCube from the active registry.
        
        Args:
            user_id: User identifier
            project_id: Project identifier
            
        Returns:
            bool: True if cleanup was successful
        """
        composite_id = f"{user_id}_{project_id}"
        if composite_id in self.active_mem_cubes:
            del self.active_mem_cubes[composite_id]
            print(f"🧹 Cleaned up MemCube: {composite_id}")
            return True
        return False

    async def handle_request(self, user_prompt: str, user_id: str = "default_user", project_id: str = "default") -> dict:
        """
        Main entry point for handling user requests.
        
        Args:
            user_prompt: The user's high-level request
            user_id: User identifier for memory context
            project_id: Project identifier for memory isolation
            
        Returns:
            dict: Formatted response with the final result
        """
        try:
            # Step 0: Initialize project-specific memory environment
            mem_cube = await self._get_or_create_mem_cube(user_id, project_id)
            if mem_cube:
                print(f"✅ Memory environment ready for project: {user_id}_{project_id}")
            
            # Step 1: Decompose the prompt into a plan
            plan = await self._decompose_prompt(user_prompt, user_id)
            
            if not plan:
                return {
                    "status": "error",
                    "message": "Could not create a valid execution plan from the prompt",
                    "result": None
                }
            
            # Step 2: Create task graph with memory context
            tasks = self._create_task_graph(plan, user_prompt, user_id, project_id)
            
            # Step 2.5: KVCache Extraction - Create stable context for caching
            if mem_cube and hasattr(mem_cube, 'act_mem') and mem_cube.act_mem:
                try:
                    # Define stable system context for caching
                    stable_context = f"""System Prompt: You are an expert software engineer working on project '{project_id}' for user '{user_id}'. 
Always follow best practices, write clean and maintainable code, and provide detailed explanations.
Current working context: {user_prompt[:200]}..."""
                    
                    # Extract and cache this stable context
                    kv_item_to_cache = mem_cube.act_mem.extract(stable_context)
                    if kv_item_to_cache:
                        mem_cube.act_mem.add([kv_item_to_cache])
                        print(f"🚀 KVCache: Extracted and cached stable context ({len(stable_context)} chars)")
                    else:
                        print("⚠️ KVCache: Failed to extract stable context")
                except Exception as e:
                    print(f"⚠️ KVCache extraction failed: {e}")
            
            # Step 3: Execute tasks asynchronously
            results = await self._execute_task_graph(tasks)
            
            # Step 4: Assemble final result
            final_result = self._assemble_final_result(results, tasks)
            
            return {
                "status": "success",
                "message": "Task completed successfully",
                "result": final_result,
                "execution_plan": plan,
                "tasks_executed": len(tasks),
                "memory_cube_used": f"{user_id}_{project_id}_codebase_cube" if mem_cube else None
            }
            
        except Exception as e:
            return {
                "status": "error",
                "message": f"Execution failed: {str(e)}",
                "result": None
            }
    
    async def _decompose_prompt(self, user_prompt: str, user_id: str = "default_user") -> List[Dict[str, Any]]:
        """
        Decompose user prompt into an executable plan using model-based intent classification.
        
        This method uses the GemmaIntentClassifier to understand the semantic meaning
        of the prompt and route it to the appropriate agents with proper workflows.
        
        Args:
            user_prompt: The user's input prompt
            user_id: User identifier for context
            
        Returns:
            List of plan steps with agent roles and dependencies
        """
        try:
            # Initialize the intent classifier
            from src.inference import get_default_classifier
            classifier = get_default_classifier()
            
            # Classify the user's intent
            context = {"user_id": user_id}
            classification = classifier.classify(user_prompt, context)
            
            print(f"🎯 Intent Classification: {classification.primary_intent} (confidence: {classification.confidence:.2f})")
            if classification.secondary_intents:
                secondary_str = ", ".join([f"{intent}({score:.2f})" for intent, score in classification.secondary_intents])
                print(f"   Secondary intents: {secondary_str}")
            
            # If confidence is too low, use fallback intent
            if not classification.meets_threshold(0.5):
                print("⚠️ Low confidence classification, using general fallback")
                classification.primary_intent = 'general_query'
                classification.confidence = 0.3
            
            # Build plan based on classification
            plan = []
            
            # Get the primary agent for the intent
            primary_agent = classifier.get_agent_for_intent(classification.primary_intent)
            if not primary_agent:
                print(f"⚠️ No agent found for intent '{classification.primary_intent}', using fallback")
                primary_agent = "codebase_expert"
            
            # Check if we have a workflow to follow
            if classification.workflow:
                # Use the predefined workflow
                workflow_steps = classification.workflow.get('workflow', [])
                for step_config in workflow_steps:
                    plan.append({
                        "step": step_config['step'],
                        "agent_role": step_config['agent'],
                        "prompt": user_prompt if step_config['step'] == 1 else f"Based on previous results, {user_prompt}",
                        "dependencies": step_config.get('dependencies', [])
                    })
            else:
                # Build plan based on intent and context modifiers
                step_counter = 1
                
                # Check if we need to add context retrieval first
                needs_context = any(mod in ['prepend_codebase_query', 'add_context_retrieval'] 
                                  for mod in classification.context_modifiers)
                
                # Special handling for code_editing intent
                if classification.primary_intent == "code_editing":
                    # Code editing always needs context unless code is provided
                    if needs_context or "file" in user_prompt.lower() or "function" in user_prompt.lower():
                        plan.append({
                            "step": step_counter,
                            "agent_role": "codebase_expert",
                            "prompt": f"Find and retrieve the code to be modified for: {user_prompt}",
                            "dependencies": []
                        })
                        step_counter += 1
                        
                        plan.append({
                            "step": step_counter,
                            "agent_role": primary_agent,
                            "prompt": user_prompt,
                            "dependencies": [step_counter - 1]
                        })
                        step_counter += 1
                    else:
                        # Direct editing
                        plan.append({
                            "step": step_counter,
                            "agent_role": primary_agent,
                            "prompt": user_prompt,
                            "dependencies": []
                        })
                        step_counter += 1
                
                # Handle other intents with context requirements
                elif needs_context and classification.primary_intent in ["code_generation", "code_analysis", "documentation"]:
                    # Add context retrieval step
                    plan.append({
                        "step": step_counter,
                        "agent_role": "codebase_expert",
                        "prompt": f"Search codebase for relevant information: {user_prompt}",
                        "dependencies": []
                    })
                    step_counter += 1
                    
                    # Add primary task with dependency
                    plan.append({
                        "step": step_counter,
                        "agent_role": primary_agent,
                        "prompt": user_prompt,
                        "dependencies": [step_counter - 1]
                    })
                    step_counter += 1
                else:
                    # Direct execution of primary intent
                    plan.append({
                        "step": step_counter,
                        "agent_role": primary_agent,
                        "prompt": user_prompt,
                        "dependencies": []
                    })
                    step_counter += 1
                
                # Process secondary intents
                for secondary_intent, confidence in classification.secondary_intents:
                    if confidence > 0.5:  # Only process high-confidence secondary intents
                        secondary_agent = classifier.get_agent_for_intent(secondary_intent)
                        if secondary_agent and secondary_agent != primary_agent:
                            # Determine appropriate prompt for secondary task
                            if secondary_intent == "code_analysis":
                                secondary_prompt = "Analyze the generated code for quality, best practices, and potential improvements"
                            elif secondary_intent == "documentation":
                                secondary_prompt = "Create comprehensive documentation for the code, including usage examples"
                            elif secondary_intent == "apply_fixes":
                                secondary_prompt = "Apply the recommended fixes and improvements from the analysis"
                            else:
                                secondary_prompt = f"Process the results for: {user_prompt}"
                            
                            plan.append({
                                "step": step_counter,
                                "agent_role": secondary_agent,
                                "prompt": secondary_prompt,
                                "dependencies": [step_counter - 1]
                            })
                            step_counter += 1
            
            # Log the execution plan
            print(f"📋 Execution plan created with {len(plan)} steps:")
            for step in plan:
                deps = f" (depends on: {step['dependencies']})" if step['dependencies'] else ""
                print(f"   Step {step['step']}: {step['agent_role']}{deps}")
            
            return plan
            
        except Exception as e:
            print(f"❌ Model-based routing failed: {e}")
            print("⚠️ Using general fallback routing")
            # Fall back to general routing
            return [{
                "step": 1,
                "agent_role": "codebase_expert",
                "prompt": f"Search codebase for relevant information: {user_prompt}",
                "dependencies": []
            }, {
                "step": 2,
                "agent_role": "code_generator",
                "prompt": user_prompt,
                "dependencies": [1]
            }]
    
    
    def _create_task_graph(self, plan: List[Dict[str, Any]], original_prompt: str, user_id: str = "default_user", project_id: str = "default") -> List[Task]:
        """
        Create a graph of Task objects based on the execution plan.
        """
        tasks = []
        task_mapping = {}  # step -> task_id mapping
        
        for step_info in plan:
            task = Task(
                prompt=step_info["prompt"],
                context={
                    "original_request": original_prompt,
                    "user_id": user_id,
                    "project_id": project_id
                },
                dependencies=[]
            )
            
            # Resolve dependencies
            for dep_step in step_info["dependencies"]:
                if dep_step in task_mapping:
                    task.dependencies.append(task_mapping[dep_step])
            
            tasks.append(task)
            task_mapping[step_info["step"]] = task.task_id
            
            # Store agent role mapping for execution
            self.active_tasks[task.task_id] = {
                "task": task,
                "agent_role": step_info["agent_role"]
            }
        
        return tasks
    
    async def _execute_task_graph(self, tasks: List[Task]) -> Dict[UUID, Result]:
        """
        Execute the task graph asynchronously, respecting dependencies.
        """
        results = {}
        remaining_tasks = {task.task_id: task for task in tasks}
        executing_tasks = set()
        
        while remaining_tasks or executing_tasks:
            # Find tasks ready to execute (no unmet dependencies)
            ready_tasks = []
            for task_id, task in remaining_tasks.items():
                if all(dep_id in results and results[dep_id].status == "success" 
                       for dep_id in task.dependencies):
                    ready_tasks.append(task_id)
            
            # Execute ready tasks in parallel
            if ready_tasks:
                execution_coroutines = []
                for task_id in ready_tasks:
                    task = remaining_tasks[task_id]
                    agent_role = self.active_tasks[task_id]["agent_role"]
                    
                    # Add context from dependency results
                    for dep_id in task.dependencies:
                        if dep_id in results:
                            task.context[f"dependency_{dep_id}"] = results[dep_id].output
                    
                    # Enrich task context with memory tier information before execution
                    await self._enrich_task_context(task, agent_role)
                    
                    # Special handling for code editor tasks
                    if agent_role == "code_editor":
                        task.context = self._prepare_code_editor_context(task, results)
                    
                    # Execute task
                    execution_coroutines.append(self._execute_single_task(task, agent_role))
                    executing_tasks.add(task_id)
                    del remaining_tasks[task_id]
                
                # Wait for all parallel executions to complete
                task_results = await asyncio.gather(*execution_coroutines, return_exceptions=True)
                
                for result in task_results:
                    if isinstance(result, Result):
                        results[result.task_id] = result
                        executing_tasks.discard(result.task_id)
                        
                        # Check for next_action in result metadata and create follow-up tool tasks
                        next_action_task = self._handle_next_action(result, tasks)
                        if next_action_task:
                            # Add the new tool execution task to remaining tasks
                            remaining_tasks[next_action_task.task_id] = next_action_task
                            
                    elif isinstance(result, Exception):
                        # Handle execution exceptions
                        print(f"Task execution failed: {str(result)}")
            
            # Small delay to prevent busy waiting
            if not ready_tasks and remaining_tasks:
                await asyncio.sleep(0.1)
        
        return results
    
    async def _enrich_task_context(self, task: Task, agent_role: str) -> None:
        """
        Enrich task context with information from all memory tiers before agent execution.
        
        This method queries:
        1. TextualMemory (RAG) for relevant code context
        2. ParametricMemory for project-specific preferences and guidelines
        3. ActivationMemory for potential cached states (handled in _execute_single_task)
        
        Args:
            task: The task to enrich with context
            agent_role: The role of the agent that will execute the task
        """
        try:
            # Only enrich context for agents that benefit from memory context
            if agent_role not in ['code_generator', 'code_editor', 'codebase_expert']:
                return
            
            # Add KVCache injection for code generator tasks
            if agent_role == 'code_generator':
                mem_cube_key = f"{task.context.get('user_id', 'default_user')}_{task.context.get('project_id', 'default')}"
                if mem_cube_key in self.active_mem_cubes:
                    mem_cube = self.active_mem_cubes[mem_cube_key]
                    if hasattr(mem_cube, 'act_mem') and mem_cube.act_mem:
                        try:
                            # Retrieve all cached KV items for injection
                            cached_items = mem_cube.act_mem.get_all()
                            if cached_items:
                                # Attach MemCube to task for agent access
                                task._mem_cube_instance = mem_cube
                                print(f"🚀 KVCache: Injected {len(cached_items)} cached items for {agent_role}")
                        except Exception as e:
                            print(f"⚠️ KVCache injection failed: {e}")
            
            user_id = task.context.get('user_id', 'default_user')
            project_id = task.context.get('project_id', 'default')
            
            print(f"🧠 Enriching {agent_role} task context for project {project_id}")
            
            # 1. Query ParametricMemory for project preferences and guidelines
            if self.project_memory_manager:
                try:
                    # Get all project preferences
                    preferences = self.project_memory_manager.get_project_preferences(
                        user_id, project_id
                    )
                    
                    if preferences:
                        # Add structured preferences to task context
                        task.context['project_preferences'] = preferences
                        
                        # Format preferences for easy agent consumption
                        formatted_preferences = self.project_memory_manager.format_preferences_for_prompt(
                            user_id, project_id
                        )
                        
                        if formatted_preferences:
                            task.context['project_guidelines'] = formatted_preferences
                            print(f"📋 Added {len(preferences)} preference categories to task context")
                        
                except Exception as e:
                    print(f"⚠️ Failed to retrieve project preferences: {e}")
            
            # 2. Query TextualMemory (RAG) for relevant code context
            # This provides codebase knowledge that complements the preferences
            if hasattr(self, 'mos_instance') and self.mos_instance:
                try:
                    # Search for relevant code context based on the task prompt
                    search_query = task.prompt[:200]  # Use first 200 chars as search query
                    
                    # Use project-specific search if project memory manager is available
                    if self.project_memory_manager:
                        search_result = self.project_memory_manager.search_project_memory(
                            user_id, project_id, search_query
                        )
                    else:
                        # Fallback to general search
                        search_result = self.mos_instance.search(
                            query=search_query, 
                            user_id=user_id
                        )
                    
                    if search_result and search_result.get('text_mem'):
                        # Extract relevant code memories
                        code_memories = []
                        for cube_result in search_result['text_mem']:
                            memories = cube_result.get('memories', [])[:3]  # Top 3 relevant
                            for memory in memories:
                                code_memories.append({
                                    'content': memory.memory[:500],  # Truncate for context
                                    'relevance': getattr(memory, 'score', 0.0)
                                })
                        
                        if code_memories:
                            task.context['code_context'] = code_memories
                            print(f"🔍 Added {len(code_memories)} code memories to task context")
                            
                except Exception as e:
                    print(f"⚠️ Failed to retrieve code context: {e}")
            
            # 3. Add metadata about context enrichment
            task.context['context_enriched'] = True
            task.context['enrichment_timestamp'] = time.time()
            task.context['memory_tiers_available'] = {
                'textual': hasattr(self, 'mos_instance') and self.mos_instance is not None,
                'parametric': self.project_memory_manager is not None,
                'activation': self.project_memory_manager is not None  # KVCache handled separately
            }
            
            print(f"✅ Task context enrichment completed for {agent_role}")
            
        except Exception as e:
            print(f"❌ Failed to enrich task context: {e}")
            # Don't let context enrichment failures break task execution
            task.context['context_enrichment_error'] = str(e)
    
    async def _execute_single_task(self, task: Task, agent_role: str) -> Result:
        """
        Execute a single task using the appropriate agent with enhanced context from memory tiers.
        """
        try:
            agent = self.agents[agent_role]
            
            # Inject MemCube instance for KVCache optimization (ActivationMemory)
            if agent_role in ['code_generator', 'code_editor'] and hasattr(task.context, 'get'):
                user_id = task.context.get('user_id', 'default_user')
                project_id = task.context.get('project_id', 'default')
                
                composite_id = f"{user_id}_{project_id}"
                if composite_id in self.active_mem_cubes:
                    mem_cube_info = self.active_mem_cubes[composite_id]
                    
                    # Try to get the actual MemCube instance from project memory manager
                    if self.project_memory_manager:
                        try:
                            # Inject the cube info for KVCache optimization
                            task._mem_cube_instance = mem_cube_info
                            print(f"🚀 Injected MemCube for KVCache optimization: {composite_id}")
                        except Exception as e:
                            print(f"⚠️ Failed to inject MemCube: {e}")
            
            # Enhanced prompt generation using enriched context
            if task.context.get('context_enriched') and agent_role in ['code_generator', 'code_editor']:
                self._enhance_agent_prompt_with_context(task, agent_role)
            
            result = await agent.execute(task)
            return result
        except KeyError:
            return Result(
                task_id=task.task_id,
                status="failure",
                output="",
                error_message=f"Unknown agent role: {agent_role}"
            )
        except Exception as e:
            return Result(
                task_id=task.task_id,
                status="failure",
                output="",
                error_message=f"Task execution failed: {str(e)}"
            )
    
    def _enhance_agent_prompt_with_context(self, task: Task, agent_role: str) -> None:
        """
        Enhance agent prompt with context from memory tiers.
        
        This method modifies the task prompt to include:
        1. Project-specific guidelines from ParametricMemory
        2. Relevant code context from TextualMemory
        3. Agent-specific instructions for prioritizing guidelines
        
        Args:
            task: The task whose prompt will be enhanced
            agent_role: The role of the agent (code_generator, code_editor, etc.)
        """
        try:
            original_prompt = task.prompt
            prompt_sections = []
            
            # 1. Add project guidelines from ParametricMemory
            project_guidelines = task.context.get('project_guidelines')
            if project_guidelines:
                prompt_sections.append(project_guidelines)
                print(f"📋 Added project guidelines to {agent_role} prompt")
            
            # 2. Add relevant code context from TextualMemory
            code_context = task.context.get('code_context')
            if code_context and len(code_context) > 0:
                context_section = "Relevant codebase context:\n"
                for i, memory in enumerate(code_context[:2], 1):  # Limit to top 2 for brevity
                    context_section += f"{i}. {memory['content'][:300]}...\n"
                prompt_sections.append(context_section)
                print(f"🔍 Added code context to {agent_role} prompt")
            
            # 3. Add agent-specific instructions based on role
            if agent_role == 'code_generator':
                instruction = """When generating code, strictly adhere to the project guidelines above. 
Ensure your code follows the specified coding style, architecture patterns, and library preferences. 
Reference the codebase context to maintain consistency with existing code patterns."""
                
            elif agent_role == 'code_editor':
                instruction = """When editing code, prioritize the project guidelines above all other considerations.
Apply the specified coding standards, architectural patterns, and preferences consistently.
Use the codebase context to understand existing patterns and maintain consistency."""
                
            else:
                instruction = """Follow the project guidelines and use the codebase context to inform your work."""
            
            prompt_sections.append(instruction)
            
            # 4. Add the original task prompt
            prompt_sections.append(f"Task: {original_prompt}")
            
            # 5. Construct the enhanced prompt
            enhanced_prompt = "\n\n".join(prompt_sections)
            
            # Update the task prompt
            task.prompt = enhanced_prompt
            
            print(f"✅ Enhanced {agent_role} prompt with memory tier context")
            print(f"📊 Prompt length: {len(original_prompt)} → {len(enhanced_prompt)} chars")
            
        except Exception as e:
            print(f"❌ Failed to enhance agent prompt: {e}")
            # Don't break execution if prompt enhancement fails
    
    def _prepare_code_editor_context(self, task: Task, results: Dict[UUID, Result]) -> Dict[str, Any]:
        """
        Prepare context for CodeEditorAgent with proper code and instructions.
        
        This method:
        1. Extracts code from codebase expert results
        2. Extracts improvement suggestions from code quality analyzer results
        3. Formats the context properly for the code editor
        """
        context = task.context.copy()
        
        # Look for code from codebase expert (dependency results)
        code_to_edit = None
        analysis_results = None
        
        for dep_key, dep_result in context.items():
            if dep_key.startswith("dependency_") and isinstance(dep_result, dict):
                # Check if this is codebase expert result
                if dep_result.get('source') == 'rag_system':
                    # Extract code from the answer
                    answer = dep_result.get('answer', '')
                    if '```' in answer:
                        # Extract code block from markdown
                        code_blocks = answer.split('```')
                        for i, block in enumerate(code_blocks):
                            if i % 2 == 1:  # Odd indices are code blocks
                                # Remove language identifier if present
                                lines = block.strip().split('\n')
                                if lines and not lines[0].strip().startswith(('def ', 'class ', 'import ', 'from ')):
                                    lines = lines[1:]  # Remove language identifier
                                code_to_edit = '\n'.join(lines)
                                break
                    else:
                        # If no code blocks, assume the whole answer might be code
                        code_to_edit = answer
                
                # Check if this is code quality analyzer result
                elif isinstance(dep_result, str) and any(keyword in dep_result.lower() for keyword in [
                    'issue', 'error', 'warning', 'improvement', 'recommendation', 'fix'
                ]):
                    analysis_results = dep_result
        
        # Set up the context for code editor
        if code_to_edit:
            context["code_to_edit"] = code_to_edit
        
        # If we have analysis results, incorporate them into the prompt
        if analysis_results:
            original_prompt = task.prompt
            enhanced_prompt = f"{original_prompt}\n\nBased on the following analysis results:\n{analysis_results}"
            task.prompt = enhanced_prompt
        
        # Detect language from code content
        if code_to_edit:
            context["language"] = self._detect_language(code_to_edit)
        
        return context
    
    def _detect_language(self, code: str) -> str:
        """Simple language detection based on code patterns."""
        code_lower = code.lower()
        
        # Python indicators
        if any(keyword in code for keyword in ['def ', 'import ', 'from ', 'class ', 'if __name__']):
            return "python"
        
        # JavaScript indicators
        elif any(keyword in code for keyword in ['function ', 'var ', 'let ', 'const ', '=>', 'console.log']):
            return "javascript"
        
        # Java indicators
        elif any(keyword in code for keyword in ['public class', 'private ', 'public static void main']):
            return "java"
        
        # C/C++ indicators
        elif any(keyword in code for keyword in ['#include', 'int main', 'printf', 'cout']):
            return "cpp"
        
        # Default to python
        return "python"

    def _handle_next_action(self, result: Result, existing_tasks: List[Task]) -> Optional[Task]:
        """
        Handle next_action metadata from cognitive agents and create tool execution tasks.
        
        Args:
            result: Result object that may contain next_action metadata
            existing_tasks: List of existing tasks to avoid conflicts
            
        Returns:
            Optional Task for tool execution, or None if no action needed
        """
        # Check if result has next_action metadata
        if not hasattr(result, 'metadata') or not result.metadata:
            return None
            
        next_action = result.metadata.get('next_action')
        if not next_action:
            return None
        
        try:
            # Validate next_action structure
            if not isinstance(next_action, dict):
                print(f"⚠️ Invalid next_action format: must be dict, got {type(next_action)}")
                return None
            
            tool_name = next_action.get('tool')
            if not tool_name:
                print(f"⚠️ next_action missing 'tool' field: {next_action}")
                return None
            
            # Extract arguments from next_action
            tool_args = next_action.get('args', {})
            
            # Special handling for common tool actions
            if tool_name == 'create_file':
                # If content is not provided in args, use the result output as content
                if 'content' not in tool_args and result.output:
                    tool_args['content'] = result.output
                    
            elif tool_name == 'edit_file':
                # If content is not provided in args, use the result output as new content
                if 'content' not in tool_args and result.output:
                    tool_args['content'] = result.output
                    
            # Create JSON command for ToolExecutorAgent
            tool_command = {
                "tool": tool_name,
                "args": tool_args
            }
            
            # Create new task for tool execution
            tool_task = Task(
                prompt=json.dumps(tool_command),
                context={
                    "original_result_id": str(result.task_id),
                    "triggered_by": "next_action",
                    "source_output": result.output
                },
                dependencies=[result.task_id]  # Depends on the task that generated the action
            )
            
            # Store agent role mapping for the new task
            self.active_tasks[tool_task.task_id] = {
                "task": tool_task,
                "agent_role": "tool_executor"
            }
            
            print(f"🔧 Created tool execution task: {tool_name} with args: {tool_args}")
            return tool_task
            
        except Exception as e:
            print(f"❌ Error handling next_action: {str(e)}")
            return None

    def _assemble_final_result(self, results: Dict[UUID, Result], tasks: List[Task]) -> Any:
        """
        Assemble the final result from all task results.
        
        For now, returns the output of the last successful task.
        In the future, this can be enhanced to create more sophisticated
        result aggregation and formatting.
        """
        # Find the last successful result
        for task in reversed(tasks):
            if task.task_id in results and results[task.task_id].status == "success":
                return results[task.task_id].output
        
        # If no successful results, return error summary
        failed_tasks = [r for r in results.values() if r.status == "failure"]
        if failed_tasks:
            return f"Execution failed. Errors: {'; '.join([r.error_message for r in failed_tasks])}"
        
        return "No results generated"
    
    def get_agent_status(self) -> Dict[str, str]:
        """
        Get the current status of all agents.
        """
        return {role: agent.status for role, agent in self.agents.items()}
    
    async def cleanup(self):
        """
        Clean up resources for all agents and MemCubes.
        """
        try:
            # Clean up all active MemCubes
            if self.active_mem_cubes:
                print(f"🧹 Cleaning up {len(self.active_mem_cubes)} active MemCubes...")
                self.active_mem_cubes.clear()
            
            # Clean up project memory manager
            if self.project_memory_manager:
                self.project_memory_manager = None
            
            # Clean up agents
            cleanup_tasks = []
            for agent in self.agents.values():
                if hasattr(agent, 'cleanup'):
                    cleanup_tasks.append(agent.cleanup())
            
            if cleanup_tasks:
                await asyncio.gather(*cleanup_tasks, return_exceptions=True)
                
            print("✅ ProjectManager cleanup completed")
            
        except Exception as e:
            print(f"❌ Error during ProjectManager cleanup: {e}")
    
    def __repr__(self) -> str:
        return f"ProjectManager(agents={list(self.agents.keys())}, active_tasks={len(self.active_tasks)})" 