# Generated by SmolLM3-3B on 2024-12-19
"""
Base classes and data contracts for the agentic architecture.

This module defines the abstract BaseAgent class and the Task/Result
data models that establish the communication protocol between the
orchestrator and the agents.
"""

from abc import ABC, abstractmethod
from typing import Any, Optional
from uuid import UUID, uuid4
from pydantic import BaseModel, Field
import sys
import os

# Add src to path for ModelManager import
src_path = os.path.join(os.path.dirname(__file__), '..', 'src')
if src_path not in sys.path:
    sys.path.insert(0, src_path)
from models.manager import model_manager


class Task(BaseModel):
    """
    Represents a single task to be executed by an agent.
    
    This standardizes the information exchanged between the orchestrator
    and the agents, preventing errors and making the system predictable.
    """
    task_id: UUID = Field(default_factory=uuid4, description="Unique identifier for each task")
    prompt: str = Field(..., description="Specific instruction for the agent")
    context: dict = Field(default_factory=dict, description="Data passed between tasks")
    dependencies: list[UUID] = Field(default_factory=list, description="List of task_ids this task depends on")


class Result(BaseModel):
    """
    Represents the result of a task execution.
    
    This provides a standardized format for agent outputs and error handling.
    """
    task_id: UUID = Field(..., description="ID of the task that generated this result")
    status: str = Field(..., description="Outcome: 'success' or 'failure'")
    output: Any = Field(..., description="Main result (code, analysis, URLs, etc.)")
    error_message: Optional[str] = Field(None, description="Error message in case of failure")
    metadata: Optional[dict] = Field(None, description="Additional metadata including next_action suggestions")


class BaseAgent(ABC):
    """
    Abstract base class that establishes a common interface for all agents.
    
    This ensures that regardless of their specific task, all agents can be
    managed, monitored, and orchestrated uniformly.
    """
    
    def __init__(self, name: str, role: str, model_name: str):
        """
        Initialize the agent with its configuration.
        
        Args:
            name: Unique name for this agent instance (e.g., "GemmaCodeGenerator")
            role: Functional label for task assignment (e.g., "code_generator")
            model_name: Name of the model in the ModelManager configuration
        """
        self.name = name
        self.role = role
        self.model_name = model_name
        self.status = 'idle'
        # NOTE: No longer storing model/tokenizer instances directly
        # They are accessed via the @property model below
    
    @property
    def model(self):
        """
        This property lazily retrieves the model from the central manager
        the first time it's needed by an agent instance.
        
        This transparently hooks into the ModelManager and provides the same
        interface that existing agent code expects, requiring no changes to
        agent execute() methods.
        
        Returns:
            The model instance from ModelManager, or None if no model_name
        """
        if not self.model_name:
            return None
        
        try:
            return model_manager.get_model(self.model_name)
        except Exception as e:
            self.status = 'error'
            error_msg = f"❌ Failed to get model {self.model_name} for agent {self.name}: {str(e)}"
            print(error_msg)
            return None
    
    @property
    def tokenizer(self):
        """
        Get the tokenizer for the agent's model.
        
        For GGUF models, this returns the model itself (integrated tokenizer).
        For HuggingFace models, this returns a separate tokenizer instance.
        
        Returns:
            Tokenizer instance or model (for GGUF)
        """
        if not self.model_name:
            return None
        
        try:
            # Check if model is GGUF (llama-cpp) - tokenizer is integrated
            model_info = model_manager.get_model_info(self.model_name)
            if model_info.get('loader') == 'llama-cpp':
                return self.model  # GGUF models have integrated tokenizers
            
            # For HuggingFace models, load separate tokenizer
            from transformers import AutoTokenizer
            model_path = model_info.get('path', self.model_name)
            tokenizer = AutoTokenizer.from_pretrained(model_path)
            
            # Set pad token if not present
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
                
            return tokenizer
            
        except Exception as e:
            error_msg = f"❌ Failed to get tokenizer for {self.model_name}: {str(e)}"
            print(error_msg)
            return None
    
    @abstractmethod
    async def execute(self, task: Task) -> Result:
        """
        The core method that every agent must implement.
        
        This is the heart of the agent - it receives a Task object containing
        all necessary information and returns a Result object with the outcome.
        
        Args:
            task: A Task object with prompt, context, and dependencies
            
        Returns:
            Result: An object containing the task outcome
            
        Note:
            This method is async to handle I/O-bound operations (API calls,
            model loading) without blocking the entire system.
        """
        pass
    
    def lazy_load_model(self):
        """
        DEPRECATED: Model loading is now handled by ModelManager.
        
        This method is kept for backward compatibility but does nothing.
        Models are loaded automatically when accessed via the @property model.
        """
        # Set status to ready since ModelManager handles loading
        self.status = 'ready'
        print(f"ℹ️  Agent {self.name} using ModelManager for model '{self.model_name}'")
    
    def generate_response(self, prompt: str, max_new_tokens: int = 512, temperature: float = 0.7) -> str:
        """
        Generate a response using the model from ModelManager.
        
        This method now works with different model types (GGUF, HuggingFace, OpenAI)
        transparently through the ModelManager.
        
        Args:
            prompt: Input text to generate from
            max_new_tokens: Maximum number of tokens to generate
            temperature: Sampling temperature for generation
            
        Returns:
            str: Generated text response
        """
        if self.model is None:
            raise RuntimeError(f"Model not available for agent {self.name}. Check model_name: {self.model_name}")
        
        try:
            # Get model info to determine type
            model_info = model_manager.get_model_info(self.model_name)
            loader_type = model_info.get('loader', 'unknown')
            
            if loader_type == 'llama-cpp':
                # GGUF model using llama-cpp-python
                response = self.model(
                    prompt,
                    max_tokens=max_new_tokens,
                    temperature=temperature,
                    stop=["</s>", "\n\n"]
                )
                return response['choices'][0]['text'].strip()
            
            elif loader_type == 'huggingface':
                # HuggingFace transformers model
                import torch
                tokenizer = self.tokenizer
                
                # Tokenize input
                inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)
                
                # Move to same device as model
                device = next(self.model.parameters()).device
                inputs = {k: v.to(device) for k, v in inputs.items()}
                
                # Generate response
                with torch.no_grad():
                    outputs = self.model.generate(
                        **inputs,
                        max_new_tokens=max_new_tokens,
                        temperature=temperature,
                        do_sample=True,
                        pad_token_id=tokenizer.pad_token_id,
                        eos_token_id=tokenizer.eos_token_id
                    )
                
                # Decode response (skip the input tokens)
                input_length = inputs['input_ids'].shape[1]
                generated_tokens = outputs[0][input_length:]
                response = tokenizer.decode(generated_tokens, skip_special_tokens=True)
                return response.strip()
            
            elif loader_type == 'openai':
                # OpenAI-compatible API (placeholder for now)
                return f"OpenAI response for: {prompt[:50]}... [Generated with {max_new_tokens} tokens, temp={temperature}]"
            
            else:
                raise ValueError(f"Unknown model loader type: {loader_type}")
            
        except Exception as e:
            error_msg = f"Generation failed for agent {self.name}: {str(e)}"
            print(error_msg)
            raise RuntimeError(error_msg)
    
    def __repr__(self) -> str:
        return f"{self.__class__.__name__}(name='{self.name}', role='{self.role}', status='{self.status}')" 