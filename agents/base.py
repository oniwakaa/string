# Generated by SmolLM3-3B on 2024-12-19
"""
Base classes and data contracts for the agentic architecture.

This module defines the abstract BaseAgent class and the Task/Result
data models that establish the communication protocol between the
orchestrator and the agents.
"""

from abc import ABC, abstractmethod
from typing import Any, Optional
from uuid import UUID, uuid4
from pydantic import BaseModel, Field


class Task(BaseModel):
    """
    Represents a single task to be executed by an agent.
    
    This standardizes the information exchanged between the orchestrator
    and the agents, preventing errors and making the system predictable.
    """
    task_id: UUID = Field(default_factory=uuid4, description="Unique identifier for each task")
    prompt: str = Field(..., description="Specific instruction for the agent")
    context: dict = Field(default_factory=dict, description="Data passed between tasks")
    dependencies: list[UUID] = Field(default_factory=list, description="List of task_ids this task depends on")


class Result(BaseModel):
    """
    Represents the result of a task execution.
    
    This provides a standardized format for agent outputs and error handling.
    """
    task_id: UUID = Field(..., description="ID of the task that generated this result")
    status: str = Field(..., description="Outcome: 'success' or 'failure'")
    output: Any = Field(..., description="Main result (code, analysis, URLs, etc.)")
    error_message: Optional[str] = Field(None, description="Error message in case of failure")
    metadata: Optional[dict] = Field(None, description="Additional metadata including next_action suggestions")


class BaseAgent(ABC):
    """
    Abstract base class that establishes a common interface for all agents.
    
    This ensures that regardless of their specific task, all agents can be
    managed, monitored, and orchestrated uniformly.
    """
    
    def __init__(self, name: str, role: str, model_identifier: str):
        """
        Initialize the agent with its configuration.
        
        Args:
            name: Unique name for this agent instance (e.g., "GemmaCodeGenerator")
            role: Functional label for task assignment (e.g., "code_generator")
            model_identifier: HuggingFace model name or local path
        """
        self.name = name
        self.role = role
        self.model_identifier = model_identifier
        self.status = 'idle'
        self.model = None
        self.tokenizer = None
    
    @abstractmethod
    async def execute(self, task: Task) -> Result:
        """
        The core method that every agent must implement.
        
        This is the heart of the agent - it receives a Task object containing
        all necessary information and returns a Result object with the outcome.
        
        Args:
            task: A Task object with prompt, context, and dependencies
            
        Returns:
            Result: An object containing the task outcome
            
        Note:
            This method is async to handle I/O-bound operations (API calls,
            model loading) without blocking the entire system.
        """
        pass
    
    def lazy_load_model(self):
        """
        Load the LLM model and tokenizer on first use.
        
        This implements the lazy loading pattern - models are not loaded
        at service startup but only when an agent is first called.
        This dramatically reduces startup time and memory consumption at rest.
        
        Note: Agents with model_identifier=None (like CodebaseExpertAgent) skip this step.
        """
        if self.model is None and self.model_identifier is not None:
            try:
                # Import here to avoid circular imports and reduce startup time
                from transformers import AutoTokenizer, AutoModelForCausalLM
                import torch
                
                self.status = 'loading_model'
                
                # Load tokenizer
                self.tokenizer = AutoTokenizer.from_pretrained(self.model_identifier)
                
                # Set pad token if not present
                if self.tokenizer.pad_token is None:
                    self.tokenizer.pad_token = self.tokenizer.eos_token
                
                # Load model with appropriate device and dtype for M4 MacBook
                device = "mps" if torch.backends.mps.is_available() else "cpu"
                
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_identifier,
                    torch_dtype=torch.float16 if device == "mps" else torch.float32,
                    device_map="auto" if device != "mps" else None,
                    trust_remote_code=True
                )
                
                if device == "mps":
                    self.model = self.model.to(device)
                
                self.status = 'ready'
                print(f"✅ Model {self.model_identifier} loaded successfully for agent {self.name}")
                
            except Exception as e:
                self.status = 'error'
                error_msg = f"❌ Failed to load model {self.model_identifier} for agent {self.name}: {str(e)}"
                print(error_msg)
                raise RuntimeError(error_msg)
        elif self.model_identifier is None:
            # Agent doesn't use a local model (e.g., CodebaseExpertAgent)
            self.status = 'ready'
    
    def generate_response(self, prompt: str, max_new_tokens: int = 512, temperature: float = 0.7) -> str:
        """
        Generate a response using the loaded model.
        
        Args:
            prompt: Input text to generate from
            max_new_tokens: Maximum number of tokens to generate
            temperature: Sampling temperature for generation
            
        Returns:
            str: Generated text response
        """
        if self.model is None or self.tokenizer is None:
            raise RuntimeError(f"Model not loaded for agent {self.name}. Call lazy_load_model() first.")
        
        try:
            # Tokenize input
            inputs = self.tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)
            
            # Move to same device as model
            device = next(self.model.parameters()).device
            inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # Generate response
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    do_sample=True,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            # Decode response (skip the input tokens)
            input_length = inputs['input_ids'].shape[1]
            generated_tokens = outputs[0][input_length:]
            response = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
            
            return response.strip()
            
        except Exception as e:
            error_msg = f"Generation failed for agent {self.name}: {str(e)}"
            print(error_msg)
            raise RuntimeError(error_msg)
    
    def __repr__(self) -> str:
        return f"{self.__class__.__name__}(name='{self.name}', role='{self.role}', status='{self.status}')" 