# Generated by SmolLM3-3B on 2025-01-01
"""
Optimal Context Size Determination

Compare 16384 vs 32768 context sizes to determine the optimal value
for MacBook Air M4 hardware based on resource usage and performance.
"""

import time
import psutil
from pathlib import Path

try:
    from llama_cpp import Llama
    LLAMA_CPP_AVAILABLE = True
except ImportError:
    print("‚ùå llama-cpp-python not available")
    LLAMA_CPP_AVAILABLE = False

def test_context_size(n_ctx: int, model_path: str):
    """Test specific context size and return metrics."""
    print(f"\nüß™ Testing context size: {n_ctx}")
    
    # Get initial memory
    process = psutil.Process()
    initial_ram = process.memory_info().rss / 1024 / 1024
    
    try:
        # Load model
        start_time = time.time()
        
        model = Llama(
            model_path=model_path,
            n_ctx=n_ctx,
            n_gpu_layers=-1,
            n_batch=512,
            verbose=False,
            use_mmap=True,
            use_mlock=False,
            n_threads=None,
        )
        
        load_time = time.time() - start_time
        post_load_ram = process.memory_info().rss / 1024 / 1024
        ram_increase = post_load_ram - initial_ram
        
        print(f"   ‚úÖ Loaded in {load_time:.2f}s, RAM: +{ram_increase:.1f} MB")
        
        # Quick inference test
        start_inference = time.time()
        response = model(
            "What is machine learning?",
            max_tokens=50,
            temperature=0.7,
            echo=False
        )
        inference_time = (time.time() - start_inference) * 1000
        
        print(f"   ‚úÖ Inference: {inference_time:.1f}ms")
        
        # Cleanup
        del model
        
        return {
            'n_ctx': n_ctx,
            'load_time': load_time,
            'ram_increase_mb': ram_increase,
            'inference_time_ms': inference_time,
            'status': 'success'
        }
        
    except Exception as e:
        print(f"   ‚ùå Failed: {e}")
        return {
            'n_ctx': n_ctx,
            'error': str(e),
            'status': 'failed'
        }

def determine_optimal_context():
    """Determine optimal context size for MacBook Air M4."""
    if not LLAMA_CPP_AVAILABLE:
        print("‚ùå llama-cpp-python not available")
        return
    
    # Find model
    model_paths = [
        "./smollm-quantized/smollm-q4_K_M.gguf",
        "./smollm-quantized/ggml-model-f16.gguf",
        "./smollm/smollm-3b-instruct-q4_k_m.gguf",
    ]
    
    model_path = None
    for path in model_paths:
        if Path(path).exists():
            model_path = path
            break
    
    if not model_path:
        print(f"‚ùå No model found. Searched: {model_paths}")
        return
    
    print(f"üöÄ Determining optimal context size for MacBook Air M4")
    print(f"Model: {model_path}")
    print(f"Target: ‚â§10GB RAM, <60s load time, reasonable inference latency")
    
    # Test different context sizes
    context_sizes = [16384, 32768]
    results = []
    
    for n_ctx in context_sizes:
        result = test_context_size(n_ctx, model_path)
        results.append(result)
        
        # Stop if we hit hardware limits
        if result.get('status') == 'failed':
            print(f"   ‚ö†Ô∏è  Stopping tests - hardware limit reached")
            break
        
        # Check if we're approaching limits
        if result.get('ram_increase_mb', 0) > 8000:  # 8GB warning threshold
            print(f"   ‚ö†Ô∏è  Approaching RAM limit - consider stopping")
    
    # Analyze results and make recommendation
    print(f"\nüìä OPTIMAL CONTEXT SIZE ANALYSIS")
    print(f"{'Context Size':<12} {'Load Time':<10} {'RAM (MB)':<10} {'Inference':<12} {'Recommendation'}")
    print(f"{'-'*65}")
    
    successful_results = [r for r in results if r.get('status') == 'success']
    
    if not successful_results:
        print("‚ùå No successful tests - all context sizes failed")
        return
    
    # Score each option (lower is better)
    best_score = float('inf')
    best_option = None
    
    for result in successful_results:
        n_ctx = result['n_ctx']
        load_time = result['load_time']
        ram_increase = result['ram_increase_mb']
        inference_time = result['inference_time_ms']
        
        # Scoring criteria (penalties for exceeding thresholds)
        score = 0
        if load_time > 60: score += 100  # Load time penalty
        if ram_increase > 8000: score += 200  # RAM penalty
        if inference_time > 5000: score += 50  # Inference penalty
        
        # Add base scores (prefer reasonable resource usage)
        score += load_time  # Prefer faster loading
        score += ram_increase / 100  # Prefer lower RAM usage
        score += inference_time / 1000  # Prefer faster inference
        
        # Determine recommendation
        if score < 50:
            recommendation = "‚úÖ Excellent"
        elif score < 100:
            recommendation = "üü° Good"
        elif score < 200:
            recommendation = "üü† Acceptable"
        else:
            recommendation = "‚ùå Not recommended"
        
        print(f"{n_ctx:<12} {load_time:<10.1f} {ram_increase:<10.1f} {inference_time:<12.1f} {recommendation}")
        
        if score < best_score:
            best_score = score
            best_option = result
    
    # Final recommendation
    print(f"\nüéØ FINAL RECOMMENDATION")
    if best_option:
        print(f"‚úÖ Optimal Context Size: {best_option['n_ctx']}")
        print(f"üìà Performance: {best_option['load_time']:.1f}s load, {best_option['ram_increase_mb']:.1f}MB RAM")
        print(f"üí° Reasoning: Best balance of context utilization and resource efficiency")
        
        # Context utilization analysis
        utilization_4096 = (4096 / 65536) * 100
        utilization_current = (best_option['n_ctx'] / 65536) * 100
        improvement = utilization_current / utilization_4096
        
        print(f"\nüìä Context Utilization Improvement:")
        print(f"   Previous (4096): {utilization_4096:.1f}% of training context")
        print(f"   Optimal ({best_option['n_ctx']}): {utilization_current:.1f}% of training context")
        print(f"   Improvement: {improvement:.1f}x better utilization")
    else:
        print("‚ùå Could not determine optimal context size")

if __name__ == "__main__":
    determine_optimal_context() 