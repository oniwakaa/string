# Generated by Claude Code on 2025-01-26
"""
Codebase State Manager for tracking file changes and managing incremental updates.

This module provides intelligent codebase loading with change detection to minimize
redundant processing and maintain optimal memory efficiency.
"""

import json
import os
import hashlib
import time
from pathlib import Path
from typing import Dict, List, Set, Optional, Tuple, Any
from dataclasses import dataclass, asdict
import logging

logger = logging.getLogger(__name__)


@dataclass
class FileManifestEntry:
    """Represents a file's state in the manifest."""
    path: str
    mtime: float
    size: int
    hash_md5: Optional[str] = None


@dataclass
class CodebaseManifest:
    """Manifest of the entire codebase state."""
    timestamp: float
    project_root: str
    total_files: int
    total_size: int
    files: Dict[str, FileManifestEntry]
    memignore_patterns_count: int
    filtering_method: str
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization."""
        return {
            'timestamp': self.timestamp,
            'project_root': self.project_root,
            'total_files': self.total_files,
            'total_size': self.total_size,
            'files': {path: asdict(entry) for path, entry in self.files.items()},
            'memignore_patterns_count': self.memignore_patterns_count,
            'filtering_method': self.filtering_method
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'CodebaseManifest':
        """Create manifest from dictionary."""
        files = {
            path: FileManifestEntry(**entry_data) 
            for path, entry_data in data['files'].items()
        }
        return cls(
            timestamp=data['timestamp'],
            project_root=data['project_root'],
            total_files=data['total_files'],
            total_size=data['total_size'],
            files=files,
            memignore_patterns_count=data['memignore_patterns_count'],
            filtering_method=data['filtering_method']
        )


@dataclass
class CodebaseChanges:
    """Represents detected changes in the codebase."""
    new_files: List[str]
    modified_files: List[str]
    deleted_files: List[str]
    total_changes: int
    
    @property
    def has_changes(self) -> bool:
        """Check if there are any changes."""
        return self.total_changes > 0


class CodebaseStateManager:
    """
    Manages codebase state tracking and change detection for optimal loading.
    
    Features:
    - Tracks file modification times and sizes
    - Detects new, modified, and deleted files
    - Generates manifest files for state persistence
    - Supports incremental updates
    """
    
    def __init__(self, project_root: str, manifest_filename: str = ".codebase_state.json"):
        """
        Initialize the codebase state manager.
        
        Args:
            project_root: Root directory of the project
            manifest_filename: Name of the manifest file to store state
        """
        self.project_root = Path(project_root).resolve()
        self.manifest_path = self.project_root / manifest_filename
        self.current_manifest: Optional[CodebaseManifest] = None
        
    def _calculate_file_hash(self, file_path: Path) -> str:
        """Calculate MD5 hash of a file for change detection."""
        try:
            hasher = hashlib.md5()
            with open(file_path, 'rb') as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hasher.update(chunk)
            return hasher.hexdigest()
        except Exception as e:
            logger.warning(f"Could not calculate hash for {file_path}: {e}")
            return ""
    
    def _scan_current_files(self, filtered_files: List[Path]) -> Dict[str, FileManifestEntry]:
        """
        Scan current files and create manifest entries.
        
        Args:
            filtered_files: List of files after .memignore filtering
            
        Returns:
            Dictionary of file paths to manifest entries
        """
        manifest_files = {}
        
        for file_path in filtered_files:
            try:
                if not file_path.exists():
                    continue
                    
                stat = file_path.stat()
                relative_path = str(file_path.relative_to(self.project_root))
                
                entry = FileManifestEntry(
                    path=relative_path,
                    mtime=stat.st_mtime,
                    size=stat.st_size,
                    hash_md5=None  # Calculate only if needed for change detection
                )
                
                manifest_files[relative_path] = entry
                
            except Exception as e:
                logger.warning(f"Error processing file {file_path}: {e}")
                continue
        
        return manifest_files
    
    def load_previous_manifest(self) -> Optional[CodebaseManifest]:
        """
        Load the previous codebase manifest if it exists.
        
        Returns:
            Previous manifest or None if not found/invalid
        """
        if not self.manifest_path.exists():
            logger.info("No previous codebase manifest found")
            return None
        
        try:
            with open(self.manifest_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            manifest = CodebaseManifest.from_dict(data)
            logger.info(f"Loaded previous manifest: {manifest.total_files} files from {manifest.timestamp}")
            return manifest
            
        except Exception as e:
            logger.warning(f"Could not load previous manifest: {e}")
            return None
    
    def save_manifest(self, manifest: CodebaseManifest) -> bool:
        """
        Save the current manifest to disk.
        
        Args:
            manifest: Manifest to save
            
        Returns:
            True if saved successfully, False otherwise
        """
        try:
            with open(self.manifest_path, 'w', encoding='utf-8') as f:
                json.dump(manifest.to_dict(), f, indent=2)
            
            logger.info(f"Saved codebase manifest: {manifest.total_files} files")
            return True
            
        except Exception as e:
            logger.error(f"Could not save manifest: {e}")
            return False
    
    def detect_changes(self, current_files: List[Path], 
                      memignore_patterns_count: int,
                      filtering_method: str) -> Tuple[CodebaseChanges, CodebaseManifest]:
        """
        Detect changes between current state and previous manifest.
        
        Args:
            current_files: List of current filtered files
            memignore_patterns_count: Number of patterns applied
            filtering_method: Method used for filtering
            
        Returns:
            Tuple of (detected changes, current manifest)
        """
        # Scan current files
        current_manifest_files = self._scan_current_files(current_files)
        
        # Create current manifest
        current_manifest = CodebaseManifest(
            timestamp=time.time(),
            project_root=str(self.project_root),
            total_files=len(current_manifest_files),
            total_size=sum(entry.size for entry in current_manifest_files.values()),
            files=current_manifest_files,
            memignore_patterns_count=memignore_patterns_count,
            filtering_method=filtering_method
        )
        
        # Load previous manifest
        previous_manifest = self.load_previous_manifest()
        
        if previous_manifest is None:
            # No previous state, everything is new
            changes = CodebaseChanges(
                new_files=list(current_manifest_files.keys()),
                modified_files=[],
                deleted_files=[],
                total_changes=len(current_manifest_files)
            )
            logger.info(f"No previous manifest found. Full load required: {changes.total_changes} files")
            return changes, current_manifest
        
        # Compare manifests
        current_paths = set(current_manifest_files.keys())
        previous_paths = set(previous_manifest.files.keys())
        
        new_files = list(current_paths - previous_paths)
        deleted_files = list(previous_paths - current_paths)
        modified_files = []
        
        # Check for modifications in common files
        common_files = current_paths & previous_paths
        for file_path in common_files:
            current_entry = current_manifest_files[file_path]
            previous_entry = previous_manifest.files[file_path]
            
            # Compare modification time and size
            if (current_entry.mtime != previous_entry.mtime or 
                current_entry.size != previous_entry.size):
                modified_files.append(file_path)
        
        changes = CodebaseChanges(
            new_files=new_files,
            modified_files=modified_files,
            deleted_files=deleted_files,
            total_changes=len(new_files) + len(modified_files) + len(deleted_files)
        )
        
        # Log change summary
        if changes.has_changes:
            logger.info(f"Codebase changes detected: {len(new_files)} new, "
                       f"{len(modified_files)} modified, {len(deleted_files)} deleted")
        else:
            logger.info("No codebase changes detected since last load")
        
        return changes, current_manifest
    
    def should_reload_codebase(self, current_files: List[Path],
                             memignore_patterns_count: int,
                             filtering_method: str) -> Tuple[bool, CodebaseChanges, CodebaseManifest]:
        """
        Determine if codebase should be reloaded based on detected changes.
        
        Args:
            current_files: List of current filtered files
            memignore_patterns_count: Number of patterns applied
            filtering_method: Method used for filtering
            
        Returns:
            Tuple of (should_reload, changes, current_manifest)
        """
        changes, current_manifest = self.detect_changes(
            current_files, memignore_patterns_count, filtering_method
        )
        
        should_reload = changes.has_changes
        
        if should_reload:
            logger.info(f"Codebase reload required due to {changes.total_changes} changes")
        else:
            logger.info("Codebase is up to date, skipping reload")
        
        return should_reload, changes, current_manifest
    
    def mark_load_complete(self, manifest: CodebaseManifest) -> bool:
        """
        Mark codebase loading as complete by saving the manifest.
        
        Args:
            manifest: Manifest representing the loaded state
            
        Returns:
            True if marked successfully, False otherwise
        """
        self.current_manifest = manifest
        return self.save_manifest(manifest)
    
    def get_files_for_incremental_update(self, changes: CodebaseChanges) -> List[str]:
        """
        Get list of files that need to be processed for incremental update.
        
        Args:
            changes: Detected changes
            
        Returns:
            List of file paths that need processing
        """
        files_to_process = []
        files_to_process.extend(changes.new_files)
        files_to_process.extend(changes.modified_files)
        
        return files_to_process
    
    def get_files_for_deletion(self, changes: CodebaseChanges) -> List[str]:
        """
        Get list of files that need to be removed from memory.
        
        Args:
            changes: Detected changes
            
        Returns:
            List of file paths that need to be removed
        """
        return changes.deleted_files
    
    def clear_workspace_context(self, workspace_id: str = "default", mos_instance=None) -> Dict[str, Any]:
        """
        Clear all context associated with a workspace from MemOS.
        
        Args:
            workspace_id: Workspace identifier
            mos_instance: MemOS instance for clearing memory cubes
            
        Returns:
            Dictionary with operation status and details
        """
        try:
            logger.info(f"ðŸ§¹ Clearing workspace context for: {workspace_id}")
            cleared_components = []
            
            # Clear MemOS memory cubes if instance is provided
            if mos_instance:
                try:
                    # Get all memory cubes for the user/workspace
                    if hasattr(mos_instance, 'mem_cubes') and mos_instance.mem_cubes:
                        cube_ids = list(mos_instance.mem_cubes.keys())
                        for cube_id in cube_ids:
                            try:
                                # Clear all memories in the cube
                                mos_instance.delete_all(mem_cube_id=cube_id, user_id=workspace_id)
                                logger.info(f"âœ… Cleared memory cube: {cube_id}")
                                cleared_components.append(f"memory_cube_{cube_id}")
                            except Exception as e:
                                logger.warning(f"Failed to clear memory cube {cube_id}: {e}")
                    
                    # Reset the MemOS instance memory cubes
                    if hasattr(mos_instance, 'mem_cubes'):
                        mos_instance.mem_cubes.clear()
                        logger.info("âœ… Cleared MemOS memory cubes registry")
                        cleared_components.append("mos_cubes_registry")
                        
                except Exception as e:
                    logger.warning(f"Failed to clear MemOS memory: {e}")
            
            # Clear the manifest file
            if self.manifest_path.exists():
                self.manifest_path.unlink()
                logger.info(f"âœ… Cleared manifest file: {self.manifest_path}")
                cleared_components.append("manifest")
            
            # Clear Qdrant storage
            qdrant_storage_path = self.project_root / "qdrant_storage"
            if qdrant_storage_path.exists():
                import shutil
                shutil.rmtree(qdrant_storage_path)
                logger.info(f"âœ… Cleared Qdrant storage: {qdrant_storage_path}")
                cleared_components.append("vector_storage")
            
            # Reset current manifest
            self.current_manifest = None
            
            return {
                "success": True,
                "workspace_id": workspace_id,
                "message": "Workspace context cleared successfully",
                "cleared_components": cleared_components,
                "timestamp": time.time()
            }
            
        except Exception as e:
            logger.error(f"âŒ Failed to clear workspace context: {e}")
            return {
                "success": False,
                "workspace_id": workspace_id,
                "error": str(e),
                "timestamp": time.time()
            }
    
    def compact_workspace_context(self, workspace_id: str = "default", 
                                 llm_service=None, mos_instance=None) -> Dict[str, Any]:
        """
        Compact workspace context by summarizing chat history.
        
        Args:
            workspace_id: Workspace identifier
            llm_service: LLM service instance for generating summaries
            mos_instance: MemOS instance for accessing memory cubes
            
        Returns:
            Dictionary with operation status and details
        """
        try:
            logger.info(f"ðŸ“¦ Compacting workspace context for: {workspace_id}")
            
            # Get chat history from MemOS
            chat_history = self._get_chat_history_from_memos(workspace_id, mos_instance)
            
            if not chat_history:
                return {
                    "success": True,
                    "workspace_id": workspace_id,
                    "message": "No chat history to compact",
                    "original_length": 0,
                    "compressed_length": 0,
                    "compression_ratio": 0.0,
                    "summary_preview": "No content to summarize",
                    "timestamp": time.time()
                }
            
            # Calculate original token count (rough estimation)
            original_tokens = sum(len(msg.split()) for msg in chat_history)
            
            # Generate compressed summary
            if llm_service:
                summary = self._generate_summary(chat_history, llm_service)
            else:
                # Fallback summary without LLM
                summary = self._create_fallback_summary(chat_history)
            
            # Calculate compressed token count
            compressed_tokens = len(summary.split())
            compression_ratio = 1 - (compressed_tokens / original_tokens) if original_tokens > 0 else 0
            
            # Replace history with summary in MemOS
            compression_success = self._replace_chat_history_in_memos(workspace_id, summary, mos_instance, chat_history)
            
            logger.info(f"âœ… Context compressed: {original_tokens} â†’ {compressed_tokens} tokens "
                       f"({compression_ratio:.1%} reduction)")
            
            return {
                "success": compression_success,
                "workspace_id": workspace_id,
                "message": "Workspace context compacted successfully" if compression_success else "Partial compression completed",
                "original_length": original_tokens,
                "compressed_length": compressed_tokens,
                "compression_ratio": compression_ratio,
                "summary_preview": summary[:200] + "..." if len(summary) > 200 else summary,
                "timestamp": time.time()
            }
            
        except Exception as e:
            logger.error(f"âŒ Failed to compact workspace context: {e}")
            return {
                "success": False,
                "workspace_id": workspace_id,
                "error": str(e),
                "timestamp": time.time()
            }
    
    def _get_chat_history_from_memos(self, workspace_id: str, mos_instance) -> List[str]:
        """
        Get chat history for a workspace from MemOS.
        """
        if not mos_instance:
            logger.warning("No MemOS instance provided for retrieving chat history")
            return []
        
        try:
            chat_history = []
            
            # Iterate through all memory cubes to find chat history
            if hasattr(mos_instance, 'mem_cubes') and mos_instance.mem_cubes:
                for cube_id, mem_cube in mos_instance.mem_cubes.items():
                    try:
                        # Get all memories from this cube
                        memories = mos_instance.get_all(mem_cube_id=cube_id, user_id=workspace_id)
                        
                        if memories and isinstance(memories, list):
                            for memory_data in memories:
                                if isinstance(memory_data, dict) and 'memories' in memory_data:
                                    for memory in memory_data['memories']:
                                        if hasattr(memory, 'content'):
                                            chat_history.append(memory.content)
                                        elif isinstance(memory, dict) and 'content' in memory:
                                            chat_history.append(memory['content'])
                                        elif isinstance(memory, str):
                                            chat_history.append(memory)
                                            
                    except Exception as e:
                        logger.warning(f"Failed to retrieve memories from cube {cube_id}: {e}")
                        continue
            
            logger.info(f"Retrieved {len(chat_history)} messages from MemOS for workspace {workspace_id}")
            return chat_history
            
        except Exception as e:
            logger.error(f"Failed to retrieve chat history from MemOS: {e}")
            return []
    
    def _generate_summary(self, chat_history: List[str], llm_service) -> str:
        """
        Generate a summary using the LLM service.
        """
        try:
            # Combine chat history
            full_conversation = "\n".join(chat_history)
            
            # Create summarization prompt
            prompt = f"""Please provide a concise summary of the following conversation, focusing on key topics, solutions provided, and any important decisions made:

{full_conversation}

Summary:"""
            
            # Generate summary (mock implementation - replace with actual LLM call)
            if hasattr(llm_service, 'generate') or hasattr(llm_service, '__call__'):
                try:
                    if hasattr(llm_service, 'generate'):
                        response = llm_service.generate(prompt, max_tokens=150)
                    else:
                        response = llm_service(prompt, max_tokens=150)
                    
                    if isinstance(response, dict) and 'text' in response:
                        return response['text'].strip()
                    elif isinstance(response, str):
                        return response.strip()
                    else:
                        return str(response).strip()
                except Exception as e:
                    logger.warning(f"LLM summarization failed: {e}, using fallback")
                    return self._create_fallback_summary(chat_history)
            else:
                logger.warning("LLM service not callable, using fallback summary")
                return self._create_fallback_summary(chat_history)
                
        except Exception as e:
            logger.warning(f"Summary generation failed: {e}, using fallback")
            return self._create_fallback_summary(chat_history)
    
    def _create_fallback_summary(self, chat_history: List[str]) -> str:
        """
        Create a simple summary without LLM.
        """
        if not chat_history:
            return "Empty conversation."
        
        # Extract user questions and topics
        user_messages = [msg for msg in chat_history if msg.startswith("User:")]
        topics = []
        
        for msg in user_messages:
            # Extract key terms
            content = msg.replace("User:", "").strip()
            if "API" in content:
                topics.append("REST API development")
            if "authentication" in content.lower():
                topics.append("authentication")
            if "database" in content.lower():
                topics.append("database integration")
        
        if topics:
            return f"Conversation covered: {', '.join(set(topics))}. Total exchanges: {len(chat_history)}"
        else:
            return f"General development discussion with {len(chat_history)} messages."
    
    def _replace_chat_history_in_memos(self, workspace_id: str, summary: str, 
                                      mos_instance, original_history: List[str]) -> bool:
        """
        Replace chat history with summary in MemOS storage.
        """
        if not mos_instance:
            logger.warning("No MemOS instance provided for replacing chat history")
            return False
        
        try:
            success_count = 0
            total_cubes = 0
            
            # Clear existing chat history from all memory cubes
            if hasattr(mos_instance, 'mem_cubes') and mos_instance.mem_cubes:
                cube_ids = list(mos_instance.mem_cubes.keys())
                total_cubes = len(cube_ids)
                
                for cube_id in cube_ids:
                    try:
                        # Clear all existing memories in the cube
                        mos_instance.delete_all(mem_cube_id=cube_id, user_id=workspace_id)
                        
                        # Add the compressed summary as a new memory
                        from datetime import datetime
                        summary_memory = {
                            "content": summary,
                            "metadata": {
                                "type": "compressed_summary",
                                "original_message_count": len(original_history),
                                "compressed_at": datetime.now().isoformat(),
                                "workspace_id": workspace_id
                            }
                        }
                        
                        # Add the summary to MemOS
                        mos_instance.add(
                            mem_cube_id=cube_id,
                            user_id=workspace_id,
                            content=summary,
                            metadata=summary_memory["metadata"]
                        )
                        
                        success_count += 1
                        logger.info(f"âœ… Replaced chat history in cube {cube_id} with summary")
                        
                    except Exception as e:
                        logger.warning(f"Failed to replace chat history in cube {cube_id}: {e}")
                        continue
            
            # Create a backup log file for tracking
            summary_path = self.project_root / f".workspace_{workspace_id}_summary.txt"
            try:
                with open(summary_path, 'w', encoding='utf-8') as f:
                    f.write(f"Workspace: {workspace_id}\n")
                    f.write(f"Compressed at: {datetime.now().isoformat()}\n")
                    f.write(f"Original messages: {len(original_history)}\n")
                    f.write(f"Cubes processed: {success_count}/{total_cubes}\n")
                    f.write(f"Summary:\n{summary}\n")
            except Exception as e:
                logger.warning(f"Failed to write summary log: {e}")
            
            logger.info(f"ðŸ“ Replaced chat history in {success_count}/{total_cubes} cubes for workspace: {workspace_id}")
            return success_count > 0
            
        except Exception as e:
            logger.error(f"Failed to replace chat history in MemOS: {e}")
            return False