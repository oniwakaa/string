# Generated by SmolLM3-3B on 2024-12-26
# Save as quantize.py in your project directory

import os
import sys
import time
import json
import logging
import traceback
from typing import Dict, List, Tuple, Optional, Any, Union
from pathlib import Path
from dataclasses import dataclass, asdict
import gc

import torch
import torch.nn as nn
import numpy as np
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    BitsAndBytesConfig,
    set_seed
)
from transformers.utils import logging as transformers_logging

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('quantization.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# Suppress transformers warnings for cleaner output
transformers_logging.set_verbosity_error()


@dataclass
class QuantizationConfig:
    """Configuration class for quantization parameters."""
    model_path: str = "./smollm"
    output_path: str = "./smollm-quantized"
    quantization_bits: int = 4
    use_double_quant: bool = True
    quant_type: str = "nf4"  # "nf4" or "fp4"
    compute_dtype: str = "bfloat16"
    device_map: str = "auto"
    max_memory: Optional[Dict[str, str]] = None
    offload_folder: Optional[str] = None
    low_cpu_mem_usage: bool = True
    torch_dtype: str = "bfloat16"
    trust_remote_code: bool = True
    use_cache: bool = True
    pad_token_id: Optional[int] = None
    seed: int = 42
    
    def __post_init__(self):
        """Validate configuration parameters."""
        if self.quantization_bits not in [4, 8]:
            raise ValueError(f"Quantization bits must be 4 or 8, got {self.quantization_bits}")
        if self.quant_type not in ["nf4", "fp4"]:
            raise ValueError(f"Quantization type must be 'nf4' or 'fp4', got {self.quant_type}")
        if not Path(self.model_path).exists():
            raise FileNotFoundError(f"Model path does not exist: {self.model_path}")


@dataclass
class BenchmarkResults:
    """Container for benchmark results."""
    model_size_mb: float
    load_time_sec: float
    inference_time_ms: float
    memory_usage_mb: float
    accuracy: Optional[float] = None
    perplexity: Optional[float] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization."""
        return asdict(self)


class MemoryMonitor:
    """Utility class for monitoring memory usage."""
    
    @staticmethod
    def get_gpu_memory() -> Dict[str, float]:
        """Get GPU memory usage in MB."""
        if torch.cuda.is_available():
            return {
                f"gpu_{i}": torch.cuda.memory_allocated(i) / 1024**2 
                for i in range(torch.cuda.device_count())
            }
        return {}
    
    @staticmethod
    def get_cpu_memory() -> float:
        """Get CPU memory usage in MB."""
        import psutil
        return psutil.Process().memory_info().rss / 1024**2
    
    @staticmethod
    def get_total_memory() -> float:
        """Get total memory usage in MB."""
        cpu_mem = MemoryMonitor.get_cpu_memory()
        gpu_mem = sum(MemoryMonitor.get_gpu_memory().values())
        return cpu_mem + gpu_mem


class ModelQuantizer:
    """Advanced model quantization with benchmarking and validation."""
    
    def __init__(self, config: QuantizationConfig):
        """Initialize quantizer with configuration."""
        self.config = config
        self.original_model = None
        self.quantized_model = None
        self.tokenizer = None
        self.benchmark_results = {}
        
        # Set deterministic seed
        set_seed(config.seed)
        torch.manual_seed(config.seed)
        np.random.seed(config.seed)
        
        # Configure device
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"Using device: {self.device}")
        
        # Create output directory
        Path(config.output_path).mkdir(parents=True, exist_ok=True)
        
    def _create_quantization_config(self) -> BitsAndBytesConfig:
        """Create BitsAndBytesConfig with optimized settings."""
        compute_dtype = getattr(torch, self.config.compute_dtype)
        
        if self.config.quantization_bits == 4:
            return BitsAndBytesConfig(
                load_in_4bit=True,
                load_in_8bit=False,
                bnb_4bit_quant_type=self.config.quant_type,
                bnb_4bit_compute_dtype=compute_dtype,
                bnb_4bit_use_double_quant=self.config.use_double_quant,
                bnb_4bit_quant_storage=compute_dtype,
            )
        else:  # 8-bit
            return BitsAndBytesConfig(
                load_in_8bit=True,
                load_in_4bit=False,
                bnb_8bit_compute_dtype=compute_dtype,
                # Note: bnb_8bit_use_double_quant may not be available in all versions
            )
    
    def _get_model_size(self, model_path: str) -> float:
        """Calculate model size in MB."""
        if isinstance(model_path, str):
            model_path = Path(model_path)
        
        total_size = 0
        if model_path.is_dir():
            for file_path in model_path.rglob("*"):
                if file_path.is_file():
                    total_size += file_path.stat().st_size
        else:
            total_size = model_path.stat().st_size
        
        return total_size / (1024 ** 2)  # Convert to MB
    
    def _benchmark_inference(self, model, tokenizer, num_samples: int = 100) -> float:
        """Benchmark inference speed."""
        # Create sample inputs
        sample_texts = [
            "The quick brown fox jumps over the lazy dog.",
            "Hello, how are you today?",
            "Machine learning is a subset of artificial intelligence.",
            "Python is a popular programming language.",
            "The weather is nice today."
        ]
        
        model.eval()
        total_time = 0
        
        with torch.no_grad():
            for i in range(num_samples):
                text = sample_texts[i % len(sample_texts)]
                inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
                
                # Move to device
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
                
                start_time = time.time()
                with torch.cuda.amp.autocast() if torch.cuda.is_available() else torch.no_grad():
                    outputs = model(**inputs)
                end_time = time.time()
                
                total_time += (end_time - start_time)
        
        return (total_time / num_samples) * 1000  # Convert to ms
    
    def _calculate_perplexity(self, model, tokenizer, text_samples: List[str]) -> float:
        """Calculate perplexity on text samples."""
        model.eval()
        total_loss = 0
        total_tokens = 0
        
        with torch.no_grad():
            for text in text_samples:
                inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
                
                outputs = model(**inputs, labels=inputs["input_ids"])
                loss = outputs.loss
                
                total_loss += loss.item() * inputs["input_ids"].size(1)
                total_tokens += inputs["input_ids"].size(1)
        
        return torch.exp(torch.tensor(total_loss / total_tokens)).item()
    
    def load_original_model(self) -> None:
        """Load the original model for comparison."""
        logger.info("Loading original model...")
        start_time = time.time()
        
        try:
            # Load tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.config.model_path,
                trust_remote_code=self.config.trust_remote_code
            )
            
            # Set pad token if not set
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                if self.config.pad_token_id is None:
                    self.config.pad_token_id = self.tokenizer.eos_token_id
            
            # Load model
            torch_dtype = getattr(torch, self.config.torch_dtype)
            self.original_model = AutoModelForCausalLM.from_pretrained(
                self.config.model_path,
                torch_dtype=torch_dtype,
                device_map=self.config.device_map,
                max_memory=self.config.max_memory,
                offload_folder=self.config.offload_folder,
                low_cpu_mem_usage=self.config.low_cpu_mem_usage,
                trust_remote_code=self.config.trust_remote_code,
                use_cache=self.config.use_cache,
                pad_token_id=self.config.pad_token_id
            )
            
            load_time = time.time() - start_time
            logger.info(f"Original model loaded in {load_time:.2f} seconds")
            
        except Exception as e:
            logger.error(f"Error loading original model: {e}")
            raise
    
    def quantize_model(self) -> None:
        """Quantize the model with optimized settings."""
        logger.info(f"Starting {self.config.quantization_bits}-bit quantization...")
        start_time = time.time()
        
        try:
            # Create quantization config
            quant_config = self._create_quantization_config()
            
            # Load quantized model
            torch_dtype = getattr(torch, self.config.torch_dtype)
            self.quantized_model = AutoModelForCausalLM.from_pretrained(
                self.config.model_path,
                quantization_config=quant_config,
                torch_dtype=torch_dtype,
                device_map=self.config.device_map,
                max_memory=self.config.max_memory,
                offload_folder=self.config.offload_folder,
                low_cpu_mem_usage=self.config.low_cpu_mem_usage,
                trust_remote_code=self.config.trust_remote_code,
                use_cache=self.config.use_cache,
                pad_token_id=self.config.pad_token_id
            )
            
            load_time = time.time() - start_time
            logger.info(f"Model quantization completed in {load_time:.2f} seconds")
            
        except Exception as e:
            logger.error(f"Error during quantization: {e}")
            raise
    
    def save_quantized_model(self) -> None:
        """Save the quantized model and tokenizer."""
        logger.info(f"Saving quantized model to {self.config.output_path}...")
        
        try:
            # Save model
            self.quantized_model.save_pretrained(
                self.config.output_path,
                safe_serialization=True,
                max_shard_size="2GB"
            )
            
            # Save tokenizer
            if self.tokenizer:
                self.tokenizer.save_pretrained(self.config.output_path)
            
            # Save configuration
            config_path = Path(self.config.output_path) / "quantization_config.json"
            with open(config_path, "w") as f:
                # Convert config to dict, handling non-serializable types
                config_dict = asdict(self.config)
                json.dump(config_dict, f, indent=2)
            
            logger.info("Quantized model saved successfully")
            
        except Exception as e:
            logger.error(f"Error saving quantized model: {e}")
            raise
    
    def benchmark_models(self) -> Tuple[BenchmarkResults, BenchmarkResults]:
        """Benchmark both original and quantized models."""
        logger.info("Starting comprehensive benchmarking...")
        
        # Sample texts for validation
        validation_texts = [
            "The quick brown fox jumps over the lazy dog and runs through the forest.",
            "Machine learning algorithms are becoming increasingly sophisticated and powerful.",
            "Python programming language provides excellent libraries for data science.",
            "Climate change is one of the most pressing challenges of our time.",
            "Artificial intelligence has the potential to revolutionize many industries."
        ]
        
        results = {}
        
        # Benchmark original model
        if self.original_model:
            logger.info("Benchmarking original model...")
            start_mem = MemoryMonitor.get_total_memory()
            
            inference_time = self._benchmark_inference(self.original_model, self.tokenizer)
            perplexity = self._calculate_perplexity(self.original_model, self.tokenizer, validation_texts)
            
            peak_mem = MemoryMonitor.get_total_memory()
            model_size = self._get_model_size(self.config.model_path)
            
            results["original"] = BenchmarkResults(
                model_size_mb=model_size,
                load_time_sec=0,  # Not measured separately
                inference_time_ms=inference_time,
                memory_usage_mb=peak_mem - start_mem,
                perplexity=perplexity
            )
            
            # Clear memory
            del self.original_model
            self.original_model = None
            gc.collect()
            torch.cuda.empty_cache() if torch.cuda.is_available() else None
        
        # Benchmark quantized model
        if self.quantized_model:
            logger.info("Benchmarking quantized model...")
            start_mem = MemoryMonitor.get_total_memory()
            
            inference_time = self._benchmark_inference(self.quantized_model, self.tokenizer)
            perplexity = self._calculate_perplexity(self.quantized_model, self.tokenizer, validation_texts)
            
            peak_mem = MemoryMonitor.get_total_memory()
            model_size = self._get_model_size(self.config.output_path)
            
            results["quantized"] = BenchmarkResults(
                model_size_mb=model_size,
                load_time_sec=0,  # Not measured separately
                inference_time_ms=inference_time,
                memory_usage_mb=peak_mem - start_mem,
                perplexity=perplexity
            )
        
        self.benchmark_results = results
        return results.get("original"), results.get("quantized")
    
    def validate_quantization(self, original_results: BenchmarkResults, 
                            quantized_results: BenchmarkResults) -> bool:
        """Validate that quantization meets quality requirements."""
        logger.info("Validating quantization quality...")
        
        # Check perplexity degradation
        if original_results.perplexity and quantized_results.perplexity:
            perplexity_increase = (quantized_results.perplexity - original_results.perplexity) / original_results.perplexity
            if perplexity_increase > 0.001:  # 0.1% threshold
                logger.warning(f"Perplexity increased by {perplexity_increase:.4f} ({perplexity_increase*100:.2f}%)")
                return False
        
        # Check model size reduction
        size_reduction = (original_results.model_size_mb - quantized_results.model_size_mb) / original_results.model_size_mb
        if size_reduction < 0.1:  # Expect at least 10% reduction
            logger.warning(f"Model size reduction only {size_reduction*100:.2f}%")
        
        # Check memory usage reduction
        memory_reduction = (original_results.memory_usage_mb - quantized_results.memory_usage_mb) / original_results.memory_usage_mb
        if memory_reduction < 0.1:  # Expect at least 10% reduction
            logger.warning(f"Memory usage reduction only {memory_reduction*100:.2f}%")
        
        logger.info("Quantization validation completed successfully")
        return True
    
    def run_full_quantization(self) -> Dict[str, BenchmarkResults]:
        """Run the complete quantization pipeline."""
        logger.info("Starting full quantization pipeline...")
        
        try:
            # Load original model for comparison
            self.load_original_model()
            
            # Quantize model
            self.quantize_model()
            
            # Save quantized model
            self.save_quantized_model()
            
            # Benchmark both models
            original_results, quantized_results = self.benchmark_models()
            
            # Validate quantization
            if original_results and quantized_results:
                self.validate_quantization(original_results, quantized_results)
            
            # Save benchmark results
            results_path = Path(self.config.output_path) / "benchmark_results.json"
            with open(results_path, "w") as f:
                json.dump({
                    "original": original_results.to_dict() if original_results else None,
                    "quantized": quantized_results.to_dict() if quantized_results else None
                }, f, indent=2)
            
            logger.info("Full quantization pipeline completed successfully")
            return self.benchmark_results
            
        except Exception as e:
            logger.error(f"Error in quantization pipeline: {e}")
            logger.error(traceback.format_exc())
            raise


def main():
    """Main execution function."""
    try:
        # Initialize configuration
        config = QuantizationConfig()
        
        # Create quantizer
        quantizer = ModelQuantizer(config)
        
        # Run quantization
        results = quantizer.run_full_quantization()
        
        # Print summary
        print("\n" + "="*60)
        print("QUANTIZATION SUMMARY")
        print("="*60)
        
        if "original" in results and "quantized" in results:
            orig = results["original"]
            quant = results["quantized"]
            
            print(f"Original Model:")
            print(f"  Size: {orig.model_size_mb:.2f} MB")
            print(f"  Inference: {orig.inference_time_ms:.2f} ms")
            print(f"  Memory: {orig.memory_usage_mb:.2f} MB")
            print(f"  Perplexity: {orig.perplexity:.4f}")
            
            print(f"\nQuantized Model:")
            print(f"  Size: {quant.model_size_mb:.2f} MB")
            print(f"  Inference: {quant.inference_time_ms:.2f} ms")
            print(f"  Memory: {quant.memory_usage_mb:.2f} MB")
            print(f"  Perplexity: {quant.perplexity:.4f}")
            
            # Calculate improvements
            size_reduction = (orig.model_size_mb - quant.model_size_mb) / orig.model_size_mb * 100
            memory_reduction = (orig.memory_usage_mb - quant.memory_usage_mb) / orig.memory_usage_mb * 100
            speed_change = (quant.inference_time_ms - orig.inference_time_ms) / orig.inference_time_ms * 100
            perplexity_change = (quant.perplexity - orig.perplexity) / orig.perplexity * 100
            
            print(f"\nImprovements:")
            print(f"  Size reduction: {size_reduction:.1f}%")
            print(f"  Memory reduction: {memory_reduction:.1f}%")
            print(f"  Speed change: {speed_change:+.1f}%")
            print(f"  Perplexity change: {perplexity_change:+.3f}%")
        
        print("\nQuantization completed successfully!")
        
    except Exception as e:
        logger.error(f"Fatal error in main: {e}")
        logger.error(traceback.format_exc())
        sys.exit(1)


if __name__ == "__main__":
    main()
