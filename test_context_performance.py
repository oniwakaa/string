# Generated by SmolLM3-3B on 2025-01-01
"""
Context Performance Testing Script

This script tests the GGUF model with different context window sizes
to measure performance impact and resource usage on MacBook Air M4.
"""

import os
import time
import psutil
import asyncio
from pathlib import Path
from typing import Dict, Any, Optional

try:
    from llama_cpp import Llama
    LLAMA_CPP_AVAILABLE = True
except ImportError:
    print("‚ùå llama-cpp-python not available")
    LLAMA_CPP_AVAILABLE = False

class ContextPerformanceTester:
    """Test performance with different context window sizes."""
    
    def __init__(self, model_path: str):
        """Initialize the tester with model path."""
        self.model_path = Path(model_path)
        self.model = None
        self.results = {}
        
    def get_system_info(self) -> Dict[str, Any]:
        """Get current system resource usage."""
        process = psutil.Process()
        memory_info = process.memory_info()
        
        return {
            'ram_mb': round(memory_info.rss / 1024 / 1024, 2),
            'vram_mb': 'N/A',  # Would need specific Metal API for actual VRAM
            'cpu_percent': process.cpu_percent(),
            'memory_percent': process.memory_percent(),
        }
    
    def test_context_size(self, n_ctx: int, test_prompt: str = None) -> Dict[str, Any]:
        """Test model performance with specific context size."""
        if not LLAMA_CPP_AVAILABLE:
            return {'error': 'llama-cpp-python not available'}
            
        print(f"\nüß™ Testing context size: {n_ctx}")
        
        # Default test prompt
        if test_prompt is None:
            test_prompt = "Explain the concept of machine learning in detail. " * 10
            
        try:
            # Record initial system state
            initial_resources = self.get_system_info()
            print(f"Initial RAM: {initial_resources['ram_mb']} MB")
            
            # Load model with specific context size
            start_time = time.time()
            
            self.model = Llama(
                model_path=str(self.model_path),
                n_ctx=n_ctx,
                n_gpu_layers=-1,  # Use Metal acceleration
                n_batch=512,
                verbose=False,
                use_mmap=True,
                use_mlock=False,
                n_threads=None,
            )
            
            load_time = time.time() - start_time
            post_load_resources = self.get_system_info()
            print(f"Model loaded in {load_time:.2f}s, RAM: {post_load_resources['ram_mb']} MB")
            
            # Test inference with progressively longer prompts
            inference_results = []
            prompt_lengths = [100, 500, 1000, 2000]  # Token approximations
            
            for target_length in prompt_lengths:
                if target_length * 4 > n_ctx:  # Skip if prompt would exceed context
                    continue
                    
                # Create prompt of approximate target length
                words_needed = target_length // 1.3  # Rough tokens to words conversion
                test_text = (test_prompt + " ") * int(words_needed / len(test_prompt.split()))
                
                # Test inference
                start_inference = time.time()
                
                try:
                    response = self.model(
                        test_text,
                        max_tokens=50,  # Short response to focus on context processing
                        temperature=0.7,
                        echo=False
                    )
                    
                    inference_time = time.time() - start_inference
                    post_inference_resources = self.get_system_info()
                    
                    inference_results.append({
                        'prompt_length_approx': target_length,
                        'inference_time': round(inference_time, 3),
                        'ram_mb': post_inference_resources['ram_mb'],
                        'response_length': len(response['choices'][0]['text']) if response.get('choices') else 0,
                    })
                    
                    print(f"  Prompt ~{target_length} tokens: {inference_time:.3f}s")
                
                except Exception as inference_error:
                    print(f"  Prompt ~{target_length} tokens: ERROR - {inference_error}")
                    continue
            
            # Cleanup
            del self.model
            self.model = None
            
            # Final resource check
            final_resources = self.get_system_info()
            
            return {
                'n_ctx': n_ctx,
                'load_time': round(load_time, 3),
                'initial_ram_mb': initial_resources['ram_mb'],
                'peak_ram_mb': post_load_resources['ram_mb'],
                'final_ram_mb': final_resources['ram_mb'],
                'ram_increase': round(post_load_resources['ram_mb'] - initial_resources['ram_mb'], 2),
                'inference_results': inference_results,
                'status': 'success'
            }
            
        except Exception as e:
            if self.model:
                del self.model
                self.model = None
            return {
                'n_ctx': n_ctx,
                'error': str(e),
                'status': 'failed'
            }
    
    def run_comprehensive_test(self) -> Dict[str, Any]:
        """Run tests with multiple context sizes."""
        if not self.model_path.exists():
            return {'error': f'Model not found: {self.model_path}'}
            
        print(f"üöÄ Starting comprehensive context performance testing")
        print(f"Model: {self.model_path}")
        print(f"System: MacBook Air M4")
        
        # Test different context sizes
        context_sizes = [4096, 8192, 16384, 32768]
        
        results = {
            'model_path': str(self.model_path),
            'test_timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'context_tests': {},
            'recommendations': {}
        }
        
        for n_ctx in context_sizes:
            print(f"\n{'='*50}")
            result = self.test_context_size(n_ctx)
            results['context_tests'][n_ctx] = result
            
            if result.get('status') == 'success':
                print(f"‚úÖ Context {n_ctx}: RAM +{result['ram_increase']}MB, Load: {result['load_time']}s")
            else:
                print(f"‚ùå Context {n_ctx}: {result.get('error', 'Failed')}")
                break  # Stop testing larger contexts if current one failed
        
        # Generate recommendations
        successful_tests = {k: v for k, v in results['context_tests'].items() 
                          if v.get('status') == 'success'}
        
        if successful_tests:
            # Find optimal context size based on performance trade-offs
            best_size = 4096
            for size, test_result in successful_tests.items():
                # Use criteria: RAM increase < 8GB, load time < 60s
                if (test_result['ram_increase'] < 8000 and 
                    test_result['load_time'] < 60):
                    best_size = size
                    
            results['recommendations'] = {
                'optimal_context_size': best_size,
                'max_tested_working': max(successful_tests.keys()),
                'resource_limits': '10GB RAM peak, <200ms latency',
                'notes': 'Recommended size balances performance and resource usage'
            }
        
        return results
    
    def save_results(self, results: Dict[str, Any], filename: str = "context_performance_results.json"):
        """Save test results to JSON file."""
        import json
        
        with open(filename, 'w') as f:
            json.dump(results, f, indent=2)
        print(f"\nüìä Results saved to: {filename}")

def main():
    """Main test execution."""
    # Find model file
    possible_paths = [
        "./smollm-quantized/smollm-q4_K_M.gguf",
        "./smollm-quantized/ggml-model-f16.gguf",
        "./smollm/smollm-3b-instruct-q4_k_m.gguf",
    ]
    
    model_path = None
    for path in possible_paths:
        if Path(path).exists():
            model_path = path
            break
    
    if not model_path:
        print(f"‚ùå No model found. Searched: {possible_paths}")
        return
    
    # Run tests
    tester = ContextPerformanceTester(model_path)
    results = tester.run_comprehensive_test()
    
    # Display summary
    print(f"\n{'='*60}")
    print("üìä CONTEXT PERFORMANCE TEST SUMMARY")
    print(f"{'='*60}")
    
    if 'recommendations' in results and results['recommendations']:
        rec = results['recommendations']
        print(f"üéØ Recommended Context Size: {rec['optimal_context_size']}")
        print(f"üìà Maximum Working Size: {rec['max_tested_working']}")
        print(f"üí° {rec['notes']}")
    else:
        print("‚ö†Ô∏è  No successful tests completed - check hardware limits")
        if 'context_tests' in results:
            for size, test in results['context_tests'].items():
                status = test.get('status', 'unknown')
                print(f"   Context {size}: {status}")
                if status == 'failed':
                    print(f"     Error: {test.get('error', 'Unknown error')}")
    
    # Save results
    tester.save_results(results)
    
    print(f"\n‚úÖ Performance testing complete!")
    print(f"üíæ Detailed results saved to context_performance_results.json")

if __name__ == "__main__":
    main() 