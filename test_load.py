# Generated by SmolLM3-3B on 2025-01-12
"""
Minimal GGUF Model Loading Test

This script tests the GGUF model loading in isolation to identify
if the issue is with the model file, llama-cpp-python installation,
or the integration in the main service.
"""

from llama_cpp import Llama

try:
    print("ğŸ”„ Attempting to load GGUF model...")
    print("ğŸ“ Model path: ./smollm-quantized/smollm-q4_K_M.gguf")
    
    llm = Llama(
        model_path="./smollm-quantized/smollm-q4_K_M.gguf",
        n_gpu_layers=-1,  # Offload all layers to GPU (Metal on Apple Silicon)
        n_ctx=4096,
        verbose=True
    )
    
    print("âœ… GGUF model loaded successfully!")
    print(f"ğŸ“Š Model info: {llm.model_path}")
    
    # Test a simple generation to verify functionality
    print("\nğŸ§ª Testing basic generation...")
    try:
        response = llm(
            "Hello, how are you?",
            max_tokens=50,
            temperature=0.7,
            stop=["<|im_end|>", "\n"],
            echo=False
        )
        
        generated_text = response["choices"][0]["text"].strip()
        print(f"âœ… Generation test successful!")
        print(f"ğŸ’¬ Response: {generated_text}")
        
    except Exception as gen_error:
        print(f"âš ï¸  Model loaded but generation failed: {gen_error}")
    
except ImportError as import_error:
    print(f"âŒ Import error - llama-cpp-python not properly installed: {import_error}")
    print("ğŸ’¡ Try: pip install llama-cpp-python")
    
except FileNotFoundError as file_error:
    print(f"âŒ Model file not found: {file_error}")
    print("ğŸ’¡ Check if the model file exists at: ./smollm-quantized/smollm-q4_K_M.gguf")
    
except Exception as e:
    print(f"âŒ Error loading GGUF model: {e}")
    print(f"ğŸ” Error type: {type(e).__name__}")
    
    # Additional diagnostic information
    import os
    model_path = "./smollm-quantized/smollm-q4_K_M.gguf"
    if os.path.exists(model_path):
        file_size = os.path.getsize(model_path) / (1024 * 1024)  # Size in MB
        print(f"ğŸ“„ Model file exists, size: {file_size:.1f} MB")
    else:
        print("ğŸ“„ Model file does not exist")
        
        # Check if directory exists
        model_dir = os.path.dirname(model_path)
        if os.path.exists(model_dir):
            print(f"ğŸ“ Directory exists: {model_dir}")
            print("ğŸ“‹ Files in directory:")
            for file in os.listdir(model_dir):
                print(f"   - {file}")
        else:
            print(f"ğŸ“ Directory does not exist: {model_dir}") 