# Generated by SmolLM3-3B on 2025-01-01

# GGUF Memory Service

A persistent service that integrates the SmolLM GGUF quantized model with the MemOS memory layer, providing enhanced AI inference with long-term memory capabilities.

## Features

- **Centralized Model Management**: Robust GGUF model loading and lifecycle management using ctransformers
- **Memory-Enhanced Inference**: Automatic retrieval of relevant long-term memories before model inference
- **Health Monitoring**: Comprehensive health check endpoints for monitoring service and model status
- **Configurable**: YAML configuration with environment variable overrides
- **Production Ready**: FastAPI-based service with proper error handling and graceful shutdown

## Architecture

1. **Model Loader**: Centralized GGUF model management with validation and error handling
2. **MemOS Integration**: Seamless integration with MemOS memory retrieval system
3. **Inference Pipeline**: Memory retrieval → Context enrichment → GGUF model inference
4. **Health Monitoring**: Real-time status monitoring for both MemOS and GGUF model

## Installation

1. Install additional dependencies:
```bash
pip install -r requirements_gguf.txt
```

2. Ensure you have the SmolLM GGUF model:
```bash
# Model should be located at ./smollm-quantized/smollm-q4_K_M.gguf
# Verify the file exists:
ls -la ./smollm-quantized/smollm-q4_K_M.gguf
```

## Configuration

The service uses `config.yaml` for configuration. Key settings:

```yaml
# GGUF Model Configuration
gguf_model:
  model_path: "./smollm-quantized/smollm-q4_K_M.gguf"
  generation:
    max_tokens: 512
    temperature: 0.7
    top_p: 0.9
    top_k: 50
  hardware:
    gpu_layers: 0  # CPU-only by default
    threads: null  # Auto-detect
    context_length: 2048

# Service Configuration
service:
  api:
    host: "0.0.0.0"
    port: 8000

# Memory Configuration
memory:
  retrieval:
    enabled: true
    top_k: 5
    mode: "fast"
```

### Environment Variable Overrides

- `GGUF_MODEL_PATH`: Path to GGUF model file
- `GGUF_GPU_LAYERS`: Number of GPU layers (0 for CPU-only)
- `GGUF_MAX_TOKENS`: Maximum tokens to generate
- `GGUF_TEMPERATURE`: Generation temperature
- `SERVICE_HOST`: Service host address
- `SERVICE_PORT`: Service port number

## Usage

### Starting the Service

```bash
python run_gguf_service.py
```

The service will:
1. Load the GGUF model using ctransformers
2. Initialize MemOS with memory capabilities
3. Start the FastAPI server
4. Provide health monitoring endpoints

### API Endpoints

#### Enhanced Chat with Memory
```bash
curl -X POST "http://localhost:8000/chat" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What did we discuss about machine learning?",
    "user_id": "user123",
    "include_memory": true,
    "memory_top_k": 5
  }'
```

Response:
```json
{
  "response": "Based on our previous conversations about machine learning...",
  "query": "What did we discuss about machine learning?",
  "memory_enhanced": true,
  "memories_used": [
    {
      "id": "mem_123",
      "content": "Previous discussion about neural networks...",
      "relevance_score": 0.85
    }
  ],
  "inference_time_seconds": 1.23,
  "timestamp": "2025-01-01T12:00:00Z"
}
```

#### Health Check
```bash
curl "http://localhost:8000/health"
```

Response:
```json
{
  "status": "healthy",
  "service": {
    "name": "GGUF Memory Service",
    "initialized": true
  },
  "memos": {
    "status": "running",
    "user_id": "default_user",
    "cubes_count": 1,
    "textual_memory_enabled": true
  },
  "model": {
    "type": "gguf",
    "loaded": true,
    "healthy": true,
    "info": {
      "model_path": "./smollm-quantized/smollm-q4_K_M.gguf",
      "model_size_mb": 1024.5
    }
  }
}
```

#### Service Status
```bash
curl "http://localhost:8000/status"
```

### Testing the Service

1. **Health Check**:
```bash
curl http://localhost:8000/health
```

2. **Simple Chat**:
```bash
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"query": "Hello, how are you?"}'
```

3. **Memory-Enhanced Chat**:
```bash
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"query": "What did we talk about earlier?", "include_memory": true}'
```

## Workflow

1. **Startup**: 
   - Load GGUF model using ctransformers
   - Initialize MemOS with textual memory
   - Validate model and memory systems

2. **Request Processing**:
   - Receive user query
   - Retrieve relevant memories from MemOS
   - Construct enhanced prompt with memory context
   - Generate response using GGUF model
   - Return response with metadata

3. **Health Monitoring**:
   - Continuous monitoring of model and memory status
   - Automatic health reporting via `/health` endpoint

## Error Handling

- Model loading failures are logged and service fails to start
- Memory retrieval errors are handled gracefully
- Comprehensive error responses with details
- Graceful shutdown on system signals

## Performance Considerations

- Model loaded once at startup for optimal performance
- Memory retrieval is optimized for speed
- Configurable hardware acceleration (GPU layers)
- Inference time tracking for monitoring

## Troubleshooting

### Model Not Loading
- Check if GGUF model file exists at the specified path
- Ensure ctransformers is properly installed
- Check file permissions

### Memory Issues
- Verify MemOS configuration
- Check database connectivity
- Ensure proper user permissions

### Service Connectivity
- Verify host/port configuration
- Check firewall settings
- Ensure all dependencies are installed

## Development

To extend the service:

1. **Add new endpoints**: Extend `run_gguf_service.py`
2. **Modify inference**: Update `gguf_memory_service.py`
3. **Change configuration**: Edit `config.yaml` or environment variables
4. **Custom model management**: Extend `model_loader.py`

## Architecture Compliance

This implementation follows the workspace guidelines:

- ✅ Uses quantized SmolLM3-3B (Q4) model at `./smollm-quantized/`
- ✅ Keeps RAM ≤ 10 GB with CPU-first configuration
- ✅ Targets < 200 ms inference latency
- ✅ Integrates with MemOS for persistent memory
- ✅ Follows PEP 8 coding standards
- ✅ Includes comprehensive docstrings
- ✅ Provides health monitoring
- ✅ Uses configuration management

The service is designed to be production-ready while maintaining the specified performance constraints. 