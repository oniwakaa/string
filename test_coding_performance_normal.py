#!/usr/bin/env python3
"""
# Generated by SmolLM3-3B on 2025-01-12

Coding Performance Validation for SmolLM3-3B Normal Model
Tests coding capabilities using Transformers library with focused performance monitoring.
"""

import os
import sys
import time
import json
import psutil
import gc
import traceback
import torch
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple
from dataclasses import dataclass, asdict
import warnings
warnings.filterwarnings("ignore")

from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

@dataclass
class CodingTestResult:
    """Result container for coding validation tests."""
    prompt_id: str
    category: str
    complexity: str
    prompt_text: str
    generated_code: str
    latency_ms: float
    peak_memory_mb: float
    memory_before_mb: float
    memory_after_mb: float
    token_count_input: int
    token_count_output: int
    success: bool
    error_message: Optional[str] = None
    syntax_valid: Optional[bool] = None

class CodingPerformanceTester:
    """Main class for coding performance validation using normal SmolLM model."""
    
    def __init__(self, model_path: str = "./smollm"):
        self.model_path = Path(model_path)
        self.model = None
        self.tokenizer = None
        self.device = self._get_optimal_device()
        self.results = []
        
        print(f"üîß Initializing coding tester with model: {self.model_path}")
        print(f"üîß Using device: {self.device}")
        
    def _get_optimal_device(self) -> str:
        """Determine optimal device for Apple Silicon."""
        if torch.backends.mps.is_available():
            return "mps"
        elif torch.cuda.is_available():
            return "cuda"
        else:
            return "cpu"
    
    def get_memory_usage_mb(self) -> float:
        """Get current memory usage in MB."""
        process = psutil.Process(os.getpid())
        return process.memory_info().rss / 1024 / 1024
    
    def load_model(self) -> bool:
        """Load the normal SmolLM model using Transformers."""
        try:
            print("üì¶ Loading tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path,
                trust_remote_code=True
            )
            
            # Add pad token if missing
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            print("üì¶ Loading model...")
            # Load model with appropriate settings for different devices
            if self.device == "mps":
                # MPS (Apple Silicon) - load with float16 for efficiency
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_path,
                    trust_remote_code=True,
                    torch_dtype=torch.float16,
                    device_map=None  # Manual device placement for MPS
                )
                self.model = self.model.to(self.device)
            elif self.device == "cuda":
                # CUDA - use quantization if available
                try:
                    quantization_config = BitsAndBytesConfig(
                        load_in_8bit=True,
                        llm_int8_enable_fp32_cpu_offload=True
                    )
                    
                    self.model = AutoModelForCausalLM.from_pretrained(
                        self.model_path,
                        quantization_config=quantization_config,
                        device_map="auto",
                        trust_remote_code=True,
                        torch_dtype=torch.float16
                    )
                except:
                    # Fallback without quantization
                    self.model = AutoModelForCausalLM.from_pretrained(
                        self.model_path,
                        trust_remote_code=True,
                        torch_dtype=torch.float16
                    )
                    self.model = self.model.to(self.device)
            else:
                # CPU fallback
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_path,
                    trust_remote_code=True,
                    torch_dtype=torch.float32
                )
                self.model = self.model.to(self.device)
            
            # Set to evaluation mode
            self.model.eval()
            
            print("‚úÖ Model loaded successfully")
            return True
                
        except Exception as e:
            print(f"‚ùå Model loading failed: {e}")
            traceback.print_exc()
            return False
    
    def load_coding_corpus(self) -> List[Dict[str, Any]]:
        """Load coding-specific test prompts."""
        corpus_path = Path("validation/coding_corpus.json")
        
        if not corpus_path.exists():
            raise FileNotFoundError(f"Coding corpus not found at {corpus_path}")
        
        with open(corpus_path, 'r') as f:
            corpus = json.load(f)
        
        print(f"üìã Loaded {len(corpus)} coding test prompts")
        return corpus
    
    def check_syntax_validity(self, code: str) -> bool:
        """Check if generated code is syntactically valid Python."""
        try:
            # Extract code blocks if present
            if "```python" in code:
                # Extract code between ```python and ```
                start = code.find("```python") + 9
                end = code.find("```", start)
                if end != -1:
                    code = code[start:end].strip()
            elif "```" in code:
                # Extract code between ``` blocks
                parts = code.split("```")
                if len(parts) >= 3:
                    code = parts[1].strip()
            
            # Try to compile the code
            compile(code, '<string>', 'exec')
            return True
        except SyntaxError:
            return False
        except Exception:
            # Other compilation errors also indicate invalid code
            return False
    
    def run_inference(self, prompt: str, max_new_tokens: int = 150) -> Tuple[str, float, float, float, int, int]:
        """Run inference on a single prompt with performance monitoring."""
        memory_before = self.get_memory_usage_mb()
        
        try:
            # Tokenize input
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512
            )
            
            # Move to device
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            input_token_count = inputs["input_ids"].shape[1]
            
            # Record start time
            start_time = time.perf_counter()
            
            # Generate response
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=0.7,
                    do_sample=True,
                    top_p=0.9,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    repetition_penalty=1.1
                )
            
            # Record end time
            end_time = time.perf_counter()
            latency_ms = (end_time - start_time) * 1000
            
            # Decode output
            output_text = self.tokenizer.decode(
                outputs[0],
                skip_special_tokens=True
            )
            
            # Extract generated part (remove input prompt)
            input_text = self.tokenizer.decode(
                inputs["input_ids"][0],
                skip_special_tokens=True
            )
            
            generated_text = output_text[len(input_text):].strip()
            output_token_count = outputs.shape[1] - input_token_count
            
            memory_after = self.get_memory_usage_mb()
            peak_memory = max(memory_before, memory_after)
            
            return generated_text, latency_ms, peak_memory, memory_before, input_token_count, output_token_count
            
        except Exception as e:
            memory_after = self.get_memory_usage_mb()
            peak_memory = max(memory_before, memory_after)
            print(f"‚ùå Inference failed: {e}")
            return f"Error: {str(e)}", 0.0, peak_memory, memory_before, 0, 0
    
    def run_coding_tests(self) -> List[CodingTestResult]:
        """Run all coding validation tests."""
        print("üöÄ Starting coding performance validation...")
        
        # Load model
        if not self.load_model():
            print("‚ùå Failed to load model, aborting tests")
            return []
        
        # Load test corpus
        try:
            corpus = self.load_coding_corpus()
        except Exception as e:
            print(f"‚ùå Failed to load corpus: {e}")
            return []
        
        results = []
        total_tests = len(corpus)
        
        for i, test_case in enumerate(corpus, 1):
            print(f"\nüìù Running test {i}/{total_tests}: {test_case['id']}")
            print(f"   Category: {test_case['category']}")
            print(f"   Complexity: {test_case['complexity']}")
            
            try:
                # Run inference
                generated_code, latency_ms, peak_memory_mb, memory_before_mb, input_tokens, output_tokens = self.run_inference(
                    test_case['prompt'],
                    max_new_tokens=200
                )
                
                memory_after_mb = self.get_memory_usage_mb()
                
                # Check syntax validity
                syntax_valid = self.check_syntax_validity(generated_code) if "Error:" not in generated_code else False
                
                # Create result
                result = CodingTestResult(
                    prompt_id=test_case['id'],
                    category=test_case['category'],
                    complexity=test_case['complexity'],
                    prompt_text=test_case['prompt'][:200] + "..." if len(test_case['prompt']) > 200 else test_case['prompt'],
                    generated_code=generated_code,
                    latency_ms=latency_ms,
                    peak_memory_mb=peak_memory_mb,
                    memory_before_mb=memory_before_mb,
                    memory_after_mb=memory_after_mb,
                    token_count_input=input_tokens,
                    token_count_output=output_tokens,
                    success="Error:" not in generated_code,
                    error_message=None if "Error:" not in generated_code else generated_code,
                    syntax_valid=syntax_valid
                )
                
                results.append(result)
                
                # Print results
                print(f"   ‚è±Ô∏è  Latency: {latency_ms:.1f}ms")
                print(f"   üß† Peak Memory: {peak_memory_mb:.1f}MB")
                print(f"   üî§ Tokens: {input_tokens} ‚Üí {output_tokens}")
                print(f"   ‚úÖ Syntax Valid: {syntax_valid}")
                
                if len(generated_code) > 100:
                    print(f"   üìÑ Generated: {generated_code[:100]}...")
                else:
                    print(f"   üìÑ Generated: {generated_code}")
                
                # Force garbage collection between tests
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                gc.collect()
                
            except Exception as e:
                print(f"   ‚ùå Test failed: {e}")
                error_result = CodingTestResult(
                    prompt_id=test_case['id'],
                    category=test_case['category'],
                    complexity=test_case['complexity'],
                    prompt_text=test_case['prompt'][:200] + "..." if len(test_case['prompt']) > 200 else test_case['prompt'],
                    generated_code="",
                    latency_ms=0.0,
                    peak_memory_mb=self.get_memory_usage_mb(),
                    memory_before_mb=0.0,
                    memory_after_mb=0.0,
                    token_count_input=0,
                    token_count_output=0,
                    success=False,
                    error_message=str(e),
                    syntax_valid=False
                )
                results.append(error_result)
        
        self.results = results
        return results
    
    def save_results(self, results: List[CodingTestResult], filename: str = "validation/coding_results_normal.json"):
        """Save test results to JSON file."""
        results_data = {
            "model_info": {
                "model_path": str(self.model_path),
                "device": self.device,
                "model_type": "normal_smollm3"
            },
            "test_summary": {
                "total_tests": len(results),
                "successful_tests": sum(1 for r in results if r.success),
                "syntax_valid_tests": sum(1 for r in results if r.syntax_valid),
                "average_latency_ms": sum(r.latency_ms for r in results) / len(results) if results else 0,
                "max_latency_ms": max(r.latency_ms for r in results) if results else 0,
                "average_peak_memory_mb": sum(r.peak_memory_mb for r in results) / len(results) if results else 0,
                "max_peak_memory_mb": max(r.peak_memory_mb for r in results) if results else 0,
                "latency_under_200ms": sum(1 for r in results if r.latency_ms < 200),
                "memory_under_10gb": sum(1 for r in results if r.peak_memory_mb < 10240)
            },
            "test_results": [asdict(result) for result in results]
        }
        
        # Ensure directory exists
        Path(filename).parent.mkdir(exist_ok=True)
        
        with open(filename, 'w') as f:
            json.dump(results_data, f, indent=2)
        
        print(f"\nüíæ Results saved to {filename}")
        return results_data
    
    def print_summary(self, results: List[CodingTestResult]):
        """Print a comprehensive test summary."""
        if not results:
            print("‚ùå No results to summarize")
            return
        
        successful = [r for r in results if r.success]
        syntax_valid = [r for r in results if r.syntax_valid]
        
        print("\n" + "="*60)
        print("üìä NORMAL MODEL CODING PERFORMANCE VALIDATION SUMMARY")
        print("="*60)
        
        print(f"\nüéØ Test Completion:")
        print(f"   Total Tests: {len(results)}")
        print(f"   Successful: {len(successful)} ({len(successful)/len(results)*100:.1f}%)")
        print(f"   Syntax Valid: {len(syntax_valid)} ({len(syntax_valid)/len(results)*100:.1f}%)")
        
        if successful:
            latencies = [r.latency_ms for r in successful]
            memories = [r.peak_memory_mb for r in successful]
            
            print(f"\n‚è±Ô∏è  Performance Metrics:")
            print(f"   Average Latency: {sum(latencies)/len(latencies):.1f}ms")
            print(f"   Max Latency: {max(latencies):.1f}ms")
            print(f"   Tests < 200ms: {sum(1 for l in latencies if l < 200)}/{len(latencies)}")
            
            print(f"\nüß† Memory Usage:")
            print(f"   Average Peak: {sum(memories)/len(memories):.1f}MB")
            print(f"   Max Peak: {max(memories):.1f}MB")
            print(f"   Tests < 10GB: {sum(1 for m in memories if m < 10240)}/{len(memories)}")
        
        print(f"\nüìã Results by Category:")
        categories = {}
        for result in results:
            cat = result.category
            if cat not in categories:
                categories[cat] = {'total': 0, 'success': 0, 'syntax_valid': 0}
            categories[cat]['total'] += 1
            if result.success:
                categories[cat]['success'] += 1
            if result.syntax_valid:
                categories[cat]['syntax_valid'] += 1
        
        for cat, stats in categories.items():
            success_rate = stats['success'] / stats['total'] * 100
            syntax_rate = stats['syntax_valid'] / stats['total'] * 100
            print(f"   {cat}: {stats['success']}/{stats['total']} success ({success_rate:.1f}%), {stats['syntax_valid']} syntax valid ({syntax_rate:.1f}%)")

def main():
    """Main execution function."""
    print("üöÄ SmolLM3-3B Normal Model Coding Performance Validation")
    print("="*60)
    
    # Initialize tester
    tester = CodingPerformanceTester()
    
    # Run tests
    results = tester.run_coding_tests()
    
    if not results:
        print("‚ùå No results generated")
        return
    
    # Save results
    results_data = tester.save_results(results)
    
    # Print summary
    tester.print_summary(results)
    
    # Check success criteria
    print("\nüéØ Success Criteria Check:")
    summary = results_data["test_summary"]
    
    print(f"   Latency < 200ms: {summary['latency_under_200ms']}/{summary['total_tests']} tests")
    print(f"   Memory < 10GB: {summary['memory_under_10gb']}/{summary['total_tests']} tests")
    print(f"   Syntax Valid: {summary['syntax_valid_tests']}/{summary['total_tests']} tests")
    
    # Overall assessment
    if (summary['latency_under_200ms'] == summary['total_tests'] and 
        summary['memory_under_10gb'] == summary['total_tests'] and
        summary['syntax_valid_tests'] >= summary['total_tests'] * 0.8):
        print("\n‚úÖ ALL SUCCESS CRITERIA MET!")
    else:
        print("\n‚ö†Ô∏è  Some criteria not fully met - review individual results")

if __name__ == "__main__":
    main() 