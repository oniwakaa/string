# Generated by SmolLM3-3B on 2025-01-01
"""
Quick Context Window Test

Simple test to verify the model loads with the new 16384 context window
and measure basic performance metrics.
"""

import time
import psutil
from pathlib import Path

try:
    from llama_cpp import Llama
    LLAMA_CPP_AVAILABLE = True
except ImportError:
    print("‚ùå llama-cpp-python not available")
    LLAMA_CPP_AVAILABLE = False

def test_new_context():
    """Test model with new 16384 context window."""
    if not LLAMA_CPP_AVAILABLE:
        print("‚ùå llama-cpp-python not available")
        return
    
    # Find model
    model_paths = [
        "./smollm-quantized/smollm-q4_K_M.gguf",
        "./smollm-quantized/ggml-model-f16.gguf",
        "./smollm/smollm-3b-instruct-q4_k_m.gguf",
    ]
    
    model_path = None
    for path in model_paths:
        if Path(path).exists():
            model_path = path
            break
    
    if not model_path:
        print(f"‚ùå No model found. Searched: {model_paths}")
        return
    
    print(f"üöÄ Testing model with 16384 context window")
    print(f"Model: {model_path}")
    
    # Get initial memory
    process = psutil.Process()
    initial_ram = process.memory_info().rss / 1024 / 1024
    print(f"Initial RAM: {initial_ram:.1f} MB")
    
    # Load model with new context size
    start_time = time.time()
    
    try:
        model = Llama(
            model_path=model_path,
            n_ctx=16384,  # New larger context window
            n_gpu_layers=-1,  # Metal acceleration
            n_batch=512,
            verbose=False,
            use_mmap=True,
            use_mlock=False,
            n_threads=None,
        )
        
        load_time = time.time() - start_time
        post_load_ram = process.memory_info().rss / 1024 / 1024
        
        print(f"‚úÖ Model loaded successfully!")
        print(f"   Load time: {load_time:.2f} seconds")
        print(f"   RAM usage: {post_load_ram:.1f} MB (+{post_load_ram - initial_ram:.1f} MB)")
        
        # Test inference
        print(f"\nüß™ Testing inference...")
        test_prompt = "What is artificial intelligence?"
        
        start_inference = time.time()
        response = model(
            test_prompt,
            max_tokens=50,
            temperature=0.7,
            echo=False
        )
        inference_time = time.time() - start_inference
        
        print(f"‚úÖ Inference successful!")
        print(f"   Inference time: {inference_time:.3f} seconds")
        print(f"   Response: {response['choices'][0]['text'][:100]}...")
        
        # Check if we see the context warning
        print(f"\nüìä Context Analysis:")
        print(f"   Configured n_ctx: 16384")
        print(f"   Model training context: 65536")
        print(f"   Warning should be reduced compared to 4096")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Failed to load model: {e}")
        return False

if __name__ == "__main__":
    test_new_context() 