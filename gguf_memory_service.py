# Generated by SmolLM3-3B on 2025-01-12
"""
GGUF Memory Service - Direct llama-cpp-python Integration

This module provides a persistent service that uses llama-cpp-python directly
for GGUF model inference, with optional MemOS memory enhancement.
"""

import asyncio
import logging
import os
import sys
from datetime import datetime
from typing import Dict, Any, Optional, List

# Add MemOS to Python path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'MemOS', 'src'))

from config_loader import ConfigLoader, load_config

# Import llama-cpp-python for direct GGUF model loading
try:
    from llama_cpp import Llama
    LLAMA_CPP_AVAILABLE = True
except ImportError as e:
    print(f"âŒ llama-cpp-python not available: {e}")
    print("ðŸ’¡ Please install with: pip install llama-cpp-python")
    LLAMA_CPP_AVAILABLE = False

# Optional MemOS imports for memory functionality
try:
    from memos.configs.mem_os import MOSConfig
    from memos.mem_os.main import MOS
    from memos.log import get_logger
    from memos.mem_user.user_manager import UserManager, UserRole
    logger = get_logger(__name__)
    MEMOS_AVAILABLE = True
except ImportError as e:
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)
    logger.warning(f"MemOS not available (memory features disabled): {e}")
    MEMOS_AVAILABLE = False

# Import our custom LLM wrapper
try:
    from llama_cpp_wrapper import LlamaCppWrapper
    WRAPPER_AVAILABLE = True
except ImportError as e:
    logger.warning(f"LlamaCpp wrapper not available: {e}")
    WRAPPER_AVAILABLE = False

# Import project memory manager
try:
    from project_memory_manager import ProjectMemoryManager
    PROJECT_MEMORY_AVAILABLE = True
except ImportError as e:
    logger.warning(f"Project memory manager not available: {e}")
    PROJECT_MEMORY_AVAILABLE = False


class GGUFMemoryService:
    """
    GGUF service that uses llama-cpp-python directly for model inference,
    with optional MemOS memory enhancement when available.
    """
    
    def __init__(self, config_path: str = "config.yaml"):
        """
        Initialize the GGUF Memory Service.
        
        Args:
            config_path (str): Path to the configuration file
        """
        self.config_loader = ConfigLoader(config_path)
        self.config = self.config_loader.load()
        self.mos_instance = None  # MemOS instance for memory management
        self.llama_wrapper = None  # Our custom LLM wrapper
        self.llm = None  # Direct llama-cpp-python Llama instance
        self._is_initialized = False
        
        # Initialize project memory manager
        self.project_memory_manager = None
        if PROJECT_MEMORY_AVAILABLE:
            self.project_memory_manager = ProjectMemoryManager()
        else:
            logger.warning("Project memory manager not available")
        
        # Check dependencies
        if not LLAMA_CPP_AVAILABLE:
            raise RuntimeError("llama-cpp-python is required but not available")
        
        logger.info("GGUF Memory Service initialized")
    
    async def startup(self) -> bool:
        """
        Start up the service by loading the GGUF model directly and optionally initializing MemOS.
        
        Returns:
            bool: True if startup successful, False otherwise
        """
        try:
            logger.info("Starting GGUF Memory Service...")
            
            # Initialize GGUF model directly
            if not await self._initialize_gguf_model():
                return False
            
            # Initialize MemOS if available (for memory features)
            if MEMOS_AVAILABLE:
                await self._initialize_memos()
            else:
                logger.warning("MemOS not available - memory features disabled")
            
            self._is_initialized = True
            self._startup_time = datetime.now().isoformat()
            logger.info("GGUF Memory Service startup completed successfully")
            return True
            
        except Exception as e:
            logger.error(f"Failed to start GGUF Memory Service: {e}")
            return False
    
    async def _initialize_gguf_model(self) -> bool:
        """
        Initialize the GGUF model using llama-cpp-python directly.
        
        Returns:
            bool: True if initialization successful
        """
        try:
            model_config = self.config.get('model', {})
            
            # Get model path - try multiple possible locations
            configured_path = model_config.get('model_path', './smollm-quantized/smollm-q4_K_M.gguf')
            
            possible_model_paths = [
                './smollm-quantized/smollm-q4_K_M.gguf',
                './smollm-quantized/ggml-model-f16.gguf',
                './smollm/smollm-3b-instruct-q4_k_m.gguf',
                configured_path  # Try configured path last
            ]
            
            # Remove duplicates while preserving order
            seen = set()
            possible_model_paths = [x for x in possible_model_paths if not (x in seen or seen.add(x))]
            
            model_path = None
            for path in possible_model_paths:
                if os.path.exists(path):
                    model_path = path
                    break
            
            if not model_path:
                logger.error(f"No GGUF model found. Searched paths: {possible_model_paths}")
                return False
            
            logger.info(f"Loading GGUF model from: {model_path}")
            
            # Get generation config
            generation_config = model_config.get('generation', {})
            
            # Initialize Llama model with optimal settings for Apple Silicon
            self.llm = Llama(
                model_path=model_path,
                n_gpu_layers=-1,  # Offload all layers to GPU (Metal on Apple Silicon)
                n_ctx=generation_config.get('n_ctx', 16384),  # Increased from 4096 to better utilize model's training context
                n_batch=generation_config.get('n_batch', 512),
                verbose=False,
                # Additional performance optimizations
                use_mmap=True,
                use_mlock=False,
                n_threads=None,  # Auto-detect optimal thread count
            )
            
            logger.info("âœ… GGUF model loaded successfully with Metal acceleration")
            
            # Test basic functionality
            test_response = self.llm(
                "Hello",
                max_tokens=5,
                temperature=0.1,
                echo=False
            )
            
            if test_response and test_response.get('choices'):
                logger.info("âœ… GGUF model health check passed")
                return True
            else:
                logger.error("âŒ GGUF model health check failed")
                return False
            
        except Exception as e:
            logger.error(f"Failed to initialize GGUF model: {e}")
            return False
    
    async def _initialize_memos(self) -> bool:
        """
        Initialize the MemOS instance with our custom LLM wrapper.
        
        Returns:
            bool: True if initialization successful
        """
        try:
            if not MEMOS_AVAILABLE or not WRAPPER_AVAILABLE:
                logger.warning("MemOS or LlamaCpp wrapper not available - skipping memory initialization")
                return True
            
            if not self.llm:
                logger.error("GGUF model must be loaded before initializing MemOS")
                return False
            
            logger.info("Initializing MemOS with LlamaCpp wrapper...")
            
            # Create our custom LLM wrapper around the loaded model
            self.llama_wrapper = LlamaCppWrapper(self.llm)
            logger.info("âœ… LlamaCpp wrapper created successfully")
            
            # Patch the GGUF LLM class to use our pre-loaded model
            self._patch_gguf_llm_for_memos()
            
            # Get MemOS configuration from config file
            memos_config = self.config.get('memos', {})
            user_id = memos_config.get('user_id', 'default_user')
            
            # Create user manager and ensure default user exists
            user_manager = UserManager()
            if not user_manager.validate_user(user_id):
                user_manager.create_user(
                    user_name=user_id,
                    role=UserRole.USER,
                    user_id=user_id
                )
                logger.info(f"Created default user: {user_id}")
            
            # Create MemOS configuration with standard GGUF backend
            mos_config_dict = {
                'user_id': user_id,
                'session_id': memos_config.get('session_id', 'default_session'),
                'enable_textual_memory': memos_config.get('enable_textual_memory', True),
                'enable_activation_memory': memos_config.get('enable_activation_memory', False),
                'top_k': memos_config.get('top_k', 5),
                'chat_model': {
                    'backend': 'gguf',
                    'config': {
                        'model_name_or_path': './smollm-quantized/smollm-q4_K_M.gguf',
                        'temperature': 0.7,
                        'max_tokens': 512,
                        'top_p': 0.9,
                        'top_k': 50,
                    }
                },
                'mem_reader': {
                    'backend': 'simple_struct',
                    'config': {
                        'llm': {
                            'backend': 'gguf',
                            'config': {
                                'model_name_or_path': './smollm-quantized/smollm-q4_K_M.gguf',
                                'temperature': 0.7,
                                'max_tokens': 512,
                                'top_p': 0.9,
                                'top_k': 50,
                            }
                        },
                        'embedder': {
                            'backend': 'sentence_transformer',
                            'config': {
                                'model_name_or_path': 'all-MiniLM-L6-v2'
                            }
                        },
                        'chunker': {
                            'backend': 'sentence',
                            'config': {
                                'tokenizer_or_token_counter': 'gpt2',
                                'chunk_size': 512,
                                'chunk_overlap': 128,
                                'min_sentences_per_chunk': 1
                            }
                        }
                    }
                }
            }
            
            # Create MemOS configuration object
            mos_config = MOSConfig(**mos_config_dict)
            
            # Initialize MemOS with our configuration  
            self.mos_instance = MOS(mos_config)
            
            # Replace the chat_llm with our wrapper after initialization
            self.mos_instance.chat_llm = self.llama_wrapper
            
            # Set up project memory manager with the MemOS instance
            if self.project_memory_manager:
                self.project_memory_manager.set_mos_instance(self.mos_instance)
                logger.info("âœ… Project memory manager configured with MemOS instance")
            
            logger.info("âœ… MemOS initialized successfully with LlamaCpp wrapper")
            
            # Register existing memory cubes if available
            try:
                import os
                memory_cubes_dir = "./memory_cubes"
                if os.path.exists(memory_cubes_dir):
                    for user_dir in os.listdir(memory_cubes_dir):
                        user_path = os.path.join(memory_cubes_dir, user_dir)
                        if os.path.isdir(user_path):
                            for cube_dir in os.listdir(user_path):
                                cube_path = os.path.join(user_path, cube_dir)
                                if os.path.isdir(cube_path) and os.path.exists(os.path.join(cube_path, "config.json")):
                                    try:
                                        self.mos_instance.register_mem_cube(
                                            mem_cube_name_or_path=cube_path,
                                            mem_cube_id=cube_dir,
                                            user_id=user_dir
                                        )
                                        logger.info(f"âœ… Registered memory cube: {cube_dir} for user {user_dir}")
                                    except Exception as e:
                                        logger.warning(f"âš ï¸ Failed to register cube {cube_dir}: {e}")
            except Exception as e:
                logger.warning(f"âš ï¸ Error registering memory cubes: {e}")
            
            # Test basic functionality
            try:
                test_response = self.mos_instance.chat("Hello", user_id=user_id)
                logger.info("âœ… MemOS health check passed")
                return True
            except Exception as e:
                logger.error(f"âŒ MemOS health check failed: {e}")
                return False
            
        except Exception as e:
            logger.error(f"Failed to initialize MemOS: {e}")
            logger.error(f"Full error: {e.__class__.__name__}: {e}")
            import traceback
            traceback.print_exc()
            return True  # Non-critical failure - continue without memory features
    
    def _patch_gguf_llm_for_memos(self):
        """Patch the MemOS GGUF LLM class to use our pre-loaded model."""
        try:
            from memos.llms.gguf import GGUFLLLM
            
            # Store the original __init__ method
            original_init = GGUFLLLM.__init__
            
            # Create a patched __init__ that returns our wrapper
            def patched_init(llm_self, config):
                # Instead of loading a new model, use our wrapper
                llm_self.config = config
                llm_self.llama_wrapper = self.llama_wrapper
                logger.info("Using patched GGUF LLM with pre-loaded model")
            
            # Store original generate method
            original_generate = GGUFLLLM.generate
            
            # Create patched generate method that delegates to our wrapper
            def patched_generate(llm_self, messages, **kwargs):
                return self.llama_wrapper.generate(messages, **kwargs)
            
            # Apply the patches
            GGUFLLLM.__init__ = patched_init
            GGUFLLLM.generate = patched_generate
            
            logger.info("âœ… Successfully patched MemOS GGUF LLM class")
            
        except Exception as e:
            logger.error(f"Failed to patch MemOS GGUF LLM class: {e}")
            raise
    
    async def memos_chat(
        self,
        query: str,
        user_id: Optional[str] = None,
        project_id: Optional[str] = None,
        include_memory: bool = True,
        **kwargs
    ) -> Dict[str, Any]:
        """
        Perform memory-aware chat using MemOS with project-specific memory isolation.
        
        Args:
            query (str): User query/prompt
            user_id (Optional[str]): User ID for memory retrieval and context
            project_id (Optional[str]): Project ID for memory isolation (default: "default")
            include_memory (bool): Whether to use memory (always True for MemOS)
            **kwargs: Additional parameters (for compatibility)
            
        Returns:
            Dict[str, Any]: Response with generated text and metadata
        """
        if not self._is_initialized:
            raise RuntimeError("Service not initialized. Call startup() first.")
        
        if not self.mos_instance:
            # Fallback to enhanced_inference if MemOS is not available
            logger.warning("MemOS not available, falling back to enhanced_inference")
            return await self.enhanced_inference(query, user_id, include_memory, **kwargs)
        
        start_time = datetime.now()
        
        try:
            # Use MemOS for memory-aware chat with project-specific context
            effective_user_id = user_id or self.config.get('memos', {}).get('user_id', 'default_user')
            effective_project_id = project_id or "default"
            
            logger.info(f"ðŸ§  [MemOS Chat] Processing query for user {effective_user_id}, project {effective_project_id}: {query[:100]}...")
            
            # First search for codebase context to enhance the query
            codebase_context = ""
            memories_used = []
            
            try:
                # Use project memory manager for project-specific search if available
                if self.project_memory_manager:
                    # Ensure project cube exists and search within project context
                    project_cube_id = self.project_memory_manager.get_or_create_project_cube(
                        effective_user_id, effective_project_id
                    )
                    if project_cube_id:
                        # Search specifically within the project's memory cube
                        search_result = self.mos_instance.search(
                            query=query, 
                            user_id=effective_user_id,
                            install_cube_ids=[project_cube_id]
                        )
                        logger.info(f"ðŸ” [Project Search] Searching project cube: {project_cube_id}")
                    else:
                        # Fallback to user-wide search
                        search_result = self.mos_instance.search(query=query, user_id=effective_user_id)
                        logger.warning(f"âš ï¸ [Search Fallback] Project cube creation failed, using user-wide search")
                else:
                    # Fallback to original behavior
                    search_result = self.mos_instance.search(query=query, user_id=effective_user_id)
                    logger.info(f"ðŸ” [Legacy Search] Using legacy user-wide search")
                
                if search_result and search_result.get('text_mem'):
                    logger.info(f"ðŸ” [Search Results] Found {len(search_result['text_mem'])} cube results")
                    
                    # Filter for codebase-related memories
                    codebase_memories = []
                    for cube_result in search_result['text_mem']:
                        cube_memories = cube_result.get('memories', [])
                        logger.info(f"ðŸ“š [Cube Analysis] Cube has {len(cube_memories)} memories")
                        
                        for memory in cube_memories[:5]:  # Top 5 most relevant
                            # Check if this memory contains code-like content
                            memory_content = memory.memory
                            if any(keyword in memory_content.lower() for keyword in [
                                'def ', 'function', 'class ', 'import ', 'return ', 
                                '.py', 'file:', 'path:', 'calculate_fibonacci', 
                                'textanalyzer', 'math_utils', 'string_processor',
                                'quantum_flux_capacitor', 'dilithium_crystals', 'quantumcomputer',
                                'unique_functions', 'temporal_coefficient', 'energy_level'
                            ]):
                                codebase_memories.append(memory_content)
                                memories_used.append({
                                    'id': memory.id,
                                    'content': memory_content[:200] + '...' if len(memory_content) > 200 else memory_content,
                                    'relevance_score': getattr(memory, 'score', 0.0),
                                    'source': 'codebase'
                                })
                    
                    if codebase_memories:
                        codebase_context = f"\n\nRelevant codebase context:\n{chr(10).join(codebase_memories[:3])}\n"
                        logger.info(f"âœ… [Codebase Context] Found {len(codebase_memories)} relevant code memories")
                    else:
                        logger.info("âš ï¸ [Codebase Context] No code-related memories found")
                else:
                    logger.info("âš ï¸ [Search] No search results returned")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ [Search Error] {e}")
            
            # Create enhanced query with codebase context
            if codebase_context:
                enhanced_query = f"{query}{codebase_context}\n\nPlease answer based on the provided codebase context when relevant."
                logger.info(f"ðŸš€ [Enhanced Query] Added codebase context ({len(codebase_context)} chars)")
            else:
                enhanced_query = query
                logger.info("ðŸ“ [Standard Query] No codebase context available, using original query")
            
            # Generate response using MemOS with enhanced query
            response = self.mos_instance.chat(query=enhanced_query, user_id=effective_user_id)
            
            end_time = datetime.now()
            inference_time = (end_time - start_time).total_seconds()
            
            logger.info(f"ðŸ§  [MemOS Chat] Generated response in {inference_time:.2f}s")
            
            return {
                'response': response,
                'query': query,
                'user_id': effective_user_id,
                'memory_enhanced': len(memories_used) > 0,  # True if codebase context was used
                'memories_used': memories_used,  # Populated from codebase search
                'inference_time_seconds': inference_time,
                'timestamp': end_time.isoformat(),
                'model_info': {
                    'type': 'memos-gguf',
                    'backend': 'llama-cpp-python-via-memos',
                    'model_path': getattr(self.llm, 'model_path', 'unknown') if self.llm else 'unknown'
                },
                'memos_enabled': True,
                'codebase_context_used': len(memories_used) > 0,
                'codebase_memories_count': len(memories_used),
            }
            
        except Exception as e:
            logger.error(f"MemOS chat failed: {e}")
            logger.error(f"Full error: {e.__class__.__name__}: {e}")
            # Fallback to enhanced_inference
            logger.warning("Falling back to enhanced_inference due to MemOS error")
            return await self.enhanced_inference(query, user_id, include_memory, **kwargs)
    
    async def enhanced_inference(
        self,
        query: str,
        user_id: Optional[str] = None,
        include_memory: bool = True,
        memory_top_k: Optional[int] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        Perform enhanced inference using direct llama-cpp-python model.
        
        Args:
            query (str): User query/prompt
            user_id (Optional[str]): User ID for memory retrieval
            include_memory (bool): Whether to include memory in the prompt
            memory_top_k (Optional[int]): Number of top memories to retrieve
            **kwargs: Additional generation parameters
            
        Returns:
            Dict[str, Any]: Response with generated text and metadata
        """
        if not self._is_initialized:
            raise RuntimeError("Service not initialized. Call startup() first.")
        
        if not self.llm:
            raise RuntimeError("GGUF model not loaded")
        
        start_time = datetime.now()
        
        try:
            # Get generation parameters
            generation_config = self.config.get('model', {}).get('generation', {})
            max_tokens = kwargs.get('max_tokens', generation_config.get('max_tokens', 256))
            temperature = kwargs.get('temperature', generation_config.get('temperature', 0.7))
            top_p = kwargs.get('top_p', generation_config.get('top_p', 0.9))
            
            # For now, implement basic chat without full MemOS integration
            # TODO: Enhanced memory integration can be added later
            memories_used = []
            
            if include_memory and MEMOS_AVAILABLE and self.mos_instance:
                # Basic memory search (if MemOS is available)
                try:
                    search_result = self.mos_instance.search(query=query, user_id=user_id)
                    if search_result and search_result.get('text_mem'):
                        # Extract memory content for context
                        memory_context = []
                        for cube_result in search_result['text_mem']:
                            memories = cube_result.get('memories', [])[:memory_top_k or 5]
                            for memory in memories:
                                memory_context.append(memory.memory)
                                memories_used.append({
                                    'id': memory.id,
                                    'content': memory.memory[:200] + '...' if len(memory.memory) > 200 else memory.memory,
                                    'relevance_score': getattr(memory.metadata, 'relativity', 0.0) if hasattr(memory, 'metadata') else 0.0,
                                })
                        
                        # Enhance query with memory context
                        if memory_context:
                            enhanced_query = f"Context from previous conversations:\n{chr(10).join(memory_context[:3])}\n\nUser question: {query}"
                        else:
                            enhanced_query = query
                except Exception as e:
                    logger.warning(f"Memory search failed: {e}")
                    enhanced_query = query
            else:
                enhanced_query = query
            
            # Use direct prompt instead of chat completion for better compatibility
            prompt = f"User: {enhanced_query}\nAssistant:"
            
            # Generate response using llama-cpp-python direct call
            response = self.llm(
                prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
                stop=["User:", "\n\n", "Assistant:", "<|im_end|>"],
                echo=False
            )
            
            # Extract generated text
            generated_text = response['choices'][0]['text'].strip()
            
            end_time = datetime.now()
            inference_time = (end_time - start_time).total_seconds()
            
            return {
                'response': generated_text,
                'query': query,
                'user_id': user_id,
                'memory_enhanced': include_memory and len(memories_used) > 0,
                'memories_used': memories_used,
                'inference_time_seconds': inference_time,
                'timestamp': end_time.isoformat(),
                'model_info': {
                    'type': 'gguf',
                    'backend': 'llama-cpp-python',
                    'model_path': self.llm.model_path if hasattr(self.llm, 'model_path') else 'unknown'
                },
            }
            
        except Exception as e:
            logger.error(f"Enhanced inference failed: {e}")
            raise
    
    async def load_codebase(
        self,
        directory_path: str,
        user_id: Optional[str] = None,
        project_id: Optional[str] = None,
        file_extensions: Optional[List[str]] = None,
        exclude_dirs: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        Load a directory of code files into MemOS for enhanced code retrieval.
        
        Args:
            directory_path (str): Path to the directory containing code files
            user_id (Optional[str]): User ID for memory context
            project_id (Optional[str]): Project ID for memory isolation (default: "default")
            file_extensions (Optional[List[str]]): File extensions to include
            exclude_dirs (Optional[List[str]]): Directory names to exclude
            
        Returns:
            Dict[str, Any]: Loading operation results
        """
        if not self._is_initialized:
            raise RuntimeError("Service not initialized. Call startup() first.")
        
        if not MEMOS_AVAILABLE or not self.mos_instance:
            raise RuntimeError("MemOS not available - cannot load codebase")
        
        start_time = datetime.now()
        
        # Default file extensions for code files
        if file_extensions is None:
            file_extensions = ['.py', '.js', '.ts', '.jsx', '.tsx', '.java', '.cpp', '.c', '.h', '.hpp', 
                             '.cs', '.php', '.rb', '.go', '.rs', '.swift', '.kt', '.scala', '.r', '.m', 
                             '.sql', '.sh', '.bash', '.yaml', '.yml', '.json', '.xml', '.html', '.css', 
                             '.md', '.txt', '.cfg', '.ini', '.conf']
        
        # Default excluded directories
        if exclude_dirs is None:
            exclude_dirs = ['node_modules', '.git', '__pycache__', '.pytest_cache', 'venv', 'env', 
                           '.venv', 'build', 'dist', '.next', '.nuxt', 'target', 'bin', 'obj', 
                           '.idea', '.vscode', 'coverage', '.coverage', '.nyc_output', 'logs']
        
        # Get effective user ID and project ID
        effective_user_id = user_id or self.config.get('memos', {}).get('user_id', 'default_user')
        effective_project_id = project_id or "default"
        
        # Use project memory manager for project-specific memory isolation
        try:
            # Create user if doesn't exist
            if not self.mos_instance.user_manager.validate_user(effective_user_id):
                self.mos_instance.create_user(user_id=effective_user_id)
                logger.info(f"âœ… Created user: {effective_user_id}")
            
            # Use project memory manager to get or create project-specific cube
            if self.project_memory_manager:
                cube_id = self.project_memory_manager.get_or_create_project_cube(
                    effective_user_id, effective_project_id
                )
                if not cube_id:
                    raise RuntimeError(f"Failed to create project cube for {effective_user_id}:{effective_project_id}")
                
                logger.info(f"âœ… Using project cube: {cube_id}")
            else:
                # Fallback to old behavior if project memory manager not available
                logger.warning("Project memory manager not available, using legacy cube creation")
                accessible_cubes = self.mos_instance.user_manager.get_user_cubes(effective_user_id)
                if not accessible_cubes:
                    # Create a default memory cube for the user (legacy)
                    cube_id = f"{effective_user_id}_codebase_cube"
                    
                    from memos.configs.mem_cube import GeneralMemCubeConfig
                    from memos.mem_cube.general import GeneralMemCube
                    
                    cube_config = GeneralMemCubeConfig(
                        user_id=effective_user_id,
                        cube_id=cube_id,
                        text_mem={
                            "backend": "general_text",
                            "config": {
                                "embedder": {
                                    "backend": "sentence_transformer",
                                    "config": {
                                        "model_name_or_path": "all-MiniLM-L6-v2",
                                        "trust_remote_code": True
                                    }
                                },
                                "vector_db": {
                                    "backend": "qdrant",
                                    "config": {
                                        "collection_name": f"codebase_{effective_user_id}_code",
                                        "vector_dimension": 384,
                                        "distance_metric": "cosine",
                                        "host": None,
                                        "port": None,
                                        "path": f"./qdrant_storage/{effective_user_id}_{cube_id}"
                                    }
                                },
                                "extractor_llm": {
                                    "backend": "openai",
                                    "config": {
                                        "model_name_or_path": "gpt-3.5-turbo",
                                        "temperature": 0.0,
                                        "max_tokens": 8192,
                                        "api_key": "fake-api-key",
                                        "api_base": "http://localhost:11434/v1"
                                    }
                                }
                            }
                        }
                    )
                    
                    mem_cube = GeneralMemCube(cube_config)
                    cube_path = f"./memory_cubes/{effective_user_id}/{cube_id}"
                    mem_cube.dump(cube_path)
                    
                    self.mos_instance.register_mem_cube(
                        mem_cube_name_or_path=cube_path,
                        mem_cube_id=cube_id,
                        user_id=effective_user_id
                    )
                    
                    logger.info(f"âœ… Created and registered legacy memory cube: {cube_id}")
                else:
                    cube_id = list(accessible_cubes)[0]  # Use first available cube
                
        except Exception as e:
            logger.warning(f"âš ï¸ Error setting up user/cube: {e}")
        
        # Validate directory path
        directory_path = os.path.abspath(directory_path)
        if not os.path.exists(directory_path):
            raise ValueError(f"Directory does not exist: {directory_path}")
        
        if not os.path.isdir(directory_path):
            raise ValueError(f"Path is not a directory: {directory_path}")
        
        logger.info(f"ðŸš€ [Load Codebase] Starting to load codebase from: {directory_path}")
        logger.info(f"ðŸ“‚ [Load Codebase] User ID: {effective_user_id}")
        logger.info(f"ðŸ—ï¸ [Load Codebase] Project ID: {effective_project_id}")
        logger.info(f"ðŸ” [Load Codebase] Extensions: {file_extensions}")
        logger.info(f"ðŸš« [Load Codebase] Excluded dirs: {exclude_dirs}")
        
        loaded_files = []
        skipped_files = []
        total_size_bytes = 0
        
        try:
            # Walk through the directory structure
            for root, dirs, files in os.walk(directory_path):
                # Skip excluded directories
                dirs[:] = [d for d in dirs if d not in exclude_dirs]
                
                for file in files:
                    file_path = os.path.join(root, file)
                    relative_path = os.path.relpath(file_path, directory_path)
                    
                    # Check file extension
                    file_ext = os.path.splitext(file)[1].lower()
                    if file_ext not in file_extensions:
                        skipped_files.append({
                            'path': relative_path,
                            'reason': f'Extension {file_ext} not in allowed list'
                        })
                        continue
                    
                    try:
                        # Read file content
                        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                            content = f.read()
                        
                        # Skip empty files
                        if not content.strip():
                            skipped_files.append({
                                'path': relative_path,
                                'reason': 'Empty file'
                            })
                            continue
                        
                        # Get file size
                        file_size = len(content.encode('utf-8'))
                        total_size_bytes += file_size
                        
                        # Format content for memory storage
                        memory_content = f"File: {relative_path}\nPath: {file_path}\nExtension: {file_ext}\nContent:\n{content}"
                        
                        # Create memory item
                        memory_data = {
                            'memory': memory_content,
                            'metadata': {
                                'type': 'codebase',
                                'file_path': relative_path,
                                'file_extension': file_ext,
                                'file_size_bytes': file_size,
                                'source': 'codebase_loader',
                                'memory_time': datetime.now().isoformat(),
                                'updated_at': datetime.now().isoformat(),
                                'tags': ['code', 'file', file_ext.lstrip('.')],
                                'entities': [os.path.basename(file_path)],
                                'visibility': 'private'
                            }
                        }
                        
                        # Add memory to MemOS (this will create a cube if needed)
                        self.mos_instance.add(
                            memory_content=memory_content,
                            user_id=effective_user_id
                        )
                        
                        loaded_files.append({
                            'path': relative_path,
                            'size_bytes': file_size,
                            'extension': file_ext
                        })
                        
                        logger.debug(f"âœ… [Load Codebase] Loaded: {relative_path} ({file_size} bytes)")
                        
                    except Exception as e:
                        logger.warning(f"âš ï¸ [Load Codebase] Failed to load {relative_path}: {e}")
                        skipped_files.append({
                            'path': relative_path,
                            'reason': f'Read error: {str(e)}'
                        })
                        continue
            
            end_time = datetime.now()
            loading_time = (end_time - start_time).total_seconds()
            
            logger.info(f"âœ… [Load Codebase] Completed successfully!")
            logger.info(f"ðŸ“Š [Load Codebase] Files loaded: {len(loaded_files)}")
            logger.info(f"ðŸ“Š [Load Codebase] Files skipped: {len(skipped_files)}")
            logger.info(f"ðŸ“Š [Load Codebase] Total size: {total_size_bytes:,} bytes")
            logger.info(f"â±ï¸ [Load Codebase] Loading time: {loading_time:.2f}s")
            
            return {
                'status': 'success',
                'directory_path': directory_path,
                'user_id': effective_user_id,
                'files_loaded': len(loaded_files),
                'files_skipped': len(skipped_files),
                'total_size_bytes': total_size_bytes,
                'loading_time_seconds': loading_time,
                'timestamp': end_time.isoformat(),
                'loaded_files': loaded_files[:100],  # Limit to first 100 for response size
                'skipped_files': skipped_files[:50],  # Limit to first 50 for response size
                'file_extensions_processed': file_extensions,
                'excluded_directories': exclude_dirs
            }
            
        except Exception as e:
            logger.error(f"âŒ [Load Codebase] Failed: {e}")
            raise RuntimeError(f"Failed to load codebase: {str(e)}")
    
    def get_service_status(self) -> Dict[str, Any]:
        """
        Get comprehensive service status information.
        
        Returns:
            Dict[str, Any]: Service status and configuration
        """
        try:
            model_info = None
            model_healthy = False
            
            if self.llm and hasattr(self.llm, 'model_path'):
                model_info = {
                    'type': 'gguf',
                    'backend': 'llama-cpp-python',
                    'model_path': self.llm.model_path
                }
            
            if self.llm and hasattr(self.llm, 'is_healthy'):
                model_healthy = self.llm.is_healthy()
            elif self.llm:
                # If no is_healthy method, consider it healthy if it exists
                model_healthy = True
            
            return {
                'service': {
                    'name': 'GGUF Memory Service',
                    'initialized': self._is_initialized,
                    'startup_time': getattr(self, '_startup_time', None),
                },
                'memos': {
                    'status': 'running' if self.mos_instance else 'not_initialized',
                    'user_id': self.mos_instance.user_id if self.mos_instance else None,
                    'cubes_count': len(self.mos_instance.mem_cubes) if self.mos_instance else 0,
                    'textual_memory_enabled': self.mos_instance.config.enable_textual_memory if self.mos_instance else False,
                },
                'model': {
                    'type': 'gguf',
                    'loaded': model_healthy,
                    'healthy': model_healthy,
                    'info': model_info,
                },
                'config': {
                    'memory_retrieval_enabled': self.config.get('memory', {}).get('retrieval', {}).get('enabled', True),
                    'memory_top_k': self.config.get('memory', {}).get('retrieval', {}).get('top_k', 5),
                    'model_path': self.config.get('model', {}).get('model_path'),
                },
            }
            
        except Exception as e:
            logger.error(f"Error getting service status: {e}")
            return {
                'service': {'name': 'GGUF Memory Service', 'initialized': False, 'error': str(e)},
                'memos': {'status': 'error'},
                'model': {'type': 'gguf', 'loaded': False, 'healthy': False},
            }
    
    def is_healthy(self) -> bool:
        """
        Check if the service is healthy and ready to handle requests.
        
        Returns:
            bool: True if service is healthy
        """
        try:
            if not self._is_initialized:
                return False
            
            if not self.llm:
                return False
            
            if hasattr(self.llm, 'is_healthy'):
                return self.llm.is_healthy()
            
            return True
            
        except Exception as e:
            logger.error(f"Health check failed: {e}")
            return False
    
    async def shutdown(self) -> None:
        """
        Gracefully shutdown the service.
        """
        try:
            logger.info("Shutting down MemOS Service...")
            
            self.llm = None
            self.mos_instance = None
            self._is_initialized = False
            
            logger.info("MemOS Service shutdown completed")
            
        except Exception as e:
            logger.error(f"Error during shutdown: {e}")


# Global service instance
_service_instance: Optional[GGUFMemoryService] = None


async def get_service_instance(config_path: str = "config.yaml") -> GGUFMemoryService:
    """
    Get or create the global service instance.
    
    Args:
        config_path (str): Path to configuration file
        
    Returns:
        GGUFMemoryService: The service instance
    """
    global _service_instance
    
    if _service_instance is None:
        _service_instance = GGUFMemoryService(config_path)
        if not await _service_instance.startup():
            raise RuntimeError("Failed to start GGUF Memory Service")
    
    return _service_instance


async def shutdown_service() -> None:
    """
    Shutdown the global service instance.
    """
    global _service_instance
    
    if _service_instance:
        await _service_instance.shutdown()
        _service_instance = None 