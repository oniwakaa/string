# Generated by SmolLM3-3B on 2025-01-12
"""
GGUF Memory Service - Direct llama-cpp-python Integration

This module provides a persistent service that uses llama-cpp-python directly
for GGUF model inference, with optional MemOS memory enhancement.
"""

import asyncio
import logging
import os
import sys
from datetime import datetime
from typing import Dict, Any, Optional, List

# Add MemOS to Python path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'MemOS', 'src'))

from config_loader import ConfigLoader, load_config

# Import llama-cpp-python for direct GGUF model loading
try:
    from llama_cpp import Llama
    LLAMA_CPP_AVAILABLE = True
except ImportError as e:
    print(f"âŒ llama-cpp-python not available: {e}")
    print("ðŸ’¡ Please install with: pip install llama-cpp-python")
    LLAMA_CPP_AVAILABLE = False

# Optional MemOS imports for memory functionality
try:
    from memos.configs.mem_os import MOSConfig
    from memos.mem_os.main import MOS
    from memos.log import get_logger
    from memos.mem_user.user_manager import UserManager, UserRole
    logger = get_logger(__name__)
    MEMOS_AVAILABLE = True
except ImportError as e:
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)
    logger.warning(f"MemOS not available (memory features disabled): {e}")
    MEMOS_AVAILABLE = False

# Import our custom LLM wrapper
try:
    from llama_cpp_wrapper import LlamaCppWrapper
    WRAPPER_AVAILABLE = True
except ImportError as e:
    logger.warning(f"LlamaCpp wrapper not available: {e}")
    WRAPPER_AVAILABLE = False


class GGUFMemoryService:
    """
    GGUF service that uses llama-cpp-python directly for model inference,
    with optional MemOS memory enhancement when available.
    """
    
    def __init__(self, config_path: str = "config.yaml"):
        """
        Initialize the GGUF Memory Service.
        
        Args:
            config_path (str): Path to the configuration file
        """
        self.config_loader = ConfigLoader(config_path)
        self.config = self.config_loader.load()
        self.mos_instance = None  # MemOS instance for memory management
        self.llama_wrapper = None  # Our custom LLM wrapper
        self.llm = None  # Direct llama-cpp-python Llama instance
        self._is_initialized = False
        
        # Check dependencies
        if not LLAMA_CPP_AVAILABLE:
            raise RuntimeError("llama-cpp-python is required but not available")
        
        logger.info("GGUF Memory Service initialized")
    
    async def startup(self) -> bool:
        """
        Start up the service by loading the GGUF model directly and optionally initializing MemOS.
        
        Returns:
            bool: True if startup successful, False otherwise
        """
        try:
            logger.info("Starting GGUF Memory Service...")
            
            # Initialize GGUF model directly
            if not await self._initialize_gguf_model():
                return False
            
            # Initialize MemOS if available (for memory features)
            if MEMOS_AVAILABLE:
                await self._initialize_memos()
            else:
                logger.warning("MemOS not available - memory features disabled")
            
            self._is_initialized = True
            self._startup_time = datetime.now().isoformat()
            logger.info("GGUF Memory Service startup completed successfully")
            return True
            
        except Exception as e:
            logger.error(f"Failed to start GGUF Memory Service: {e}")
            return False
    
    async def _initialize_gguf_model(self) -> bool:
        """
        Initialize the GGUF model using llama-cpp-python directly.
        
        Returns:
            bool: True if initialization successful
        """
        try:
            model_config = self.config.get('model', {})
            
            # Get model path - try multiple possible locations
            configured_path = model_config.get('model_path', './smollm-quantized/smollm-q4_K_M.gguf')
            
            possible_model_paths = [
                './smollm-quantized/smollm-q4_K_M.gguf',
                './smollm-quantized/ggml-model-f16.gguf',
                './smollm/smollm-3b-instruct-q4_k_m.gguf',
                configured_path  # Try configured path last
            ]
            
            # Remove duplicates while preserving order
            seen = set()
            possible_model_paths = [x for x in possible_model_paths if not (x in seen or seen.add(x))]
            
            model_path = None
            for path in possible_model_paths:
                if os.path.exists(path):
                    model_path = path
                    break
            
            if not model_path:
                logger.error(f"No GGUF model found. Searched paths: {possible_model_paths}")
                return False
            
            logger.info(f"Loading GGUF model from: {model_path}")
            
            # Get generation config
            generation_config = model_config.get('generation', {})
            
            # Initialize Llama model with optimal settings for Apple Silicon
            self.llm = Llama(
                model_path=model_path,
                n_gpu_layers=-1,  # Offload all layers to GPU (Metal on Apple Silicon)
                n_ctx=generation_config.get('n_ctx', 16384),  # Increased from 4096 to better utilize model's training context
                n_batch=generation_config.get('n_batch', 512),
                verbose=False,
                # Additional performance optimizations
                use_mmap=True,
                use_mlock=False,
                n_threads=None,  # Auto-detect optimal thread count
            )
            
            logger.info("âœ… GGUF model loaded successfully with Metal acceleration")
            
            # Test basic functionality
            test_response = self.llm(
                "Hello",
                max_tokens=5,
                temperature=0.1,
                echo=False
            )
            
            if test_response and test_response.get('choices'):
                logger.info("âœ… GGUF model health check passed")
                return True
            else:
                logger.error("âŒ GGUF model health check failed")
                return False
            
        except Exception as e:
            logger.error(f"Failed to initialize GGUF model: {e}")
            return False
    
    async def _initialize_memos(self) -> bool:
        """
        Initialize the MemOS instance with our custom LLM wrapper.
        
        Returns:
            bool: True if initialization successful
        """
        try:
            if not MEMOS_AVAILABLE or not WRAPPER_AVAILABLE:
                logger.warning("MemOS or LlamaCpp wrapper not available - skipping memory initialization")
                return True
            
            if not self.llm:
                logger.error("GGUF model must be loaded before initializing MemOS")
                return False
            
            logger.info("Initializing MemOS with LlamaCpp wrapper...")
            
            # Create our custom LLM wrapper around the loaded model
            self.llama_wrapper = LlamaCppWrapper(self.llm)
            logger.info("âœ… LlamaCpp wrapper created successfully")
            
            # Patch the GGUF LLM class to use our pre-loaded model
            self._patch_gguf_llm_for_memos()
            
            # Get MemOS configuration from config file
            memos_config = self.config.get('memos', {})
            user_id = memos_config.get('user_id', 'default_user')
            
            # Create user manager and ensure default user exists
            user_manager = UserManager()
            if not user_manager.validate_user(user_id):
                user_manager.create_user(
                    user_name=user_id,
                    role=UserRole.USER,
                    user_id=user_id
                )
                logger.info(f"Created default user: {user_id}")
            
            # Create MemOS configuration with standard GGUF backend
            mos_config_dict = {
                'user_id': user_id,
                'session_id': memos_config.get('session_id', 'default_session'),
                'enable_textual_memory': memos_config.get('enable_textual_memory', True),
                'enable_activation_memory': memos_config.get('enable_activation_memory', False),
                'top_k': memos_config.get('top_k', 5),
                'chat_model': {
                    'backend': 'gguf',
                    'config': {
                        'model_name_or_path': './smollm-quantized/smollm-q4_K_M.gguf',
                        'temperature': 0.7,
                        'max_tokens': 512,
                        'top_p': 0.9,
                        'top_k': 50,
                    }
                },
                'mem_reader': {
                    'backend': 'simple_struct',
                    'config': {
                        'llm': {
                            'backend': 'gguf',
                            'config': {
                                'model_name_or_path': './smollm-quantized/smollm-q4_K_M.gguf',
                                'temperature': 0.7,
                                'max_tokens': 512,
                                'top_p': 0.9,
                                'top_k': 50,
                            }
                        },
                        'embedder': {
                            'backend': 'sentence_transformer',
                            'config': {
                                'model_name_or_path': 'all-MiniLM-L6-v2'
                            }
                        },
                        'chunker': {
                            'backend': 'sentence',
                            'config': {
                                'tokenizer_or_token_counter': 'gpt2',
                                'chunk_size': 512,
                                'chunk_overlap': 128,
                                'min_sentences_per_chunk': 1
                            }
                        }
                    }
                }
            }
            
            # Create MemOS configuration object
            mos_config = MOSConfig(**mos_config_dict)
            
            # Initialize MemOS with our configuration  
            self.mos_instance = MOS(mos_config)
            
            # Replace the chat_llm with our wrapper after initialization
            self.mos_instance.chat_llm = self.llama_wrapper
            
            logger.info("âœ… MemOS initialized successfully with LlamaCpp wrapper")
            
            # Test basic functionality
            try:
                test_response = self.mos_instance.chat("Hello", user_id=user_id)
                logger.info("âœ… MemOS health check passed")
                return True
            except Exception as e:
                logger.error(f"âŒ MemOS health check failed: {e}")
                return False
            
        except Exception as e:
            logger.error(f"Failed to initialize MemOS: {e}")
            logger.error(f"Full error: {e.__class__.__name__}: {e}")
            import traceback
            traceback.print_exc()
            return True  # Non-critical failure - continue without memory features
    
    def _patch_gguf_llm_for_memos(self):
        """Patch the MemOS GGUF LLM class to use our pre-loaded model."""
        try:
            from memos.llms.gguf import GGUFLLLM
            
            # Store the original __init__ method
            original_init = GGUFLLLM.__init__
            
            # Create a patched __init__ that returns our wrapper
            def patched_init(llm_self, config):
                # Instead of loading a new model, use our wrapper
                llm_self.config = config
                llm_self.llama_wrapper = self.llama_wrapper
                logger.info("Using patched GGUF LLM with pre-loaded model")
            
            # Store original generate method
            original_generate = GGUFLLLM.generate
            
            # Create patched generate method that delegates to our wrapper
            def patched_generate(llm_self, messages, **kwargs):
                return self.llama_wrapper.generate(messages, **kwargs)
            
            # Apply the patches
            GGUFLLLM.__init__ = patched_init
            GGUFLLLM.generate = patched_generate
            
            logger.info("âœ… Successfully patched MemOS GGUF LLM class")
            
        except Exception as e:
            logger.error(f"Failed to patch MemOS GGUF LLM class: {e}")
            raise
    
    async def memos_chat(
        self,
        query: str,
        user_id: Optional[str] = None,
        include_memory: bool = True,
        **kwargs
    ) -> Dict[str, Any]:
        """
        Perform memory-aware chat using MemOS.
        
        Args:
            query (str): User query/prompt
            user_id (Optional[str]): User ID for memory retrieval and context
            include_memory (bool): Whether to use memory (always True for MemOS)
            **kwargs: Additional parameters (for compatibility)
            
        Returns:
            Dict[str, Any]: Response with generated text and metadata
        """
        if not self._is_initialized:
            raise RuntimeError("Service not initialized. Call startup() first.")
        
        if not self.mos_instance:
            # Fallback to enhanced_inference if MemOS is not available
            logger.warning("MemOS not available, falling back to enhanced_inference")
            return await self.enhanced_inference(query, user_id, include_memory, **kwargs)
        
        start_time = datetime.now()
        
        try:
            # Use MemOS for memory-aware chat
            effective_user_id = user_id or self.config.get('memos', {}).get('user_id', 'default_user')
            
            logger.info(f"ðŸ§  [MemOS Chat] Processing query for user {effective_user_id}: {query[:100]}...")
            
            # First search for codebase context to enhance the query
            codebase_context = ""
            memories_used = []
            
            try:
                # Search for relevant memories (will search all accessible cubes)
                search_result = self.mos_instance.search(query=query, user_id=effective_user_id)
                
                if search_result and search_result.get('text_mem'):
                    logger.info(f"ðŸ” [Search Results] Found {len(search_result['text_mem'])} cube results")
                    
                    # Filter for codebase-related memories
                    codebase_memories = []
                    for cube_result in search_result['text_mem']:
                        cube_memories = cube_result.get('memories', [])
                        logger.info(f"ðŸ“š [Cube Analysis] Cube has {len(cube_memories)} memories")
                        
                        for memory in cube_memories[:5]:  # Top 5 most relevant
                            # Check if this memory contains code-like content
                            memory_content = memory.memory
                            if any(keyword in memory_content.lower() for keyword in [
                                'def ', 'function', 'class ', 'import ', 'return ', 
                                '.py', 'file:', 'path:', 'calculate_fibonacci', 
                                'textanalyzer', 'math_utils', 'string_processor',
                                'quantum_flux_capacitor', 'dilithium_crystals', 'quantumcomputer',
                                'unique_functions', 'temporal_coefficient', 'energy_level'
                            ]):
                                codebase_memories.append(memory_content)
                                memories_used.append({
                                    'id': memory.id,
                                    'content': memory_content[:200] + '...' if len(memory_content) > 200 else memory_content,
                                    'relevance_score': getattr(memory, 'score', 0.0),
                                    'source': 'codebase'
                                })
                    
                    if codebase_memories:
                        codebase_context = f"\n\nRelevant codebase context:\n{chr(10).join(codebase_memories[:3])}\n"
                        logger.info(f"âœ… [Codebase Context] Found {len(codebase_memories)} relevant code memories")
                    else:
                        logger.info("âš ï¸ [Codebase Context] No code-related memories found")
                else:
                    logger.info("âš ï¸ [Search] No search results returned")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ [Search Error] {e}")
            
            # Create enhanced query with codebase context
            if codebase_context:
                enhanced_query = f"{query}{codebase_context}\n\nPlease answer based on the provided codebase context when relevant."
                logger.info(f"ðŸš€ [Enhanced Query] Added codebase context ({len(codebase_context)} chars)")
            else:
                enhanced_query = query
                logger.info("ðŸ“ [Standard Query] No codebase context available, using original query")
            
            # Generate response using MemOS with enhanced query
            response = self.mos_instance.chat(query=enhanced_query, user_id=effective_user_id)
            
            end_time = datetime.now()
            inference_time = (end_time - start_time).total_seconds()
            
            logger.info(f"ðŸ§  [MemOS Chat] Generated response in {inference_time:.2f}s")
            
            return {
                'response': response,
                'query': query,
                'user_id': effective_user_id,
                'memory_enhanced': len(memories_used) > 0,  # True if codebase context was used
                'memories_used': memories_used,  # Populated from codebase search
                'inference_time_seconds': inference_time,
                'timestamp': end_time.isoformat(),
                'model_info': {
                    'type': 'memos-gguf',
                    'backend': 'llama-cpp-python-via-memos',
                    'model_path': getattr(self.llm, 'model_path', 'unknown') if self.llm else 'unknown'
                },
                'memos_enabled': True,
                'codebase_context_used': len(memories_used) > 0,
                'codebase_memories_count': len(memories_used),
            }
            
        except Exception as e:
            logger.error(f"MemOS chat failed: {e}")
            logger.error(f"Full error: {e.__class__.__name__}: {e}")
            # Fallback to enhanced_inference
            logger.warning("Falling back to enhanced_inference due to MemOS error")
            return await self.enhanced_inference(query, user_id, include_memory, **kwargs)
    
    async def enhanced_inference(
        self,
        query: str,
        user_id: Optional[str] = None,
        include_memory: bool = True,
        memory_top_k: Optional[int] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        Perform enhanced inference using direct llama-cpp-python model.
        
        Args:
            query (str): User query/prompt
            user_id (Optional[str]): User ID for memory retrieval
            include_memory (bool): Whether to include memory in the prompt
            memory_top_k (Optional[int]): Number of top memories to retrieve
            **kwargs: Additional generation parameters
            
        Returns:
            Dict[str, Any]: Response with generated text and metadata
        """
        if not self._is_initialized:
            raise RuntimeError("Service not initialized. Call startup() first.")
        
        if not self.llm:
            raise RuntimeError("GGUF model not loaded")
        
        start_time = datetime.now()
        
        try:
            # Get generation parameters
            generation_config = self.config.get('model', {}).get('generation', {})
            max_tokens = kwargs.get('max_tokens', generation_config.get('max_tokens', 256))
            temperature = kwargs.get('temperature', generation_config.get('temperature', 0.7))
            top_p = kwargs.get('top_p', generation_config.get('top_p', 0.9))
            
            # For now, implement basic chat without full MemOS integration
            # TODO: Enhanced memory integration can be added later
            memories_used = []
            
            if include_memory and MEMOS_AVAILABLE and self.mos_instance:
                # Basic memory search (if MemOS is available)
                try:
                    search_result = self.mos_instance.search(query=query, user_id=user_id)
                    if search_result and search_result.get('text_mem'):
                        # Extract memory content for context
                        memory_context = []
                        for cube_result in search_result['text_mem']:
                            memories = cube_result.get('memories', [])[:memory_top_k or 5]
                            for memory in memories:
                                memory_context.append(memory.memory)
                                memories_used.append({
                                    'id': memory.id,
                                    'content': memory.memory[:200] + '...' if len(memory.memory) > 200 else memory.memory,
                                    'relevance_score': getattr(memory.metadata, 'relativity', 0.0) if hasattr(memory, 'metadata') else 0.0,
                                })
                        
                        # Enhance query with memory context
                        if memory_context:
                            enhanced_query = f"Context from previous conversations:\n{chr(10).join(memory_context[:3])}\n\nUser question: {query}"
                        else:
                            enhanced_query = query
                except Exception as e:
                    logger.warning(f"Memory search failed: {e}")
                    enhanced_query = query
            else:
                enhanced_query = query
            
            # Use direct prompt instead of chat completion for better compatibility
            prompt = f"User: {enhanced_query}\nAssistant:"
            
            # Generate response using llama-cpp-python direct call
            response = self.llm(
                prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
                stop=["User:", "\n\n", "Assistant:", "<|im_end|>"],
                echo=False
            )
            
            # Extract generated text
            generated_text = response['choices'][0]['text'].strip()
            
            end_time = datetime.now()
            inference_time = (end_time - start_time).total_seconds()
            
            return {
                'response': generated_text,
                'query': query,
                'user_id': user_id,
                'memory_enhanced': include_memory and len(memories_used) > 0,
                'memories_used': memories_used,
                'inference_time_seconds': inference_time,
                'timestamp': end_time.isoformat(),
                'model_info': {
                    'type': 'gguf',
                    'backend': 'llama-cpp-python',
                    'model_path': self.llm.model_path if hasattr(self.llm, 'model_path') else 'unknown'
                },
            }
            
        except Exception as e:
            logger.error(f"Enhanced inference failed: {e}")
            raise
    
    def get_service_status(self) -> Dict[str, Any]:
        """
        Get comprehensive service status information.
        
        Returns:
            Dict[str, Any]: Service status and configuration
        """
        try:
            model_info = None
            model_healthy = False
            
            if self.llm and hasattr(self.llm, 'model_path'):
                model_info = {
                    'type': 'gguf',
                    'backend': 'llama-cpp-python',
                    'model_path': self.llm.model_path
                }
            
            if self.llm and hasattr(self.llm, 'is_healthy'):
                model_healthy = self.llm.is_healthy()
            elif self.llm:
                # If no is_healthy method, consider it healthy if it exists
                model_healthy = True
            
            return {
                'service': {
                    'name': 'GGUF Memory Service',
                    'initialized': self._is_initialized,
                    'startup_time': getattr(self, '_startup_time', None),
                },
                'memos': {
                    'status': 'running' if self.mos_instance else 'not_initialized',
                    'user_id': self.mos_instance.user_id if self.mos_instance else None,
                    'cubes_count': len(self.mos_instance.mem_cubes) if self.mos_instance else 0,
                    'textual_memory_enabled': self.mos_instance.config.enable_textual_memory if self.mos_instance else False,
                },
                'model': {
                    'type': 'gguf',
                    'loaded': model_healthy,
                    'healthy': model_healthy,
                    'info': model_info,
                },
                'config': {
                    'memory_retrieval_enabled': self.config.get('memory', {}).get('retrieval', {}).get('enabled', True),
                    'memory_top_k': self.config.get('memory', {}).get('retrieval', {}).get('top_k', 5),
                    'model_path': self.config.get('model', {}).get('model_path'),
                },
            }
            
        except Exception as e:
            logger.error(f"Error getting service status: {e}")
            return {
                'service': {'name': 'GGUF Memory Service', 'initialized': False, 'error': str(e)},
                'memos': {'status': 'error'},
                'model': {'type': 'gguf', 'loaded': False, 'healthy': False},
            }
    
    def is_healthy(self) -> bool:
        """
        Check if the service is healthy and ready to handle requests.
        
        Returns:
            bool: True if service is healthy
        """
        try:
            if not self._is_initialized:
                return False
            
            if not self.llm:
                return False
            
            if hasattr(self.llm, 'is_healthy'):
                return self.llm.is_healthy()
            
            return True
            
        except Exception as e:
            logger.error(f"Health check failed: {e}")
            return False
    
    async def shutdown(self) -> None:
        """
        Gracefully shutdown the service.
        """
        try:
            logger.info("Shutting down MemOS Service...")
            
            self.llm = None
            self.mos_instance = None
            self._is_initialized = False
            
            logger.info("MemOS Service shutdown completed")
            
        except Exception as e:
            logger.error(f"Error during shutdown: {e}")


# Global service instance
_service_instance: Optional[GGUFMemoryService] = None


async def get_service_instance(config_path: str = "config.yaml") -> GGUFMemoryService:
    """
    Get or create the global service instance.
    
    Args:
        config_path (str): Path to configuration file
        
    Returns:
        GGUFMemoryService: The service instance
    """
    global _service_instance
    
    if _service_instance is None:
        _service_instance = GGUFMemoryService(config_path)
        if not await _service_instance.startup():
            raise RuntimeError("Failed to start GGUF Memory Service")
    
    return _service_instance


async def shutdown_service() -> None:
    """
    Shutdown the global service instance.
    """
    global _service_instance
    
    if _service_instance:
        await _service_instance.shutdown()
        _service_instance = None 