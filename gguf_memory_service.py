# Generated by SmolLM3-3B on 2025-01-12
"""
GGUF Memory Service - Direct llama-cpp-python Integration

This module provides a persistent service that uses llama-cpp-python directly
for GGUF model inference, with optional MemOS memory enhancement.
"""

import asyncio
import logging
import os
import sys
from datetime import datetime
from typing import Dict, Any, Optional, List

# Add MemOS to Python path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'MemOS', 'src'))

from config_loader import ConfigLoader, load_config

# Import ModelManager for centralized model management
try:
    sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))
    from models.manager import ModelManager, model_manager
    MODELMANAGER_AVAILABLE = True
except ImportError as e:
    print(f"‚ùå ModelManager not available: {e}")
    print("üí° ModelManager is required for model management")
    MODELMANAGER_AVAILABLE = False

# Import llama-cpp-python for fallback compatibility
try:
    from llama_cpp import Llama
    LLAMA_CPP_AVAILABLE = True
except ImportError as e:
    print(f"‚ùå llama-cpp-python not available: {e}")
    print("üí° Please install with: pip install llama-cpp-python")
    LLAMA_CPP_AVAILABLE = False

# Optional MemOS imports for memory functionality
try:
    from memos.configs.mem_os import MOSConfig
    from memos.mem_os.main import MOS
    from memos.log import get_logger
    from memos.mem_user.user_manager import UserManager, UserRole
    logger = get_logger(__name__)
    MEMOS_AVAILABLE = True
except ImportError as e:
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)
    logger.warning(f"MemOS not available (memory features disabled): {e}")
    MEMOS_AVAILABLE = False

# Import our custom LLM wrapper
try:
    from llama_cpp_wrapper import LlamaCppWrapper
    WRAPPER_AVAILABLE = True
except ImportError as e:
    logger.warning(f"LlamaCpp wrapper not available: {e}")
    WRAPPER_AVAILABLE = False

# Import project memory manager
try:
    from project_memory_manager import ProjectMemoryManager
    PROJECT_MEMORY_AVAILABLE = True
except ImportError as e:
    logger.warning(f"Project memory manager not available: {e}")
    PROJECT_MEMORY_AVAILABLE = False

# Import ResourceManager for shared resource management
try:
    from src.core.resource_manager import get_resource_manager
    RESOURCE_MANAGER_AVAILABLE = True
except ImportError as e:
    logger.warning(f"ResourceManager not available: {e}")
    RESOURCE_MANAGER_AVAILABLE = False


class GGUFMemoryService:
    """
    GGUF service that uses llama-cpp-python directly for model inference,
    with optional MemOS memory enhancement when available.
    """
    
    def __init__(self, config_path: str = "config.yaml"):
        """
        Initialize the GGUF Memory Service.
        
        Args:
            config_path (str): Path to the configuration file
        """
        self.config_loader = ConfigLoader(config_path)
        self.config = self.config_loader.load()
        self.mos_instance = None  # MemOS instance for memory management
        self.llama_wrapper = None  # Our custom LLM wrapper
        self.llm = None  # Direct llama-cpp-python Llama instance
        self._is_initialized = False
        
        # Initialize project memory manager
        self.project_memory_manager = None
        if PROJECT_MEMORY_AVAILABLE:
            self.project_memory_manager = ProjectMemoryManager()
        else:
            logger.warning("Project memory manager not available")
        
        # Initialize ResourceManager for shared resource management
        self.resource_manager = None
        if RESOURCE_MANAGER_AVAILABLE:
            self.resource_manager = get_resource_manager()
            
            # Pre-initialize Qdrant client to claim the storage path first
            logger.info("üîß Pre-initializing ResourceManager Qdrant client to prevent conflicts")
            try:
                self.resource_manager.get_qdrant_client()
                logger.info("‚úÖ ResourceManager Qdrant client pre-initialized successfully")
            except Exception as e:
                logger.error(f"‚ùå ResourceManager Qdrant pre-initialization failed: {e}")
            
            logger.info("‚úÖ ResourceManager initialized for shared resource management")
        else:
            logger.warning("ResourceManager not available - MemOS will use internal factories")
        
        # Check dependencies
        if not MODELMANAGER_AVAILABLE:
            raise RuntimeError("ModelManager is required but not available")
        
        if not LLAMA_CPP_AVAILABLE:
            logger.warning("llama-cpp-python not available - using ModelManager only")
        
        logger.info("GGUF Memory Service initialized")
    
    async def startup(self) -> bool:
        """
        Start up the service by loading the GGUF model directly and optionally initializing MemOS.
        
        Returns:
            bool: True if startup successful, False otherwise
        """
        try:
            logger.info("Starting GGUF Memory Service...")
            
            # Initialize GGUF model directly
            if not await self._initialize_gguf_model():
                return False
            
            # Initialize MemOS if available (for memory features)
            if MEMOS_AVAILABLE:
                await self._initialize_memos()
            else:
                logger.warning("MemOS not available - memory features disabled")
            
            self._is_initialized = True
            self._startup_time = datetime.now().isoformat()
            logger.info("GGUF Memory Service startup completed successfully")
            return True
            
        except Exception as e:
            logger.error(f"Failed to start GGUF Memory Service: {e}")
            return False
    
    async def _initialize_gguf_model(self) -> bool:
        """
        Initialize the GGUF model using ModelManager for centralized model management.
        
        Returns:
            bool: True if initialization successful
        """
        try:
            # Use ModelManager to get the default model (SmolLM3-3B)
            model_name = "SmolLM3-3B"  # Default model as configured in models.yaml
            
            logger.info(f"Loading GGUF model '{model_name}' via ModelManager")
            
            # Get model from ModelManager - this handles all loading, caching, and memory management
            self.llm = model_manager.get_model(model_name)
            
            if not self.llm:
                logger.error(f"Failed to load model '{model_name}' via ModelManager")
                return False
            
            logger.info(f"‚úÖ GGUF model '{model_name}' loaded successfully via ModelManager with LRU eviction")
            
            # Test basic functionality
            test_response = self.llm(
                "Hello",
                max_tokens=5,
                temperature=0.1,
                echo=False
            )
            
            if test_response and test_response.get('choices'):
                logger.info("‚úÖ GGUF model health check passed")
                return True
            else:
                logger.error("‚ùå GGUF model health check failed")
                return False
            
        except Exception as e:
            logger.error(f"Failed to initialize GGUF model via ModelManager: {e}")
            return False
    
    async def _initialize_memos(self) -> bool:
        """
        Initialize the MemOS instance with our custom LLM wrapper.
        
        Returns:
            bool: True if initialization successful
        """
        try:
            if not MEMOS_AVAILABLE or not WRAPPER_AVAILABLE:
                logger.warning("MemOS or LlamaCpp wrapper not available - skipping memory initialization")
                return True
            
            if not self.llm:
                logger.error("GGUF model must be loaded before initializing MemOS")
                return False
            
            logger.info("Initializing MemOS with LlamaCpp wrapper...")
            
            # Create our custom LLM wrapper around the loaded model
            self.llama_wrapper = LlamaCppWrapper(self.llm)
            logger.info("‚úÖ LlamaCpp wrapper created successfully")
            
            # Patch the GGUF LLM class to use our pre-loaded model
            self._patch_gguf_llm_for_memos()
            
            # Get MemOS configuration from config file
            memos_config = self.config.get('memos', {})
            user_id = memos_config.get('user_id', 'default_user')
            
            # Create user manager and ensure default user exists
            user_manager = UserManager()
            if not user_manager.validate_user(user_id):
                user_manager.create_user(
                    user_name=user_id,
                    role=UserRole.USER,
                    user_id=user_id
                )
                logger.info(f"Created default user: {user_id}")
            
            # Create MemOS configuration with standard GGUF backend
            mos_config_dict = {
                'user_id': user_id,
                'session_id': memos_config.get('session_id', 'default_session'),
                'enable_textual_memory': memos_config.get('enable_textual_memory', True),
                'enable_activation_memory': memos_config.get('enable_activation_memory', False),
                'top_k': memos_config.get('top_k', 5),
                'chat_model': {
                    'backend': 'gguf',
                    'config': {
                        'model_name_or_path': './smollm-quantized/smollm-q4_K_M.gguf',
                        'temperature': 0.7,
                        'max_tokens': 512,
                        'top_p': 0.9,
                        'top_k': 50,
                    }
                },
                'mem_reader': {
                    'backend': 'simple_struct',
                    'config': {
                        'llm': {
                            'backend': 'gguf',
                            'config': {
                                'model_name_or_path': './smollm-quantized/smollm-q4_K_M.gguf',
                                'temperature': 0.7,
                                'max_tokens': 512,
                                'top_p': 0.9,
                                'top_k': 50,
                            }
                        },
                        'embedder': {
                            'backend': 'sentence_transformer',
                            'config': {
                                'model_name_or_path': 'all-MiniLM-L6-v2'
                            }
                        },
                        'chunker': {
                            'backend': 'sentence',
                            'config': {
                                'tokenizer_or_token_counter': 'gpt2',
                                'chunk_size': 512,
                                'chunk_overlap': 128,
                                'min_sentences_per_chunk': 1
                            }
                        }
                    }
                }
            }
            
            # Create MemOS configuration object
            mos_config = MOSConfig(**mos_config_dict)
            
            # Initialize MemOS with our configuration  
            self.mos_instance = MOS(mos_config)
            
            # Replace the chat_llm with our wrapper after initialization
            self.mos_instance.chat_llm = self.llama_wrapper
            
            # Set up project memory manager with the MemOS instance
            if self.project_memory_manager:
                self.project_memory_manager.set_mos_instance(self.mos_instance)
                logger.info("‚úÖ Project memory manager configured with MemOS instance")
            
            logger.info("‚úÖ MemOS initialized successfully with LlamaCpp wrapper")
            
            # Register existing memory cubes if available - SKIP when ResourceManager is active
            if not self.resource_manager:
                try:
                    import os
                    memory_cubes_dir = "./memory_cubes"
                    logger.info("ResourceManager not available - registering legacy memory cubes")
                    if os.path.exists(memory_cubes_dir):
                        for user_dir in os.listdir(memory_cubes_dir):
                            user_path = os.path.join(memory_cubes_dir, user_dir)
                            if os.path.isdir(user_path):
                                for cube_dir in os.listdir(user_path):
                                    cube_path = os.path.join(user_path, cube_dir)
                                    if os.path.isdir(cube_path) and os.path.exists(os.path.join(cube_path, "config.json")):
                                        try:
                                            self.mos_instance.register_mem_cube(
                                                mem_cube_name_or_path=cube_path,
                                                mem_cube_id=cube_dir,
                                                user_id=user_dir
                                            )
                                            logger.info(f"‚úÖ Registered memory cube: {cube_dir} for user {user_dir}")
                                        except Exception as e:
                                            logger.warning(f"‚ö†Ô∏è Failed to register cube {cube_dir}: {e}")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Error registering memory cubes: {e}")
            else:
                logger.info("‚úÖ ResourceManager active - skipping legacy memory cube registration")
            
            # Test basic functionality - SKIP when ResourceManager is active to prevent Qdrant conflicts
            if not self.resource_manager:
                try:
                    test_response = self.mos_instance.chat("Hello", user_id=user_id)
                    logger.info("‚úÖ MemOS health check passed")
                    return True
                except Exception as e:
                    logger.error(f"‚ùå MemOS health check failed: {e}")
                    return False
            else:
                logger.info("‚úÖ ResourceManager active - skipping MemOS health check to prevent Qdrant conflicts")
                return True
            
        except Exception as e:
            logger.error(f"Failed to initialize MemOS: {e}")
            logger.error(f"Full error: {e.__class__.__name__}: {e}")
            import traceback
            traceback.print_exc()
            return True  # Non-critical failure - continue without memory features
    
    def _patch_gguf_llm_for_memos(self):
        """Patch the MemOS GGUF LLM class to use our pre-loaded model."""
        try:
            from memos.llms.gguf import GGUFLLLM
            
            # Store the original __init__ method
            original_init = GGUFLLLM.__init__
            
            # Create a patched __init__ that returns our wrapper
            def patched_init(llm_self, config):
                # Instead of loading a new model, use our wrapper
                llm_self.config = config
                llm_self.llama_wrapper = self.llama_wrapper
                logger.info("Using patched GGUF LLM with pre-loaded model")
            
            # Store original generate method
            original_generate = GGUFLLLM.generate
            
            # Create patched generate method that delegates to our wrapper
            def patched_generate(llm_self, messages, **kwargs):
                return self.llama_wrapper.generate(messages, **kwargs)
            
            # Apply the patches
            GGUFLLLM.__init__ = patched_init
            GGUFLLLM.generate = patched_generate
            
            logger.info("‚úÖ Successfully patched MemOS GGUF LLM class")
            
        except Exception as e:
            logger.error(f"Failed to patch MemOS GGUF LLM class: {e}")
            raise
    
    async def memos_chat(
        self,
        query: str,
        user_id: Optional[str] = None,
        project_id: Optional[str] = None,
        include_memory: bool = True,
        **kwargs
    ) -> Dict[str, Any]:
        """
        Perform memory-aware chat using MemOS with project-specific memory isolation.
        
        Args:
            query (str): User query/prompt
            user_id (Optional[str]): User ID for memory retrieval and context
            project_id (Optional[str]): Project ID for memory isolation (default: "default")
            include_memory (bool): Whether to use memory (always True for MemOS)
            **kwargs: Additional parameters (for compatibility)
            
        Returns:
            Dict[str, Any]: Response with generated text and metadata
        """
        if not self._is_initialized:
            raise RuntimeError("Service not initialized. Call startup() first.")
        
        if not self.mos_instance:
            # Fallback to enhanced_inference if MemOS is not available
            logger.warning("MemOS not available, falling back to enhanced_inference")
            return await self.enhanced_inference(query, user_id, include_memory, **kwargs)
        
        start_time = datetime.now()
        
        try:
            # Use MemOS for memory-aware chat with project-specific context
            effective_user_id = user_id or self.config.get('memos', {}).get('user_id', 'default_user')
            effective_project_id = project_id or "default"
            
            logger.info(f"üß† [MemOS Chat] Processing query for user {effective_user_id}, project {effective_project_id}: {query[:100]}...")
            
            # First search for codebase context to enhance the query
            codebase_context = ""
            memories_used = []
            
            try:
                # Use project memory manager for project-specific search if available
                if self.project_memory_manager:
                    # Ensure project cube exists and search within project context
                    project_cube_id = self.project_memory_manager.get_or_create_project_cube(
                        effective_user_id, effective_project_id
                    )
                    if project_cube_id:
                        # Search specifically within the project's memory cube
                        search_result = self.mos_instance.search(
                            query=query, 
                            user_id=effective_user_id,
                            install_cube_ids=[project_cube_id]
                        )
                        logger.info(f"üîç [Project Search] Searching project cube: {project_cube_id}")
                    else:
                        # Fallback to user-wide search
                        search_result = self.mos_instance.search(query=query, user_id=effective_user_id)
                        logger.warning(f"‚ö†Ô∏è [Search Fallback] Project cube creation failed, using user-wide search")
                else:
                    # Fallback to original behavior
                    search_result = self.mos_instance.search(query=query, user_id=effective_user_id)
                    logger.info(f"üîç [Legacy Search] Using legacy user-wide search")
                
                if search_result and search_result.get('text_mem'):
                    logger.info(f"üîç [Search Results] Found {len(search_result['text_mem'])} cube results")
                    
                    # Filter for codebase-related memories
                    codebase_memories = []
                    for cube_result in search_result['text_mem']:
                        cube_memories = cube_result.get('memories', [])
                        logger.info(f"üìö [Cube Analysis] Cube has {len(cube_memories)} memories")
                        
                        for memory in cube_memories[:5]:  # Top 5 most relevant
                            # Check if this memory contains code-like content
                            memory_content = memory.memory
                            if any(keyword in memory_content.lower() for keyword in [
                                'def ', 'function', 'class ', 'import ', 'return ', 
                                '.py', 'file:', 'path:', 'calculate_fibonacci', 
                                'textanalyzer', 'math_utils', 'string_processor',
                                'quantum_flux_capacitor', 'dilithium_crystals', 'quantumcomputer',
                                'unique_functions', 'temporal_coefficient', 'energy_level'
                            ]):
                                codebase_memories.append(memory_content)
                                memories_used.append({
                                    'id': memory.id,
                                    'content': memory_content[:200] + '...' if len(memory_content) > 200 else memory_content,
                                    'relevance_score': getattr(memory, 'score', 0.0),
                                    'source': 'codebase'
                                })
                    
                    if codebase_memories:
                        codebase_context = f"\n\nRelevant codebase context:\n{chr(10).join(codebase_memories[:3])}\n"
                        logger.info(f"‚úÖ [Codebase Context] Found {len(codebase_memories)} relevant code memories")
                    else:
                        logger.info("‚ö†Ô∏è [Codebase Context] No code-related memories found")
                else:
                    logger.info("‚ö†Ô∏è [Search] No search results returned")
                    
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è [Search Error] {e}")
            
            # Create enhanced query with codebase context
            if codebase_context:
                enhanced_query = f"{query}{codebase_context}\n\nPlease answer based on the provided codebase context when relevant."
                logger.info(f"üöÄ [Enhanced Query] Added codebase context ({len(codebase_context)} chars)")
            else:
                enhanced_query = query
                logger.info("üìù [Standard Query] No codebase context available, using original query")
            
            # Generate response using MemOS with enhanced query
            response = self.mos_instance.chat(query=enhanced_query, user_id=effective_user_id)
            
            end_time = datetime.now()
            inference_time = (end_time - start_time).total_seconds()
            
            logger.info(f"üß† [MemOS Chat] Generated response in {inference_time:.2f}s")
            
            return {
                'response': response,
                'query': query,
                'user_id': effective_user_id,
                'memory_enhanced': len(memories_used) > 0,  # True if codebase context was used
                'memories_used': memories_used,  # Populated from codebase search
                'inference_time_seconds': inference_time,
                'timestamp': end_time.isoformat(),
                'model_info': {
                    'type': 'memos-gguf',
                    'backend': 'llama-cpp-python-via-memos',
                    'model_path': getattr(self.llm, 'model_path', 'unknown') if self.llm else 'unknown'
                },
                'memos_enabled': True,
                'codebase_context_used': len(memories_used) > 0,
                'codebase_memories_count': len(memories_used),
            }
            
        except Exception as e:
            logger.error(f"MemOS chat failed: {e}")
            logger.error(f"Full error: {e.__class__.__name__}: {e}")
            # Fallback to enhanced_inference
            logger.warning("Falling back to enhanced_inference due to MemOS error")
            return await self.enhanced_inference(query, user_id, include_memory, **kwargs)
    
    async def enhanced_inference(
        self,
        query: str,
        user_id: Optional[str] = None,
        include_memory: bool = True,
        memory_top_k: Optional[int] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        Perform enhanced inference using direct llama-cpp-python model.
        
        Args:
            query (str): User query/prompt
            user_id (Optional[str]): User ID for memory retrieval
            include_memory (bool): Whether to include memory in the prompt
            memory_top_k (Optional[int]): Number of top memories to retrieve
            **kwargs: Additional generation parameters
            
        Returns:
            Dict[str, Any]: Response with generated text and metadata
        """
        if not self._is_initialized:
            raise RuntimeError("Service not initialized. Call startup() first.")
        
        if not self.llm:
            raise RuntimeError("GGUF model not loaded")
        
        start_time = datetime.now()
        
        try:
            # Get generation parameters
            generation_config = self.config.get('model', {}).get('generation', {})
            max_tokens = kwargs.get('max_tokens', generation_config.get('max_tokens', 256))
            temperature = kwargs.get('temperature', generation_config.get('temperature', 0.7))
            top_p = kwargs.get('top_p', generation_config.get('top_p', 0.9))
            
            # For now, implement basic chat without full MemOS integration
            # TODO: Enhanced memory integration can be added later
            memories_used = []
            
            if include_memory and MEMOS_AVAILABLE and self.mos_instance:
                # Basic memory search (if MemOS is available)
                try:
                    search_result = self.mos_instance.search(query=query, user_id=user_id)
                    if search_result and search_result.get('text_mem'):
                        # Extract memory content for context
                        memory_context = []
                        for cube_result in search_result['text_mem']:
                            memories = cube_result.get('memories', [])[:memory_top_k or 5]
                            for memory in memories:
                                memory_context.append(memory.memory)
                                memories_used.append({
                                    'id': memory.id,
                                    'content': memory.memory[:200] + '...' if len(memory.memory) > 200 else memory.memory,
                                    'relevance_score': getattr(memory.metadata, 'relativity', 0.0) if hasattr(memory, 'metadata') else 0.0,
                                })
                        
                        # Enhance query with memory context
                        if memory_context:
                            enhanced_query = f"Context from previous conversations:\n{chr(10).join(memory_context[:3])}\n\nUser question: {query}"
                        else:
                            enhanced_query = query
                except Exception as e:
                    logger.warning(f"Memory search failed: {e}")
                    enhanced_query = query
            else:
                enhanced_query = query
            
            # Use direct prompt instead of chat completion for better compatibility
            prompt = f"User: {enhanced_query}\nAssistant:"
            
            # Generate response using llama-cpp-python direct call
            response = self.llm(
                prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
                stop=["User:", "\n\n", "Assistant:", "<|im_end|>"],
                echo=False
            )
            
            # Extract generated text
            generated_text = response['choices'][0]['text'].strip()
            
            end_time = datetime.now()
            inference_time = (end_time - start_time).total_seconds()
            
            return {
                'response': generated_text,
                'query': query,
                'user_id': user_id,
                'memory_enhanced': include_memory and len(memories_used) > 0,
                'memories_used': memories_used,
                'inference_time_seconds': inference_time,
                'timestamp': end_time.isoformat(),
                'model_info': {
                    'type': 'gguf',
                    'backend': 'llama-cpp-python',
                    'model_path': self.llm.model_path if hasattr(self.llm, 'model_path') else 'unknown'
                },
            }
            
        except Exception as e:
            logger.error(f"Enhanced inference failed: {e}")
            raise
    
    async def load_codebase(
        self,
        directory_path: str,
        user_id: Optional[str] = None,
        project_id: Optional[str] = None,
        custom_memignore_path: Optional[str] = None,
        additional_patterns: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        Load a directory of code files into MemOS using .memignore-based filtering.
        
        This method provides complete user control over file inclusion/exclusion through
        .memignore files, replacing the old universal filtering approach.
        
        Args:
            directory_path (str): Path to the directory containing code files
            user_id (Optional[str]): User ID for memory context
            project_id (Optional[str]): Project ID for memory isolation (default: "default")
            custom_memignore_path (Optional[str]): Custom path to .memignore file
            additional_patterns (Optional[List[str]]): Additional exclusion patterns
            
        Returns:
            Dict[str, Any]: Loading operation results with comprehensive filtering stats
        """
        if not self._is_initialized:
            raise RuntimeError("Service not initialized. Call startup() first.")
        
        if not MEMOS_AVAILABLE or not self.mos_instance:
            raise RuntimeError("MemOS not available - cannot load codebase")
        
        # Import .memignore filter
        try:
            from src.core.memignore_filter import MemignoreFilter
        except ImportError as e:
            logger.error(f"‚ùå Failed to import MemignoreFilter: {e}")
            raise RuntimeError("MemignoreFilter not available - install pathspec with: pip install pathspec")
        
        start_time = datetime.now()
        
        # Get effective user ID and project ID
        effective_user_id = user_id or self.config.get('memos', {}).get('user_id', 'default_user')
        effective_project_id = project_id or "default"
        
        logger.info(f"üöÄ [Load Codebase] Starting .memignore-based codebase loading")
        logger.info(f"üìÇ [Load Codebase] Directory: {directory_path}")
        logger.info(f"üë§ [Load Codebase] User ID: {effective_user_id}")
        logger.info(f"üèóÔ∏è [Load Codebase] Project ID: {effective_project_id}")
        logger.info(f"üîß [Load Codebase] Custom .memignore: {custom_memignore_path or 'None'}")
        logger.info(f"‚ûï [Load Codebase] Additional patterns: {len(additional_patterns or [])}")
        
        # Initialize .memignore filter
        memignore_filter = MemignoreFilter()
        
        # Get project-specific MemCube via ResourceManager - THE CRITICAL FIX
        project_mem_cube = None
        cube_id = None
        
        try:
            if self.resource_manager and self.project_memory_manager:
                cube_id = self.project_memory_manager.get_or_create_project_cube(
                    effective_user_id, effective_project_id
                )
                if not cube_id:
                    raise RuntimeError(f"Failed to create project cube for {effective_user_id}:{effective_project_id}")
                
                # Direct access to ResourceManager MemCube - bypasses cache lookup issues  
                project_mem_cube = self.resource_manager.get_mem_cube(
                    cube_id, 
                    self.project_memory_manager._create_minimal_config(effective_user_id, effective_project_id)
                )
                
                if not project_mem_cube:
                    raise RuntimeError(f"ResourceManager failed to provide MemCube for {cube_id}")
                    
                logger.info(f"‚úÖ Using ResourceManager project cube: {cube_id} (text_mem available: {project_mem_cube.text_mem is not None})")
            else:
                # Fallback to old behavior if project memory manager not available
                logger.warning("Project memory manager not available, using legacy cube creation")
                accessible_cubes = self.mos_instance.user_manager.get_user_cubes(effective_user_id)
                if not accessible_cubes:
                    # Create a default memory cube for the user (legacy)
                    cube_id = f"{effective_user_id}_codebase_cube"
                    
                    from memos.configs.mem_cube import GeneralMemCubeConfig
                    from memos.mem_cube.general import GeneralMemCube
                    
                    cube_config = GeneralMemCubeConfig(
                        user_id=effective_user_id,
                        cube_id=cube_id,
                        text_mem={
                            "backend": "general_text",
                            "config": {
                                "embedder": {
                                    "backend": "sentence_transformer",
                                    "config": {
                                        "model_name_or_path": "all-MiniLM-L6-v2",
                                        "trust_remote_code": True
                                    }
                                },
                                "vector_db": {
                                    "backend": "qdrant",
                                    "config": {
                                        "collection_name": f"codebase_{effective_user_id}_code",
                                        "vector_dimension": 384,
                                        "distance_metric": "cosine",
                                        "host": None,
                                        "port": None,
                                        "path": f"./qdrant_storage/{effective_user_id}_{cube_id}"
                                    }
                                },
                                "extractor_llm": {
                                    "backend": "openai",
                                    "config": {
                                        "model_name_or_path": "gpt-3.5-turbo",
                                        "temperature": 0.0,
                                        "max_tokens": 8192,
                                        "api_key": "fake-api-key",
                                        "api_base": "http://localhost:11434/v1"
                                    }
                                }
                            }
                        }
                    )
                    
                    # Use ResourceManager to create MemCube with shared resources
                    if self.resource_manager:
                        mem_cube = self.resource_manager.get_mem_cube(cube_id, cube_config)
                        cube_path = f"./memory_cubes/{effective_user_id}/{cube_id}"
                        # ResourceManager handles internal resource sharing, no need to dump
                        logger.info(f"‚úÖ MemCube created via ResourceManager with shared resources")
                    else:
                        # Fallback to direct creation if ResourceManager unavailable
                        mem_cube = GeneralMemCube(cube_config)  
                        cube_path = f"./memory_cubes/{effective_user_id}/{cube_id}"
                        mem_cube.dump(cube_path)
                    
                    self.mos_instance.register_mem_cube(
                        mem_cube_name_or_path=cube_path,
                        mem_cube_id=cube_id,
                        user_id=effective_user_id
                    )
                    
                    logger.info(f"‚úÖ Created and registered legacy memory cube: {cube_id}")
                else:
                    cube_id = list(accessible_cubes)[0]  # Use first available cube
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error setting up user/cube: {e}")
        
        # Validate directory path
        directory_path = os.path.abspath(directory_path)
        if not os.path.exists(directory_path):
            raise ValueError(f"Directory does not exist: {directory_path}")
        
        if not os.path.isdir(directory_path):
            raise ValueError(f"Path is not a directory: {directory_path}")
        
        try:
            # Use .memignore filtering to get list of files to load
            filtered_files = memignore_filter.filter_codebase_files(
                project_root=directory_path,
                custom_memignore_path=custom_memignore_path,
                additional_patterns=additional_patterns
            )
            
            # Get filtering statistics
            filter_stats = memignore_filter.get_filtering_stats()
            
            logger.info(f"üîç [Filtering] Found {len(filtered_files)} files to load after .memignore filtering")
            
            # Check if .memignore exists to provide user feedback
            from pathlib import Path
            memignore_path = Path(directory_path) / ".memignore"
            memignore_exists = memignore_path.exists()
            
            if not memignore_exists and filter_stats.total_files_found > 1000:
                logger.warning("üí° [Tip] Large codebase detected with no .memignore file.")
                logger.warning("   Consider creating a .memignore file to exclude unwanted files and improve performance.")
                logger.warning("   Example: echo 'node_modules/\\n__pycache__/\\n*.log' > .memignore")
            
            # Load filtered files into memory
            loaded_files = []
            failed_files = []
            total_size_bytes = 0
            
            for file_path in filtered_files:
                try:
                    # Read file content
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        content = f.read()
                    
                    # Skip empty files
                    if not content.strip():
                        failed_files.append({
                            'path': str(file_path.relative_to(directory_path)),
                            'reason': 'Empty file'
                        })
                        continue
                    
                    # Get file info
                    relative_path = str(file_path.relative_to(directory_path))
                    file_ext = file_path.suffix.lower()
                    file_size = len(content.encode('utf-8'))
                    total_size_bytes += file_size
                    
                    # Format content for memory storage
                    memory_content = f"File: {relative_path}\nPath: {file_path}\nExtension: {file_ext}\nContent:\n{content}"
                    
                    # Create memory item
                    if project_mem_cube and project_mem_cube.text_mem:
                        try:
                            from memos.memories.textual.item import TextualMemoryItem, TextualMemoryMetadata
                            
                            metadata_obj = TextualMemoryMetadata(
                                type='fact',
                                source='file',
                                memory_time=datetime.now().strftime('%Y-%m-%d'),
                                tags=['code', 'file', file_ext.lstrip('.') if file_ext else 'no-ext'],
                                entities=[file_path.name],
                                confidence=100.0,
                                visibility='private',
                                updated_at=datetime.now().isoformat()
                            )
                            
                            memory_item = TextualMemoryItem(
                                memory=memory_content,
                                metadata=metadata_obj
                            )
                            
                            # Add to MemCube
                            project_mem_cube.text_mem.add([memory_item])
                            
                            loaded_files.append({
                                'path': relative_path,
                                'size_bytes': file_size,
                                'extension': file_ext
                            })
                            
                            logger.debug(f"‚úÖ [Memory] Added: {relative_path} ({file_size} bytes)")
                            
                        except Exception as e:
                            logger.warning(f"‚ö†Ô∏è [Memory] Failed to add {relative_path}: {e}")
                            failed_files.append({
                                'path': relative_path,
                                'reason': f'Memory error: {str(e)}'
                            })
                            continue
                    else:
                        logger.warning(f"‚ö†Ô∏è [Skip] No MemCube available for {relative_path}")
                        failed_files.append({
                            'path': relative_path,
                            'reason': 'No MemCube available'
                        })
                        continue
                        
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è [Load] Failed to process {file_path}: {e}")
                    failed_files.append({
                        'path': str(file_path.relative_to(directory_path)) if file_path else 'unknown',
                        'reason': f'Processing error: {str(e)}'
                    })
                    continue
            
            end_time = datetime.now()
            loading_time = (end_time - start_time).total_seconds()
            
            # Calculate efficiency metrics
            if filter_stats.total_files_found > 0:
                inclusion_rate = filter_stats.total_files_included / filter_stats.total_files_found
            else:
                inclusion_rate = 0.0
                
            success_rate = len(loaded_files) / len(filtered_files) if filtered_files else 0.0
            
            logger.info(f"‚úÖ [Load Codebase] Completed successfully!")
            logger.info(f"üìä [Results] Files loaded: {len(loaded_files)}")
            logger.info(f"üìä [Results] Files failed: {len(failed_files)}")
            logger.info(f"üìä [Results] Total size: {total_size_bytes:,} bytes ({total_size_bytes / 1024 / 1024:.1f} MB)")
            logger.info(f"üìä [Results] Inclusion rate: {inclusion_rate:.1%}")
            logger.info(f"üìä [Results] Success rate: {success_rate:.1%}")
            logger.info(f"‚è±Ô∏è [Results] Total time: {loading_time:.2f}s")
            
            # User feedback on results
            if len(loaded_files) == 0:
                logger.warning("üö® No files were loaded! Check your .memignore patterns.")
            elif inclusion_rate < 0.1:
                logger.warning(f"‚ö†Ô∏è Very low inclusion rate ({inclusion_rate:.1%}). Review .memignore patterns.")
            elif len(loaded_files) > 5000:
                logger.info("üí° Large number of files loaded. Consider adding more exclusion patterns for better performance.")
            
            return {
                'status': 'success',
                'directory_path': directory_path,
                'user_id': effective_user_id,
                'project_id': effective_project_id,
                'cube_id': cube_id,
                'files_loaded': len(loaded_files),
                'files_failed': len(failed_files),
                'total_size_bytes': total_size_bytes,
                'loading_time_seconds': loading_time,
                'timestamp': end_time.isoformat(),
                'filtering_method': '.memignore-based',
                'memignore_exists': memignore_exists,
                'memignore_patterns_count': len(filter_stats.memignore_patterns_used),
                'inclusion_rate': inclusion_rate,
                'success_rate': success_rate,
                'filtering_stats': {
                    'total_files_found': filter_stats.total_files_found,
                    'total_files_included': filter_stats.total_files_included,
                    'total_files_excluded': filter_stats.total_files_excluded,
                    'total_size_included_mb': filter_stats.total_size_included / 1024 / 1024,
                    'total_size_excluded_mb': filter_stats.total_size_excluded / 1024 / 1024,
                    'processing_time_seconds': filter_stats.processing_time_seconds,
                    'exclusion_breakdown': dict(filter_stats.exclusion_reasons),
                    'memignore_patterns': filter_stats.memignore_patterns_used[:20]  # Limit for response size
                },
                'loaded_files': loaded_files[:100],  # Limit response size
                'failed_files': failed_files[:50]    # Limit response size
            }
            
        except Exception as e:
            logger.error(f"‚ùå [Load Codebase] Failed: {e}")
            raise RuntimeError(f"Failed to load codebase: {str(e)}")
    
    def get_service_status(self) -> Dict[str, Any]:
        """
        Get comprehensive service status information.
        
        Returns:
            Dict[str, Any]: Service status and configuration
        """
        try:
            model_info = None
            model_healthy = False
            
            if self.llm and hasattr(self.llm, 'model_path'):
                model_info = {
                    'type': 'gguf',
                    'backend': 'llama-cpp-python',
                    'model_path': self.llm.model_path
                }
            
            if self.llm and hasattr(self.llm, 'is_healthy'):
                model_healthy = self.llm.is_healthy()
            elif self.llm:
                # If no is_healthy method, consider it healthy if it exists
                model_healthy = True
            
            return {
                'service': {
                    'name': 'GGUF Memory Service',
                    'initialized': self._is_initialized,
                    'startup_time': getattr(self, '_startup_time', None),
                },
                'memos': {
                    'status': 'running' if self.mos_instance else 'not_initialized',
                    'user_id': self.mos_instance.user_id if self.mos_instance else None,
                    'cubes_count': len(self.mos_instance.mem_cubes) if self.mos_instance else 0,
                    'textual_memory_enabled': self.mos_instance.config.enable_textual_memory if self.mos_instance else False,
                },
                'model': {
                    'type': 'gguf',
                    'loaded': model_healthy,
                    'healthy': model_healthy,
                    'info': model_info,
                },
                'config': {
                    'memory_retrieval_enabled': self.config.get('memory', {}).get('retrieval', {}).get('enabled', True),
                    'memory_top_k': self.config.get('memory', {}).get('retrieval', {}).get('top_k', 5),
                    'using_modelmanager': True,
                },
                'modelmanager': {
                    'memory_stats': model_manager.get_memory_stats(),
                    'available_models': model_manager.list_available_models(),
                    'idle_timeout_seconds': model_manager.IDLE_TIMEOUT_SECONDS,
                },
            }
            
        except Exception as e:
            logger.error(f"Error getting service status: {e}")
            return {
                'service': {'name': 'GGUF Memory Service', 'initialized': False, 'error': str(e)},
                'memos': {'status': 'error'},
                'model': {'type': 'gguf', 'loaded': False, 'healthy': False},
            }
    
    def is_healthy(self) -> bool:
        """
        Check if the service is healthy and ready to handle requests.
        
        Returns:
            bool: True if service is healthy
        """
        try:
            if not self._is_initialized:
                return False
            
            if not self.llm:
                return False
            
            if hasattr(self.llm, 'is_healthy'):
                return self.llm.is_healthy()
            
            return True
            
        except Exception as e:
            logger.error(f"Health check failed: {e}")
            return False
    
    async def shutdown(self) -> None:
        """
        Gracefully shutdown the service.
        """
        try:
            logger.info("Shutting down MemOS Service...")
            
            self.llm = None
            self.mos_instance = None
            self._is_initialized = False
            
            logger.info("MemOS Service shutdown completed")
            
        except Exception as e:
            logger.error(f"Error during shutdown: {e}")


# Global service instance
_service_instance: Optional[GGUFMemoryService] = None


async def get_service_instance(config_path: str = "config.yaml") -> GGUFMemoryService:
    """
    Get or create the global service instance.
    
    Args:
        config_path (str): Path to configuration file
        
    Returns:
        GGUFMemoryService: The service instance
    """
    global _service_instance
    
    if _service_instance is None:
        _service_instance = GGUFMemoryService(config_path)
        if not await _service_instance.startup():
            raise RuntimeError("Failed to start GGUF Memory Service")
    
    return _service_instance


async def shutdown_service() -> None:
    """
    Shutdown the global service instance.
    """
    global _service_instance
    
    if _service_instance:
        await _service_instance.shutdown()
        _service_instance = None 